{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Machine Learning: Logistic Regression",
   "id": "330777458282cdcb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0 Imports",
   "id": "22666f501b637f10"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-12T09:39:09.192982Z",
     "start_time": "2025-05-12T09:39:09.189564Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix, recall_score, matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:39:09.282961Z",
     "start_time": "2025-05-12T09:39:09.250025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(\"../dat/dips/DIPS_Data_cleaned.csv\", sep = \",\", low_memory = False)\n",
    "data_change = pd.read_csv(\"../dat/dips/DIPS_Data_cleaned_change.csv\", sep = \",\", low_memory = False)\n",
    "data_pred = pd.read_csv(\"../dat/dips/DIPS_Data_cleaned_pred.csv\", sep = \",\", low_memory = False)\n",
    "data_pred_y = pd.read_csv(\"../dat/dips/DIPS_Data_cleaned_pred_y.csv\", sep = \",\", low_memory = False)"
   ],
   "id": "5be374b80c4c838f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ad681ddc35f4da59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1 Logistic Regression Model",
   "id": "e807f0425825e774"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:39:09.349216Z",
     "start_time": "2025-05-12T09:39:09.340081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predictors\n",
    "X = data_change\n",
    "X = X.drop([\"hpi\"], axis = 1)\n",
    "\n",
    "# Target\n",
    "y = data_change[\"hpi\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "print(\"Logistic Regression \\n\",\n",
    "      \"X_train shape: \", X_train.shape, round(X_train.shape[0]/len(X), 2), \"\\n\",\n",
    "      \"X_test shape: \", X_test.shape, round(X_test.shape[0]/len(X), 2),  \"\\n\",\n",
    "      \"y_train shape: \", y_train.shape, round(y_train.shape[0]/len(y), 2), \"\\n\",\n",
    "      \"y_test shape: \", y_test.shape, round(y_test.shape[0]/len(y), 2), \"\\n\")"
   ],
   "id": "202a8106e8144fa2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression \n",
      " X_train shape:  (983, 22) 0.8 \n",
      " X_test shape:  (246, 22) 0.2 \n",
      " y_train shape:  (983,) 0.8 \n",
      " y_test shape:  (246,) 0.2 \n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:39:09.425186Z",
     "start_time": "2025-05-12T09:39:09.416323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sklearn_log_regression(x_test_log, x_train_log, y_train_log, y_test_log):\n",
    "    \"\"\"Computes OLS weights for linear regression without regularization using the sklearn library on the training set and\n",
    "       returns weights, testset predictions and metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # linear regression model\n",
    "    model = linear_model.LogisticRegression(fit_intercept = True, max_iter = 10000, penalty = None)\n",
    "\n",
    "    # fitting it on the training data\n",
    "    model.fit(x_train_log, y_train_log)\n",
    "\n",
    "    # 1: ESTIMATING WEIGHTS\n",
    "    weights_log = model.coef_\n",
    "    features_log = model.feature_names_in_\n",
    "\n",
    "    # 2: COMPUTE TEST SET PREDICTIONS\n",
    "    y_pred_log = model.predict(x_test_log)\n",
    "\n",
    "    # 3: COMPUTE METRICS\n",
    "    accuracy_log = model.score(x_test_log, y_test_log)\n",
    "    macro_f1_log = recall_score(y_test_log, y_pred_log, average = \"macro\")\n",
    "    micro_f1_log = recall_score(y_test_log, y_pred_log, average = \"micro\")\n",
    "    mcc_log = matthews_corrcoef(y_test_log, y_pred_log)\n",
    "\n",
    "    # 4: COMPUTE CONFUSION MATRIX\n",
    "    cm_log = confusion_matrix(y_test_log, y_pred_log)\n",
    "    precision_log = cm_log[1][1] / (cm_log[1][1] + cm_log[0][1])\n",
    "    recall_log = cm_log[1][1] / (cm_log[1][1] + cm_log[1][0])\n",
    "\n",
    "    # store metrics in a dictionary\n",
    "    metrics_log = {\n",
    "        \"accuracy\": round(accuracy_log, 4),\n",
    "        \"macro_f1\": round(macro_f1_log, 4),\n",
    "        \"micro_f1\": round(micro_f1_log, 4),\n",
    "        \"mcc\": round(mcc_log, 4),\n",
    "        \"precision\": round(precision_log, 4),\n",
    "        \"recall\": round(recall_log, 4),\n",
    "        \"confusion_matrix\": cm_log\n",
    "    }\n",
    "\n",
    "    return weights_log, y_pred_log, features_log, metrics_log"
   ],
   "id": "9ae73ab4a999e6cd",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:39:09.502231Z",
     "start_time": "2025-05-12T09:39:09.491283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sklearn_log_regression(x_test_l, x_train_l, y_train_l, y_test_l):\n",
    "    \"\"\"Computes OLS weights for linear regression with lasso regularization using the sklearn library on the training set and\n",
    "       returns weights, testset predictions and metrics.\n",
    "    \"\"\"\n",
    "    # 1: GRID SEARCH\n",
    "    log_model = linear_model.LogisticRegression(random_state = 42, solver = \"saga\")\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', None]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = log_model, param_grid = param_grid, cv = 10)\n",
    "    grid_search.fit(x_train_l, y_train_l)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best parameters: \", grid_search.best_params_)\n",
    "    print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "    # 2: FITTING THE MODEL\n",
    "    model = linear_model.LogisticRegression(C = best_model.C,\n",
    "                                            penalty = best_model.penalty,\n",
    "                                            solver = \"saga\",\n",
    "                                            max_iter = 10000)\n",
    "    model.fit(x_train_l, y_train_l)\n",
    "\n",
    "\n",
    "    # 1: CROSS VALIDATION\n",
    "    # model_cv = linear_model.LogisticRegressionCV(fit_intercept = True, max_iter = 10000, penalty = \"l1\", solver = \"saga\", Cs = [0.001, 0.01, 0.1, 1, 10], cv = 5, scoring = \"accuracy\")\n",
    "    #\n",
    "    # model_cv.fit(x_train_l, y_train_l)\n",
    "    #\n",
    "    # best_c = model_cv.C_[0]\n",
    "    # print(\"Best C: \", best_c)\n",
    "    #\n",
    "    # model = linear_model.LogisticRegression(fit_intercept = True, max_iter = 10000, penalty = \"l1\", solver = \"saga\", C = best_c)\n",
    "    #\n",
    "    # model.fit(x_train_l, y_train_l)\n",
    "\n",
    "    # 3: ESTIMATING WEIGHTS\n",
    "    weights_l = model.coef_\n",
    "    features_l = model.feature_names_in_\n",
    "\n",
    "    # 4: COMPUTE TEST SET PREDICTIONS\n",
    "    y_pred_l = model.predict(x_test_l)\n",
    "\n",
    "    # 5: COMPUTE METRICS\n",
    "    accuracy_l = model.score(x_test_l, y_test_l)\n",
    "    macro_f1_l = recall_score(y_test_l, y_pred_l, average = \"macro\")\n",
    "    micro_f1_l = recall_score(y_test_l, y_pred_l, average = \"micro\")\n",
    "    mcc_l = matthews_corrcoef(y_test_l, y_pred_l)\n",
    "\n",
    "    cm_l = confusion_matrix(y_test_l, y_pred_l)\n",
    "    precision_l = cm_l[1][1] / (cm_l[1][1] + cm_l[0][1])\n",
    "    recall_l = cm_l[1][1] / (cm_l[1][1] + cm_l[1][0])\n",
    "\n",
    "    # store metrics in a dictionary\n",
    "    metrics_l = {\n",
    "        \"accuracy\": round(accuracy_l, 4),\n",
    "        \"macro_f1\": round(macro_f1_l, 4),\n",
    "        \"micro_f1\": round(micro_f1_l, 4),\n",
    "        \"mcc\": round(mcc_l, 4),\n",
    "        \"precision\": round(precision_l, 4),\n",
    "        \"recall\": round(recall_l, 4),\n",
    "        \"confusion_matrix\": cm_l\n",
    "    }\n",
    "\n",
    "    return weights_l, y_pred_l, features_l, metrics_l"
   ],
   "id": "d10ab9c99c3b29db",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:39:12.964731Z",
     "start_time": "2025-05-12T09:39:09.579696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "\n",
    "weights_lr, y_pred_lr, features_lr, metrics_lr = sklearn_log_regression(X_test, X_train, y_train, y_test)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Execution time: \", {end - start}, \"seconds\")\n",
    "time_lr = end - start\n",
    "time_lr_df = pd.DataFrame({\"time\": [time_lr]})\n",
    "time_lr_df.to_csv(\"../exp/times_ML/time_lr.csv\", sep = \",\", index = False)\n",
    "\n",
    "# save weights and predictions\n",
    "weights_lr_df = pd.DataFrame(weights_lr, columns = features_lr)\n",
    "y_pred_lr = pd.DataFrame(y_pred_lr, columns = [\"y_pred\"])\n",
    "\n",
    "weights_lr_df.to_csv(\"../exp/weights/weights_lr.csv\", sep = \",\", index = False)\n",
    "y_pred_lr.to_csv(\"../exp/predictions/y_pred_lr.csv\", sep = \",\", index = False)"
   ],
   "id": "6577e6e241c5ba13",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1207: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'C': 0.01, 'penalty': 'l2'}\n",
      "Best score:  0.7333848690991548\n",
      "Execution time:  {3.3719699382781982} seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
      "50 fits failed out of a total of 200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "50 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/kimzierahn/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.71923315 0.72024325        nan 0.72830344 0.71923315 0.73338487\n",
      "        nan 0.72830344 0.72933416 0.72830344        nan 0.72830344\n",
      " 0.72930324 0.72931354        nan 0.72830344 0.72830344 0.72830344\n",
      "        nan 0.72830344]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:39:13.055541Z",
     "start_time": "2025-05-12T09:39:13.046573Z"
    }
   },
   "cell_type": "code",
   "source": "metrics_lr",
   "id": "77df803f8de1c61c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7358,\n",
       " 'macro_f1': 0.5555,\n",
       " 'micro_f1': 0.7358,\n",
       " 'mcc': np.float64(0.2023),\n",
       " 'precision': np.float64(0.625),\n",
       " 'recall': np.float64(0.1449),\n",
       " 'confusion_matrix': array([[171,   6],\n",
       "        [ 59,  10]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:39:13.124938Z",
     "start_time": "2025-05-12T09:39:13.122507Z"
    }
   },
   "cell_type": "code",
   "source": "# metrics_lasso",
   "id": "69cbb0b951703703",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:39:13.198377Z",
     "start_time": "2025-05-12T09:39:13.193694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# plot feature importance\n",
    "def plot_feature_importance(weights, features, title):\n",
    "    \"\"\"Plot feature importance for linear regression model\"\"\"\n",
    "    # get absolute value of weights\n",
    "    weights = np.abs(weights)\n",
    "\n",
    "    # create dataframe\n",
    "    df = pd.DataFrame({\"features\": features, \"weights\": weights[0]})\n",
    "\n",
    "    # sort by weights\n",
    "    df = df.sort_values(by = \"weights\", ascending = False)\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    sns.barplot(x = \"weights\", y = \"features\", data = df)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ],
   "id": "d845e2d0d648cf6c",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:39:13.271591Z",
     "start_time": "2025-05-12T09:39:13.268545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# plot_feature_importance(weights_lr, features_lr, \"Feature Importance for Logistic Regression\")\n",
    "# plot_feature_importance(weights_lasso, features_lasso, \"Feature Importance for Lasso Logistic Regression\")"
   ],
   "id": "d8d402afba3f9e12",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:39:13.341817Z",
     "start_time": "2025-05-12T09:39:13.338850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # plot feature importance in one plot\n",
    "# def plot_feature_importance_combined(weights_log, features_log, weights_l, features_l):\n",
    "#     \"\"\"Plot feature importance for linear regression model\"\"\"\n",
    "#     # get absolute value of weights\n",
    "#     weights_log = np.abs(weights_log)\n",
    "#     weights_l = np.abs(weights_l)\n",
    "#\n",
    "#     # create dataframe\n",
    "#     df_lr = pd.DataFrame({\"features\": features_log, \"weights\": weights_log[0], \"model\": \"Logistic Regression\"})\n",
    "#     df_lasso = pd.DataFrame({\"features\": features_l, \"weights\": weights_l[0], \"model\": \"Lasso Logistic Regression\"})\n",
    "#\n",
    "#     # combine dataframes\n",
    "#     df = pd.concat([df_lr, df_lasso])\n",
    "#\n",
    "#     # sort by weights\n",
    "#     df = df.sort_values(by = \"weights\", ascending = False)\n",
    "#\n",
    "#     # plot\n",
    "#     plt.figure(figsize = (10, 10))\n",
    "#     sns.barplot(x = \"weights\", y = \"features\", hue = \"model\", data = df)\n",
    "#     plt.title(\"Feature Importance for Logistic Regression and Lasso Logistic Regression\")\n",
    "#     plt.show()"
   ],
   "id": "8df9619e319aa179",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:39:13.430158Z",
     "start_time": "2025-05-12T09:39:13.409532Z"
    }
   },
   "cell_type": "code",
   "source": "plot_feature_importance_combined(weights_lr, features_lr, weights_lasso, features_lasso)",
   "id": "df76b8761be79c6f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_feature_importance_combined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mplot_feature_importance_combined\u001B[49m(weights_lr, features_lr, weights_lasso, features_lasso)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'plot_feature_importance_combined' is not defined"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T09:39:13.440177Z",
     "start_time": "2025-05-12T07:52:59.946699Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "39432fd3fb4fd997",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
