{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LLM: Zero-shot classification through LLMs and prompts",
   "id": "dd0560725aae3b41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Models**:\n",
    "\n",
    "- GPT-4o (OpenAI)\n",
    "- Gemini (Google)\n",
    "- Gemma (Google)\n",
    "- Llama (Meta)\n",
    "- Claude (Anthropic)\n",
    "- DeepSeek\n"
   ],
   "id": "d7a620a322cdfbae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0 Imports",
   "id": "cc42939b0f2e6112"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:58:17.450451Z",
     "start_time": "2025-05-18T12:58:15.374887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import anthropic\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import pdist"
   ],
   "id": "2aaae4e734b1e3c4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:58:17.537252Z",
     "start_time": "2025-05-18T12:58:17.464472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_test_simple_prompt_df = pd.read_csv(\"../dat/prompts/X_test_simple_prompt.csv\", sep = \",\", index_col = 0)\n",
    "X_test_class_definitions_prompt_df = pd.read_csv(\"../dat/prompts/X_test_class_definitions_prompt.csv\", sep = \",\", index_col = 0)\n",
    "X_test_profiled_simple_prompt_df = pd.read_csv(\"../dat/prompts/X_test_profiled_simple_prompt.csv\", sep = \",\", index_col = 0)\n",
    "X_test_few_shot_prompt_df = pd.read_csv(\"../dat/prompts/X_test_few_shot_prompt.csv\", sep = \",\", index_col = 0)\n",
    "X_test_vignette_prompt_df = pd.read_csv(\"../dat/prompts/X_test_vignette_prompt.csv\", sep = \",\", index_col = 0)\n",
    "X_test_claude_prompt_df = pd.read_csv(\"../dat/prompts/X_test_claude_prompt.csv\", sep = \",\", index_col = 0)"
   ],
   "id": "6f52bfed5f097045",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:58:18.654262Z",
     "start_time": "2025-05-18T12:58:18.649820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert to arrays\n",
    "X_test_simple_prompt = X_test_simple_prompt_df.values.flatten()\n",
    "X_test_class_definitions_prompt = X_test_class_definitions_prompt_df.values.flatten()\n",
    "X_test_profiled_simple_prompt = X_test_profiled_simple_prompt_df.values.flatten()\n",
    "X_test_few_shot_prompt = X_test_few_shot_prompt_df.values.flatten()\n",
    "X_test_vignette_prompt = X_test_vignette_prompt_df.values.flatten()\n",
    "X_test_claude_prompt = X_test_claude_prompt_df.values.flatten()"
   ],
   "id": "817be744e50093b5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:59:32.320016Z",
     "start_time": "2025-05-18T12:59:32.316452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "simple_instruction = \"Respond only with YES or NO.\"\n",
    "class_definitions_instruction = \"Respond only with YES or NO.\"\n",
    "profiled_simple_instruction = \"Respond only with YES or NO.\"\n",
    "few_shot_instruction = \"Respond only with YES or NO.\"\n",
    "vignette_instruction = \"Respond only with YES or NO.\"\n",
    "claude_instruction = \"You are an expert psychologist tasked with predicting whether an individual will develop a psychological disorder between two time points (T1 and T2) based on various psychological measures and demographic information. Your goal is to provide an accurate YES or NO prediction, supported by a brief explanation of your reasoning. Example output format: \\n Prediction: [YES/NO] \\n Explanation: [Brief explanation supporting your prediction].\"\n"
   ],
   "id": "42733c2134765065",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b5e409ae0b81dc9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1 Zero-shot classification with LLMs",
   "id": "73ae6d6f42430acd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this section, I will use the prompts created in the previous section to **classify the test set using different LLMs**. The LLMs will be used to classify whether a person develops a psychological disorder between time point T1 and T2.",
   "id": "e7c46aa0e2c9031f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1 ChatGPT-4o (OpenAI)",
   "id": "7c184484efa02be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.1.1 Testing prompting",
   "id": "b3181ef2a9c5ad83"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:33.332086Z",
     "start_time": "2025-05-16T10:07:33.329074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# client = OpenAI(\n",
    "#     api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    "# )\n",
    "#\n",
    "# # testing\n",
    "# response = client.responses.create(\n",
    "#     model = \"gpt-4o-mini\",\n",
    "#     instructions = \"You are a coding assistant that talks like a pirate.\",\n",
    "#     input = \"How do I check if a Python object is an instance of a class?\",\n",
    "# )\n",
    "#\n",
    "# print(response.output_text)"
   ],
   "id": "738fb998b35415c6",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f96758305e52ce67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.1.2. Prompting with ChatGPT-4o",
   "id": "34bf1926b6e19c7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:33.646671Z",
     "start_time": "2025-05-16T10:07:33.642573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# simple_prompt_array_GPT = []\n",
    "#\n",
    "# client = OpenAI(\n",
    "#     api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    "# )\n",
    "#\n",
    "# # measure time in seconds\n",
    "# start = time.time()\n",
    "#\n",
    "# # iterate over the test set and save the response for each prompt in an array\n",
    "# for prompt in X_test_simple_prompt:\n",
    "#     response = client.responses.create(\n",
    "#         model = \"gpt-4o\",\n",
    "#         instructions = simple_instruction,\n",
    "#         input = prompt,\n",
    "#     )\n",
    "#     simple_prompt_array_GPT.append(response.output_text)\n",
    "#     print(response.output_text)\n",
    "#\n",
    "# end = time.time()\n",
    "# print(f\"Time taken: {end - start} seconds\")\n",
    "# time_GPT_simple_prompt = end - start\n",
    "# time_GPT_simple_prompt_df = pd.DataFrame({\"time\": [time_GPT_simple_prompt]})\n",
    "# time_GPT_simple_prompt_df.to_csv(\"../exp/times_LLMs/GPT4/time_GPT4_simple_prompt.csv\", sep = \",\", index = False)\n",
    "#\n",
    "# # value counts for array\n",
    "# counts_simple_GPT = pd.Series(simple_prompt_array_GPT).value_counts()\n",
    "# print(counts_simple_GPT)\n",
    "#\n",
    "# # convert YES to 1 and NO to 0\n",
    "# simple_prompt_array_GPT = [1 if response == \"YES\" else 0 if response == \"NO\" else np.nan for response in simple_prompt_array_GPT]\n",
    "# simple_prompt_array_GPT\n",
    "#\n",
    "# # save the array to a csv file\n",
    "# simple_prompt_df_GPT = pd.DataFrame(simple_prompt_array_GPT, columns = [\"y_pred\"])\n",
    "# simple_prompt_df_GPT.to_csv(\"../exp/preds_LLMs/GPT4/y_pred_GPT4_simple_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "bff465ba605628f4",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:33.956912Z",
     "start_time": "2025-05-16T10:07:33.953548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class_def_array_GPT = []\n",
    "#\n",
    "# client = OpenAI(\n",
    "#     api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    "# )\n",
    "#\n",
    "# # measure time in seconds\n",
    "# start = time.time()\n",
    "#\n",
    "# # iterate over the test set and save the response for each prompt in an array\n",
    "# for prompt in X_test_class_definitions_prompt:\n",
    "#     response = client.responses.create(\n",
    "#         model = \"gpt-4o\",\n",
    "#         instructions = class_definitions_instruction,\n",
    "#         input = prompt,\n",
    "#     )\n",
    "#     class_def_array_GPT.append(response.output_text)\n",
    "#     print(response.output_text)\n",
    "#\n",
    "# end = time.time()\n",
    "# print(f\"Time taken: {end - start} seconds\")\n",
    "# time_GPT_class_definitions = end - start\n",
    "# time_GPT_class_definitions_df = pd.DataFrame({\"time\": [time_GPT_class_definitions]})\n",
    "# time_GPT_class_definitions_df.to_csv(\"../exp/times_LLMs/GPT4/time_GPT4_class_definitions_prompt.csv\", sep = \",\", index = False)\n",
    "#\n",
    "# # value counts for array\n",
    "# counts_class_def_GPT = pd.Series(class_def_array_GPT).value_counts()\n",
    "# print(counts_class_def_GPT)\n",
    "#\n",
    "# # convert YES to 1 and NO to 0\n",
    "# class_def_array_GPT = [1 if response == \"YES\" else 0 for response in class_def_array_GPT]\n",
    "# class_def_array_GPT\n",
    "#\n",
    "# # save the array to a csv file\n",
    "# class_def_df_GPT = pd.DataFrame(class_def_array_GPT, columns = [\"y_pred\"])\n",
    "# class_def_df_GPT.to_csv(\"../exp/preds_LLMs/GPT4/y_pred_GPT4_class_definitions_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "63f70e77e37a9919",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:34.224268Z",
     "start_time": "2025-05-16T10:07:34.220191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# profiled_simple_array_GPT = []\n",
    "#\n",
    "# client = OpenAI(\n",
    "#     api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    "# )\n",
    "#\n",
    "# # measure time in seconds\n",
    "# start = time.time()\n",
    "#\n",
    "# # iterate over the test set and save the response for each prompt in an array\n",
    "# for prompt in X_test_profiled_simple_prompt:\n",
    "#     response = client.responses.create(\n",
    "#         model = \"gpt-4o\",\n",
    "#         instructions = profiled_simple_instruction,\n",
    "#         input = prompt,\n",
    "#     )\n",
    "#     profiled_simple_array_GPT.append(response.output_text)\n",
    "#     print(response.output_text)\n",
    "#\n",
    "# end = time.time()\n",
    "# print(f\"Time taken: {end - start} seconds\")\n",
    "# time_GPT_profiled_simple = end - start\n",
    "# time_GPT_profiled_simple_df = pd.DataFrame({\"time\": [time_GPT_profiled_simple]})\n",
    "# time_GPT_profiled_simple_df.to_csv(\"../exp/times_LLMs/GPT4/time_GPT4_profiled_simple_prompt.csv\", sep = \",\", index = False)\n",
    "#\n",
    "# # value counts for array\n",
    "# counts_profiled_simple_GPT = pd.Series(profiled_simple_array_GPT).value_counts()\n",
    "# print(counts_profiled_simple_GPT)\n",
    "#\n",
    "# # convert YES to 1 and NO to 0\n",
    "# profiled_simple_array_GPT_val = [1 if response == \"YES\" else 0 for response in profiled_simple_array_GPT]\n",
    "# profiled_simple_array_GPT_val\n",
    "#\n",
    "# # save the array to a csv file\n",
    "# profiled_simple_df_GPT = pd.DataFrame(profiled_simple_array_GPT_val, columns = [\"y_pred\"])\n",
    "# profiled_simple_df_GPT.to_csv(\"../exp/preds_LLMs/GPT4/y_pred_GPT4_profiled_simple_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "8fff5eef0784f0bf",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:34.485185Z",
     "start_time": "2025-05-16T10:07:34.481200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# few_shot_array_GPT = []\n",
    "#\n",
    "# client = OpenAI(\n",
    "#     api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    "# )\n",
    "#\n",
    "# # measure time in seconds\n",
    "# start = time.time()\n",
    "#\n",
    "# # iterate over the test set and save the response for each prompt in an array\n",
    "# for prompt in X_test_few_shot_prompt:\n",
    "#     response = client.responses.create(\n",
    "#         model = \"gpt-4o\",\n",
    "#         instructions = few_shot_instruction,\n",
    "#         input = prompt,\n",
    "#     )\n",
    "#     few_shot_array_GPT.append(response.output_text)\n",
    "#     print(response.output_text)\n",
    "#\n",
    "# end = time.time()\n",
    "# print(f\"Time taken: {end - start} seconds\")\n",
    "# time_GPT_few_shot = end - start\n",
    "# time_GPT_few_shot_df = pd.DataFrame({\"time\": [time_GPT_few_shot]})\n",
    "# time_GPT_few_shot_df.to_csv(\"../exp/times_LLMs/GPT4/time_GPT4_few_shot_prompt.csv\", sep = \",\", index = False)\n",
    "#\n",
    "# # value counts for array\n",
    "# counts_few_shot_GPT = pd.Series(few_shot_array_GPT).value_counts()\n",
    "# print(counts_few_shot_GPT)\n",
    "#\n",
    "# # convert YES to 1 and NO to 0\n",
    "# few_shot_array_GPT_val = [1 if response == \"YES\" else 0 for response in few_shot_array_GPT]\n",
    "# few_shot_array_GPT_val\n",
    "#\n",
    "# # save the array to a csv file\n",
    "# few_shot_df_GPT = pd.DataFrame(few_shot_array_GPT_val, columns = [\"y_pred\"])\n",
    "# few_shot_df_GPT.to_csv(\"../exp/preds_LLMs/GPT4/y_pred_GPT4_few_shot_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "f7389ea105f9761b",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:34.750441Z",
     "start_time": "2025-05-16T10:07:34.746971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# vignette_array_GPT = []\n",
    "#\n",
    "# client = OpenAI(\n",
    "#     api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    "# )\n",
    "#\n",
    "# # measure time in seconds\n",
    "# start = time.time()\n",
    "#\n",
    "# # iterate over the test set and save the response for each prompt in an array\n",
    "# for prompt in X_test_vignette_prompt:\n",
    "#     response = client.responses.create(\n",
    "#         model = \"gpt-4o\",\n",
    "#         instructions = vignette_instruction,\n",
    "#         input = prompt,\n",
    "#     )\n",
    "#     vignette_array_GPT.append(response.output_text)\n",
    "#     print(response.output_text)\n",
    "#\n",
    "# end = time.time()\n",
    "# print(f\"Time taken: {end - start} seconds\")\n",
    "# time_GPT_vignette = end - start\n",
    "# time_GPT_vignette_df = pd.DataFrame({\"time\": [time_GPT_vignette]})\n",
    "# time_GPT_vignette_df.to_csv(\"../exp/times_LLMs/GPT4/time_GPT4_vignette_prompt.csv\", sep = \",\", index = False)\n",
    "#\n",
    "# # value counts for array\n",
    "# counts_vignette_GPT = pd.Series(vignette_array_GPT).value_counts()\n",
    "# print(counts_vignette_GPT)\n",
    "#\n",
    "# # convert YES to 1 and NO to 0\n",
    "# vignette_array_GPT_val = [1 if response == \"YES\" else 0 for response in vignette_array_GPT]\n",
    "# vignette_array_GPT_val\n",
    "#\n",
    "# # save the array to a csv file\n",
    "# vignette_df_GPT = pd.DataFrame(vignette_array_GPT_val, columns = [\"y_pred\"])\n",
    "# vignette_df_GPT.to_csv(\"../exp/preds_LLMs/GPT4/y_pred_GPT4_vignette_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "11d6cfb1d391ec89",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.1.3 Misclassified cases reasons",
   "id": "1af0af9d4db4cb1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:35.012534Z",
     "start_time": "2025-05-16T10:07:35.009262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# y_pred_GPT4_simple_prompt = pd.read_csv(\"../exp/preds_LLMs/y_pred_GPT4_simple_prompt.csv\", sep = \",\")\n",
    "# y_pred_GPT4_class_definition_prompt = pd.read_csv(\"../exp/preds_LLMs/y_pred_GPT4_class_definitions_prompt.csv\", sep = \",\")\n",
    "# y_pred_GPT4_profiled_simple_prompt = pd.read_csv(\"../exp/preds_LLMs/y_pred_GPT4_profiled_simple_prompt.csv\", sep = \",\")\n",
    "# y_pred_GPT4_few_shot_prompt = pd.read_csv(\"../exp/preds_LLMs/y_pred_GPT4_few_shot_prompt.csv\", sep = \",\")\n",
    "# y_pred_GPT4_vignette_prompt = pd.read_csv(\"../exp/preds_LLMs/y_pred_GPT4_vignette_prompt.csv\", sep = \",\")\n",
    "#\n",
    "# # convert to array\n",
    "# y_pred_GPT4_simple_prompt = y_pred_GPT4_simple_prompt[\"y_pred\"].to_numpy()\n",
    "# y_pred_GPT4_class_definition_prompt = y_pred_GPT4_class_definition_prompt[\"y_pred\"].to_numpy()\n",
    "# y_pred_GPT4_profiled_simple_prompt = y_pred_GPT4_profiled_simple_prompt[\"y_pred\"].to_numpy()\n",
    "# y_pred_GPT4_few_shot_prompt = y_pred_GPT4_few_shot_prompt[\"y_pred\"].to_numpy()\n",
    "# y_pred_GPT4_vignette_prompt = y_pred_GPT4_vignette_prompt[\"y_pred\"].to_numpy()"
   ],
   "id": "187e592c37102291",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:35.288160Z",
     "start_time": "2025-05-16T10:07:35.283506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # indentify misclassified cases by comparing y_pred_GPT4_XXX and y_test, save index\n",
    "# misclassified_cases_simple = []\n",
    "# misclassified_cases_class_def = []\n",
    "# misclassified_cases_profiled_simple = []\n",
    "# misclassified_cases_few_shot = []\n",
    "# misclassified_cases_vignette = []\n",
    "#\n",
    "# for i in range(len(y_pred_GPT4_simple_prompt)):\n",
    "#     if y_pred_GPT4_simple_prompt[i] != y_test.iloc[i]:\n",
    "#         misclassified_cases_simple.append(i)\n",
    "# total_cases_simple = len(y_pred_GPT4_simple_prompt)\n",
    "# misscl_cases_simple = len(misclassified_cases_simple)\n",
    "# correct_clases_simple = total_cases_simple - misscl_cases_simple\n",
    "#\n",
    "# for i in range(len(y_pred_GPT4_class_definition_prompt)):\n",
    "#     if y_pred_GPT4_class_definition_prompt[i] != y_test.iloc[i]:\n",
    "#         misclassified_cases_class_def.append(i)\n",
    "# total_cases_class_def = len(y_pred_GPT4_class_definition_prompt)\n",
    "# misscl_cases_class_def = len(misclassified_cases_class_def)\n",
    "# correct_clases_class_def = total_cases_class_def - misscl_cases_class_def\n",
    "#\n",
    "# for i in range(len(y_pred_GPT4_profiled_simple_prompt)):\n",
    "#     if y_pred_GPT4_profiled_simple_prompt[i] != y_test.iloc[i]:\n",
    "#         misclassified_cases_profiled_simple.append(i)\n",
    "# total_cases_profiled = len(y_pred_GPT4_profiled_simple_prompt)\n",
    "# misscl_cases_profiled = len(misclassified_cases_profiled_simple)\n",
    "# correct_clases_profiled = total_cases_profiled - misscl_cases_profiled\n",
    "#\n",
    "# for i in range(len(y_pred_GPT4_few_shot_prompt)):\n",
    "#     if y_pred_GPT4_few_shot_prompt[i] != y_test.iloc[i]:\n",
    "#         misclassified_cases_few_shot.append(i)\n",
    "# total_cases_few_shot = len(y_pred_GPT4_few_shot_prompt)\n",
    "# misscl_cases_few_shot = len(misclassified_cases_few_shot)\n",
    "# correct_clases_few_shot = total_cases_few_shot - misscl_cases_few_shot\n",
    "#\n",
    "# for i in range(len(y_pred_GPT4_vignette_prompt)):\n",
    "#     if y_pred_GPT4_vignette_prompt[i] != y_test.iloc[i]:\n",
    "#         misclassified_cases_vignette.append(i)\n",
    "# total_cases_vignette = len(y_pred_GPT4_vignette_prompt)\n",
    "# misscl_cases_vignette = len(misclassified_cases_vignette)\n",
    "# correct_clases_vignette = total_cases_vignette - misscl_cases_vignette"
   ],
   "id": "3aa9a0da4c196d87",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:35.546853Z",
     "start_time": "2025-05-16T10:07:35.543656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # save as df with total, correct and missclassified cases\n",
    "# simple_cases_df = pd.DataFrame({\"total\": [total_cases_simple], \"correct\": [correct_clases_simple], \"missclassified\": [misscl_cases_simple]})\n",
    "# simple_cases_df.to_csv(\"../exp/reasons_misclassifications_LLMs/GPT4/simple_cases_GPT_df.csv\", sep = \",\", index = True)\n",
    "#\n",
    "# class_def_cases_df = pd.DataFrame({\"total\": [total_cases_class_def], \"correct\": [correct_clases_class_def], \"missclassified\": [misscl_cases_class_def]})\n",
    "# class_def_cases_df.to_csv(\"../exp/reasons_misclassifications_LLMs/GPT4/class_def_cases_GPT_df.csv\", sep = \",\", index = True)\n",
    "#\n",
    "# profiled_cases_df = pd.DataFrame({\"total\": [total_cases_profiled], \"correct\": [correct_clases_profiled], \"missclassified\": [misscl_cases_profiled]})\n",
    "# profiled_cases_df.to_csv(\"../exp/reasons_misclassifications_LLMs/GPT4/profiled_cases_GPT_df.csv\", sep = \",\", index = True)\n",
    "#\n",
    "# few_shot_cases_df = pd.DataFrame({\"total\": [total_cases_few_shot], \"correct\": [correct_clases_few_shot], \"missclassified\": [misscl_cases_few_shot]})\n",
    "# few_shot_cases_df.to_csv(\"../exp/reasons_misclassifications_LLMs/GPT4/few_shot_cases_GPT_df.csv\", sep = \",\", index = True)\n",
    "#\n",
    "# vignette_cases_df = pd.DataFrame({\"total\": [total_cases_vignette], \"correct\": [correct_clases_vignette], \"missclassified\": [misscl_cases_vignette]})\n",
    "# vignette_cases_df.to_csv(\"../exp/reasons_misclassifications_LLMs/GPT4/vignette_cases_GPT_df.csv\", sep = \",\", index = True)\n"
   ],
   "id": "7b6031706cb820c3",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:35.828914Z",
     "start_time": "2025-05-16T10:07:35.824823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# simple_prompt_reasons = []\n",
    "# class_def_prompt_reasons = []\n",
    "# profiled_simple_prompt_reasons = []\n",
    "# few_shot_prompt_reasons = []\n",
    "# vignette_prompt_reasons = []\n",
    "#\n",
    "# client = OpenAI(\n",
    "#     api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    "# )\n",
    "#\n",
    "# instruction_reason = \"Please categorize why you misclassified the data. Respond only with the following categories as reasons for the misclassification in order to improve prompting. Possible categories are: \\nLack of context (emphasize or indicate the context of the query), \\nLack of examples (few-shot prompting with several examples of appropriate responses are shown before posing the actual question missing), \\nLack of feedback (interactive refining the prompt), \\nLack of counterfactual demonstrations (instances containing false facts to improve faithfulness in knowledge conflict situations), \\nLack of opinion-based information (reframe the context as a narrator’s statement and inquire about the narrator’s opinions), \\nKnowledge conflicts (memorized facts became outdated and counterfactual facts), \\nPrediction with Abstention (model is uncertain about their predictions) \\n \\n Do not mention specific change (e.g., increase or decrease) in predictors, do not go into detail of this specific case and do not repeat the question. Only respond with one or multiple of the categories as reasons for the misclassification, separated by ','. Mention the most important category first.\"\n",
    "#\n",
    "# # iterate over the misclassified cases and save the response for each prompt in an array\n",
    "# print(\"Simple prompt: \\n \\n\")\n",
    "# for i in misclassified_cases_simple:\n",
    "#     response = client.responses.create(\n",
    "#         model = \"gpt-4o\",\n",
    "#         instructions = instruction_reason,\n",
    "#         input = f\"Misclassified case {i}: Prompt: {X_test_simple_prompt[i]} Response: {y_pred_GPT4_simple_prompt[i]} True label: {y_test.iloc[i]}\"\n",
    "#     )\n",
    "#     simple_prompt_reasons.append(response.output_text)\n",
    "#     print(response.output_text)\n",
    "#\n",
    "# print(\"\\n \\n Class definition prompt: \\n \\n\")\n",
    "# for i in misclassified_cases_class_def:\n",
    "#     response = client.responses.create(\n",
    "#         model = \"gpt-4o\",\n",
    "#         instructions = instruction_reason,\n",
    "#         input = f\"Misclassified case {i}: Prompt: {X_test_class_definitions_prompt[i]} Response: {y_pred_GPT4_class_definition_prompt[i]} True label: {y_test.iloc[i]}\"\n",
    "#     )\n",
    "#     class_def_prompt_reasons.append(response.output_text)\n",
    "#     print(response.output_text)\n",
    "#\n",
    "# print(\"\\n \\n Profiled simple prompt: \\n \\n\")\n",
    "# for i in misclassified_cases_profiled_simple:\n",
    "#     response = client.responses.create(\n",
    "#         model = \"gpt-4o\",\n",
    "#         instructions = instruction_reason,\n",
    "#         input = f\"Misclassified case {i}: Prompt: {X_test_profiled_simple_prompt[i]} Response: {y_pred_GPT4_profiled_simple_prompt[i]} True label: {y_test.iloc[i]}\"\n",
    "#     )\n",
    "#     profiled_simple_prompt_reasons.append(response.output_text)\n",
    "#     print(response.output_text)\n",
    "#\n",
    "# print(\"\\n \\n Few shot prompt: \\n \\n\")\n",
    "# for i in misclassified_cases_few_shot:\n",
    "#     response = client.responses.create(\n",
    "#         model = \"gpt-4o\",\n",
    "#         instructions = instruction_reason,\n",
    "#         input = f\"Misclassified case {i}: Prompt: {X_test_few_shot_prompt[i]} Response: {y_pred_GPT4_few_shot_prompt[i]} True label: {y_test.iloc[i]}\"\n",
    "#     )\n",
    "#     few_shot_prompt_reasons.append(response.output_text)\n",
    "#     print(response.output_text)\n",
    "#\n",
    "# print(\"\\n \\n Vignette prompt: \\n \\n\")\n",
    "# for i in misclassified_cases_vignette:\n",
    "#     response = client.responses.create(\n",
    "#         model = \"gpt-4o\",\n",
    "#         instructions = instruction_reason,\n",
    "#         input = f\"Misclassified case {i}: Prompt: {X_test_vignette_prompt[i]} Response: {y_pred_GPT4_vignette_prompt[i]} True label: {y_test.iloc[i]}\"\n",
    "#     )\n",
    "#     vignette_prompt_reasons.append(response.output_text)\n",
    "#     print(response.output_text)"
   ],
   "id": "286fbda7d1c1de5c",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:36.098877Z",
     "start_time": "2025-05-16T10:07:36.095971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# all_reasons_simple = []\n",
    "# all_reasons_class_def = []\n",
    "# all_reasons_profiled_simple = []\n",
    "# all_reasons_few_shot = []\n",
    "# all_reasons_vignette = []\n",
    "#\n",
    "# for reason in simple_prompt_reasons:\n",
    "#     reason = reason.split(\", \")\n",
    "#     reason = [re.sub(r'[^A-Za-z\\s]', '', r).strip() for r in reason]\n",
    "#     all_reasons_simple.append(reason)\n",
    "#\n",
    "# for reason in class_def_prompt_reasons:\n",
    "#     reason = reason.split(\", \")\n",
    "#     reason = [re.sub(r'[^A-Za-z\\s]', '', r).strip() for r in reason]\n",
    "#     all_reasons_class_def.append(reason)\n",
    "#\n",
    "# for reason in profiled_simple_prompt_reasons:\n",
    "#     reason = reason.split(\", \")\n",
    "#     reason = [re.sub(r'[^A-Za-z\\s]', '', r).strip() for r in reason]\n",
    "#     all_reasons_profiled_simple.append(reason)\n",
    "#\n",
    "# for reason in few_shot_prompt_reasons:\n",
    "#     reason = reason.split(\", \")\n",
    "#     reason = [re.sub(r'[^A-Za-z\\s]', '', r).strip() for r in reason]\n",
    "#     all_reasons_few_shot.append(reason)\n",
    "#\n",
    "# for reason in vignette_prompt_reasons:\n",
    "#     reason = reason.split(\", \")\n",
    "#     reason = [re.sub(r'[^A-Za-z\\s]', '', r).strip() for r in reason]\n",
    "#     all_reasons_vignette.append(reason)"
   ],
   "id": "eda04f93a440ac40",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:36.381501Z",
     "start_time": "2025-05-16T10:07:36.377141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# simple_prompt_reasons_dict = {}\n",
    "# class_def_prompt_reasons_dict = {}\n",
    "# profiled_simple_prompt_reasons_dict = {}\n",
    "# few_shot_prompt_reasons_dict = {}\n",
    "# vignette_prompt_reasons_dict = {}\n",
    "#\n",
    "# for i in all_reasons_simple:\n",
    "#     for j in i:\n",
    "#         # count the occurrences of each reason\n",
    "#         if j in simple_prompt_reasons_dict:\n",
    "#             simple_prompt_reasons_dict[j] += 1\n",
    "#         else:\n",
    "#             simple_prompt_reasons_dict[j] = 1\n",
    "# simple_prompt_reasons_df = pd.DataFrame.from_dict(simple_prompt_reasons_dict, orient='index', columns=['count'])\n",
    "# # simple_prompt_reasons_df.to_csv(\"../exp/reasons_misclassifications_LLMs/GPT4/simple_prompt_reasons.csv\", sep = \",\", index = True)\n",
    "#\n",
    "#\n",
    "# for i in all_reasons_class_def:\n",
    "#     for j in i:\n",
    "#         # count the occurrences of each reason\n",
    "#         if j in class_def_prompt_reasons_dict:\n",
    "#             class_def_prompt_reasons_dict[j] += 1\n",
    "#         else:\n",
    "#             class_def_prompt_reasons_dict[j] = 1\n",
    "# class_def_prompt_reasons_df = pd.DataFrame.from_dict(class_def_prompt_reasons_dict, orient='index', columns=['count'])\n",
    "# # class_def_prompt_reasons_df.to_csv(\"../exp/reasons_misclassifications_LLMs/GPT4/class_def_prompt_reasons.csv\", sep = \",\", index = True)\n",
    "#\n",
    "# for i in all_reasons_profiled_simple:\n",
    "#     for j in i:\n",
    "#         # count the occurrences of each reason\n",
    "#         if j in profiled_simple_prompt_reasons_dict:\n",
    "#             profiled_simple_prompt_reasons_dict[j] += 1\n",
    "#         else:\n",
    "#             profiled_simple_prompt_reasons_dict[j] = 1\n",
    "# profiled_simple_prompt_reasons_df = pd.DataFrame.from_dict(profiled_simple_prompt_reasons_dict, orient='index', columns=['count'])\n",
    "# # profiled_simple_prompt_reasons_df.to_csv(\"../exp/reasons_misclassifications_LLMs/GPT4/profiled_simple_prompt_reasons.csv\", sep = \",\", index = True)\n",
    "#\n",
    "#\n",
    "# for i in all_reasons_few_shot:\n",
    "#     for j in i:\n",
    "#         # count the occurrences of each reason\n",
    "#         if j in few_shot_prompt_reasons_dict:\n",
    "#             few_shot_prompt_reasons_dict[j] += 1\n",
    "#         else:\n",
    "#             few_shot_prompt_reasons_dict[j] = 1\n",
    "# few_shot_prompt_reasons_df = pd.DataFrame.from_dict(few_shot_prompt_reasons_dict, orient='index', columns=['count'])\n",
    "# # few_shot_prompt_reasons_df.to_csv(\"../exp/reasons_misclassifications_LLMs/GPT4/few_shot_prompt_reasons.csv\", sep = \",\", index = True)\n",
    "#\n",
    "# for i in all_reasons_vignette:\n",
    "#     for j in i:\n",
    "#         # count the occurrences of each reason\n",
    "#         if j in vignette_prompt_reasons_dict:\n",
    "#             vignette_prompt_reasons_dict[j] += 1\n",
    "#         else:\n",
    "#             vignette_prompt_reasons_dict[j] = 1\n",
    "# vignette_prompt_reasons_df = pd.DataFrame.from_dict(vignette_prompt_reasons_dict, orient='index', columns=['count'])\n",
    "# # vignette_prompt_reasons_df.to_csv(\"../exp/reasons_misclassifications_LLMs/GPT4/vignette_prompt_reasons.csv\", sep = \",\", index = True)"
   ],
   "id": "953f9cd49f772076",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:36.732876Z",
     "start_time": "2025-05-16T10:07:36.728056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(simple_prompt_reasons_dict, \"\\n \\n\")\n",
    "# print(class_def_prompt_reasons_dict, \"\\n \\n\")\n",
    "# print(profiled_simple_prompt_reasons_dict, \"\\n \\n\")\n",
    "# print(few_shot_prompt_reasons_dict, \"\\n \\n\")\n",
    "# print(vignette_prompt_reasons_dict)"
   ],
   "id": "a17c7952d84333ba",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "36a05f87c5fa026e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2 Gemini (Google)",
   "id": "603d4a5c922bb11d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.2.1 Testing prompting",
   "id": "929a34c8bb3628be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:37.225406Z",
     "start_time": "2025-05-16T10:07:37.221031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# client = genai.Client(api_key = os.environ.get(\"GEMINI_API_KEY\"))\n",
    "#\n",
    "# response = client.models.generate_content(\n",
    "#     model = \"gemini-2.0-flash\",\n",
    "#     contents = \"Explain how AI works in a few words\",\n",
    "# )\n",
    "#\n",
    "# print(response.text)"
   ],
   "id": "db7c84d6d0cf7c",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:37.711521Z",
     "start_time": "2025-05-16T10:07:37.705340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# client = genai.Client(api_key = os.environ.get(\"GEMINI_API_KEY\"))\n",
    "#\n",
    "# response = client.models.generate_content(\n",
    "#     model = \"gemini-2.0-flash\",\n",
    "#     config = types.GenerateContentConfig(\n",
    "#         system_instruction = simple_instruction),\n",
    "#     contents = simple_prompt\n",
    "# )\n",
    "#\n",
    "# # gemini-2.5-pro-preview-05-06"
   ],
   "id": "b921335db7bb8d34",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.3 Gemma (Google)",
   "id": "2121e6074cbc6d60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:38.064480Z",
     "start_time": "2025-05-16T10:07:38.061870Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ec22e26e6d7db2ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.4 Llama (Meta)",
   "id": "f599161aabc9b77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:38.387615Z",
     "start_time": "2025-05-16T10:07:38.383977Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "325176ecf397915c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.5 Claude (Anthropic)",
   "id": "c4e1b70cdbf6eee3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.5.1 Testing prompting",
   "id": "536d090fb805aeed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:26:01.795066Z",
     "start_time": "2025-05-18T13:25:40.511178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# client = anthropic.Anthropic(api_key = os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "#\n",
    "# message = client.messages.create(\n",
    "#     model = \"claude-3-7-sonnet-20250219\",\n",
    "#     max_tokens = 20000,\n",
    "#     temperature = 1,\n",
    "#     thinking = {\n",
    "#         \"type\": \"enabled\",\n",
    "#         \"budget_tokens\": 16000\n",
    "#     },\n",
    "#     system = claude_instruction,\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [\n",
    "#                 {\n",
    "#                     \"type\": \"text\",\n",
    "#                     \"text\": X_test_claude_prompt[0]\n",
    "#                 }\n",
    "#             ]\n",
    "#         }\n",
    "#     ]\n",
    "# )\n",
    "# print(message.content)"
   ],
   "id": "42f91f18bea3b4ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ThinkingBlock(signature='ErUBCkYIAxgCIkDffTxBOdm1xHuuprm6mOE0hHT/lSo/htgiGsrgKTObUT63OVxdYuLoOiRRT/F9rkti/Tj/X9GS3uPmBHFpJsqfEgyylE8MaEDQcWZshF4aDMDoSXKovP4KDsAtniIwvAzWRfPaCRykRCslJUMmbaAfvuDhxom5rPN4ppwl+/h5Q3/FvdgH5UIn1bevv49gKh3EP6HsuEP6sjr3HKelTCOkvkEaD5wDrvWha8+n7hgC', thinking=\"Let me conduct a thorough psychological assessment based on the provided variables.\\n\\n<psychological_assessment>\\nI'll first categorize the variables into risk factors and protective factors at baseline (T1), then analyze the changes between T1 and T2, and finally make an overall assessment.\\n\\n**Baseline (T1) Assessment:**\\n\\n**Protective Factors at T1:**\\n- T1 Social support: 0.1421 (positive value indicates good social support)\\n- T1 General self-efficacy: 0.3650 (positive value indicates good self-efficacy)\\n- T1 Life satisfaction: 0.3373 (positive value indicates good life satisfaction)\\n- T1 Problem-focused coping: 1.7319 (strong positive value indicates good coping skills)\\n- T1 Emotion-focused coping: 0.2078 (positive value indicates presence of emotion-focused coping)\\n\\n**Risk Factors at T1:**\\n- T1 Positive mental health: -0.0279 (slightly negative, suggesting slightly below average mental health)\\n- T1 Stress: 0.4419 (positive value suggests presence of stress)\\n- T1 Anxiety sensitivity: 0.1594 (positive value indicates some anxiety sensitivity)\\n- T1 Fear of bodily sensations: 0.2864 (positive value suggests some fear of physical sensations)\\n- T1 Dysfunctional attitudes: 0.2751 (positive value indicates some dysfunctional attitudes)\\n- T1 General psychopathology (GSI): 0.0172 (slightly positive, suggesting minimal psychopathology)\\n\\n**Demographic Factors:**\\n- Education: 2.0 (likely moderate education level, generally protective)\\n- T1 BMI: 1.0 (underweight or low normal range, could be a risk factor depending on context)\\n- Socioeconomic status: 2.0 (moderate SES, generally protective)\\n\\n**Changes from T1 to T2:**\\n\\n**Protective Changes (Improvements):**\\n- Change in social support: 0.7057 (substantial improvement in social support)\\n- Change in life satisfaction: 0.1407 (slight improvement in life satisfaction)\\n- Change in anxiety sensitivity: -0.8617 (substantial decrease in anxiety sensitivity)\\n- Change in fear of bodily sensations: -0.8470 (substantial decrease in fear of bodily sensations)\\n- Change in general psychopathology (GSI): -0.8257 (substantial decrease in general psychopathology)\\n\\n**Risk Changes (Declines):**\\n- Change in positive mental health: -0.7520 (substantial decrease in positive mental health)\\n- Change in general self-efficacy: -0.1820 (modest decrease in self-efficacy)\\n- Change in dysfunctional attitudes: 0.4849 (increase in dysfunctional attitudes)\\n\\n**Overall Assessment:**\\n\\nAt T1, the individual appears to have a mixed profile with both protective and risk factors. The protective factors include good social support, self-efficacy, life satisfaction, and strong problem-focused coping skills. The risk factors include slightly below-average positive mental health, presence of stress, and some anxiety sensitivity, fear of bodily sensations, and dysfunctional attitudes. However, the general psychopathology score is only slightly elevated, suggesting minimal psychological symptoms at T1.\\n\\nThe changes between T1 and T2 show a mixed pattern as well. On one hand, there are several positive changes: social support improved substantially, life satisfaction increased slightly, and notably, anxiety sensitivity, fear of bodily sensations, and general psychopathology all decreased substantially. These changes suggest significant improvement in several important domains related to psychological health.\\n\\nOn the other hand, there are concerning changes: a substantial decrease in positive mental health, a modest decrease in self-efficacy, and an increase in dysfunctional attitudes. These changes might suggest increased vulnerability to psychological problems.\\n\\nWhen weighing these factors, I note that despite the decrease in positive mental health and increase in dysfunctional attitudes, the substantial decreases in anxiety sensitivity, fear of bodily sensations, and especially general psychopathology (GSI) are particularly significant. The GSI is a broad measure of psychological symptoms, and its substantial decrease (-0.8257) suggests that overall psychological symptoms decreased significantly from T1 to T2. This is a strong indicator that the person is unlikely to have developed a new psychological disorder between T1 and T2.\\n\\nAdditionally, the improvements in social support and life satisfaction, though the latter is modest, are protective factors that would help buffer against the development of psychological disorders.\\n\\nWhile the decrease in positive mental health and increase in dysfunctional attitudes are concerning, they appear to be outweighed by the improvements in anxiety sensitivity, fear of bodily sensations, and general psychopathology. The person may be experiencing some difficulties in certain areas (perhaps cognitive patterns and positive affect), but these don't seem to have translated into increased psychopathology overall.\\n\\nBased on this comprehensive analysis, I would predict that this individual did NOT develop a psychological disorder between T1 and T2. The key evidence for this is the substantial decrease in general psychopathology (GSI) and anxiety-related measures, along with improved social support, despite some concerning changes in other domains.\\n</psychological_assessment>\\n\\nBased on my assessment, I will now provide a prediction and brief explanation:\\n\\nPrediction: NO\\nExplanation: The substantial decrease in general psychopathology (GSI: -0.8257) and anxiety-related measures, combined with significantly improved social support, strongly suggest the individual did not develop a psychological disorder between T1 and T2, despite some concerning decreases in positive mental health and self-efficacy.\", type='thinking'), TextBlock(citations=None, text='Prediction: NO\\nExplanation: Despite some concerning trends like decreased positive mental health (-0.75) and increased dysfunctional attitudes (0.48), the substantial reduction in general psychopathology (GSI: -0.83) and anxiety-related measures combined with improved social support (0.71) strongly indicate the individual did not develop a psychological disorder between timepoints.', type='text')]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:08:00.741681Z",
     "start_time": "2025-05-18T13:08:00.738728Z"
    }
   },
   "cell_type": "code",
   "source": "# print(message.content[0].thinking)",
   "id": "79e3b3290d646d8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me analyze the available data to determine if this person develops a psychological disorder between time points T1 and T2.\n",
      "\n",
      "First, I'll look at the baseline (T1) measures:\n",
      "- T1 Positive mental health: -0.0279170753483525 (slightly below average)\n",
      "- T1 Social support: 0.1421238143169474 (slightly above average)\n",
      "- T1 General self-efficacy: 0.3649793457412237 (above average)\n",
      "- T1 Life satisfaction: 0.3372886835461141 (above average)\n",
      "- T1 Stress: 0.4419361727222826 (above average)\n",
      "- T1 Problem-focused coping: 1.7319368683783989 (well above average)\n",
      "- T1 Emotion-focused coping: 0.2078300133169115 (slightly above average)\n",
      "- T1 Anxiety sensitivity: 0.1594156886399411 (slightly above average)\n",
      "- T1 Fear of bodily sensations: 0.2863750811390516 (above average)\n",
      "- T1 Dysfunctional attitudes: 0.2750686254386546 (above average)\n",
      "- T1 General psychopathology (GSI): 0.0172227087467131 (very slightly above average)\n",
      "- Education: 2.0\n",
      "- T1 BMI: 1.0\n",
      "- Socioeconomic status: 2.0\n",
      "\n",
      "Now I'll look at the changes from T1 to T2:\n",
      "- Change in Positive mental health: -0.7520166349788642 (substantial decrease)\n",
      "- Change in Social support: 0.7057099569575698 (substantial increase)\n",
      "- Change in General self-efficacy: -0.1819798191096402 (moderate decrease)\n",
      "- Change in Life satisfaction: 0.1407378746091848 (moderate increase)\n",
      "- Change in Anxiety sensitivity: -0.8617238998696288 (substantial decrease)\n",
      "- Change in Fear of bodily sensations: -0.846980042266938 (substantial decrease)\n",
      "- Change in Dysfunctional attitudes: 0.4849021865236002 (substantial increase)\n",
      "- Change in General psychopathology (GSI): -0.8256637821414634 (substantial decrease)\n",
      "\n",
      "To determine if the person develops a psychological disorder, I should primarily focus on the General psychopathology (GSI) measure, which is a global indicator of psychological distress or disorder. The Global Severity Index (GSI) is typically used as an overall measure of psychological distress.\n",
      "\n",
      "At T1, the GSI was very slightly above average (0.0172227087467131), and the change from T1 to T2 shows a substantial decrease (-0.8256637821414634). This means that the overall level of psychological distress decreased substantially between the two time points.\n",
      "\n",
      "Other relevant indicators also show improvements:\n",
      "- Anxiety sensitivity decreased substantially\n",
      "- Fear of bodily sensations decreased substantially\n",
      "\n",
      "Some concerning changes:\n",
      "- Positive mental health decreased substantially\n",
      "- Dysfunctional attitudes increased substantially\n",
      "- General self-efficacy decreased moderately\n",
      "\n",
      "However, some positive changes:\n",
      "- Social support increased substantially\n",
      "- Life satisfaction increased moderately\n",
      "\n",
      "The most direct indicator of psychological disorder is the GSI, which shows a substantial decrease (improvement). This strongly suggests that the person did not develop a psychological disorder between T1 and T2. In fact, their overall psychological functioning appears to have improved significantly based on the GSI.\n",
      "\n",
      "The decrease in positive mental health and increase in dysfunctional attitudes are concerning factors, but they don't necessarily indicate the development of a psychological disorder when the overall GSI has improved so substantially.\n",
      "\n",
      "Based on this analysis, I would conclude that the person did NOT develop a psychological disorder between time points T1 and T2.\n",
      "\n",
      "Actually, I should be more careful with my interpretation here. I notice that the change in GSI is negative (-0.8256637821414634), which means that the T2 value would be approximately -0.8084410733947503 (adding the T1 value and the change). \n",
      "\n",
      "If these values are standardized (like z-scores), then a negative value would indicate below-average psychopathology, which is good. However, if these are raw scores, a negative value might not make sense for a measure like GSI which typically doesn't take negative values.\n",
      "\n",
      "So my interpretation depends on the scale and standardization of these measurements. But since most of these values appear to be in a similar range (mostly between -1 and +2), I'm assuming they are standardized scores, in which case the decrease in GSI would indeed represent an improvement.\n",
      "\n",
      "Therefore, my answer remains that the person did NOT develop a psychological disorder between T1 and T2.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:34:17.480538Z",
     "start_time": "2025-05-18T13:34:17.456611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction = re.findall(r'Prediction: (.*)', message.content[1].text)\n",
    "prediction[0]"
   ],
   "id": "2e198379eb34a06",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m prediction \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39mfindall(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPrediction: (.*)\u001B[39m\u001B[38;5;124m'\u001B[39m, message\u001B[38;5;241m.\u001B[39mcontent[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mtext)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mprediction\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:28:22.529311Z",
     "start_time": "2025-05-18T13:28:22.524548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# extract what comes after Explanation:\n",
    "explanation = re.findall(r'Explanation: (.*)', message.content[1].text)\n",
    "explanation[0]"
   ],
   "id": "fb9e1f526e0ea4fe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Despite some concerning trends like decreased positive mental health (-0.75) and increased dysfunctional attitudes (0.48), the substantial reduction in general psychopathology (GSI: -0.83) and anxiety-related measures combined with improved social support (0.71) strongly indicate the individual did not develop a psychological disorder between timepoints.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.5.2 Prompting with Claude 3.7 Sonnet",
   "id": "121677042242e32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:41:46.655289Z",
     "start_time": "2025-05-18T13:41:46.648551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# simple_prompt_array_claude = []\n",
    "# simple_prompt_thinking_claude = []\n",
    "#\n",
    "# client = anthropic.Anthropic(api_key = os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "#\n",
    "# # measure time in seconds\n",
    "# start = time.time()\n",
    "#\n",
    "# # iterate over the test set and save the response for each prompt in an array\n",
    "# for prompt in X_test_simple_prompt:\n",
    "#     message = client.messages.create(\n",
    "#         model = \"claude-3-7-sonnet-20250219\",\n",
    "#         max_tokens = 20000,\n",
    "#         temperature = 1,\n",
    "#         thinking = {\n",
    "#             \"type\": \"enabled\",\n",
    "#             \"budget_tokens\": 16000\n",
    "#         },\n",
    "#         system = simple_instruction,\n",
    "#         messages = [\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": [\n",
    "#                     {\n",
    "#                         \"type\": \"text\",\n",
    "#                         \"text\": prompt\n",
    "#                     }\n",
    "#                 ]\n",
    "#             }\n",
    "#         ]\n",
    "#     )\n",
    "#     simple_prompt_array_claude.append(message.content[1].text)\n",
    "#     simple_prompt_thinking_claude.append(message.content[0].thinking)\n",
    "#     print(message.content[1].text)\n",
    "#\n",
    "# end = time.time()\n",
    "# print(f\"Time taken: {end - start} seconds\")\n",
    "# time_claude_simple_prompt = end - start\n",
    "# time_claude_simple_prompt_df = pd.DataFrame({\"time\": [time_claude_simple_prompt]})\n",
    "# time_claude_simple_prompt_df.to_csv(\"../exp/times_LLMs/Claude/time_claude_simple_prompt.csv\", sep = \",\", index = False)\n",
    "#\n",
    "# # value counts for array\n",
    "# counts_simple_claude = pd.Series(simple_prompt_array_claude).value_counts()\n",
    "# print(counts_simple_claude)\n",
    "#\n",
    "# # convert YES to 1 and NO to 0\n",
    "# simple_prompt_array_claude = [1 if response == \"YES\" else 0 if response == \"NO\" else np.nan for response in simple_prompt_array_claude]\n",
    "#\n",
    "# # save the array to a csv file\n",
    "# simple_prompt_df_claude = pd.DataFrame(simple_prompt_array_claude, columns = [\"y_pred\"])\n",
    "# simple_prompt_df_claude.to_csv(\"../exp/preds_LLMs/Claude/y_pred_claude_simple_prompt.csv\", sep = \",\", index = False)\n",
    "#\n",
    "# simple_prompt_df_thinking_claude = pd.DataFrame(simple_prompt_thinking_claude, columns = [\"thinking\"])\n",
    "# simple_prompt_df_thinking_claude.to_csv(\"../exp/preds_LLMs/Claude/Thinking/thinking_claude_simple_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "82b0ac9aa76bd4a3",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:42:29.437573Z",
     "start_time": "2025-05-18T13:42:29.430405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class_def_array_claude = []\n",
    "# class_def_thinking_claude = []\n",
    "#\n",
    "# client = anthropic.Anthropic(api_key = os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "#\n",
    "# # measure time in seconds\n",
    "# start = time.time()\n",
    "#\n",
    "# # iterate over the test set and save the response for each prompt in an array\n",
    "# for prompt in X_test_class_definitions_prompt:\n",
    "#     message = client.messages.create(\n",
    "#         model = \"claude-3-7-sonnet-20250219\",\n",
    "#         max_tokens = 20000,\n",
    "#         temperature = 1,\n",
    "#         thinking = {\n",
    "#             \"type\": \"enabled\",\n",
    "#             \"budget_tokens\": 16000\n",
    "#         },\n",
    "#         system = class_definitions_instruction,\n",
    "#         messages = [\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": [\n",
    "#                     {\n",
    "#                         \"type\": \"text\",\n",
    "#                         \"text\": prompt\n",
    "#                     }\n",
    "#                 ]\n",
    "#             }\n",
    "#         ]\n",
    "#     )\n",
    "#     class_def_array_claude.append(message.content[1].text)\n",
    "#     class_def_thinking_claude.append(message.content[0].thinking)\n",
    "#     print(message.content[1].text)\n",
    "#\n",
    "# end = time.time()\n",
    "# print(f\"Time taken: {end - start} seconds\")\n",
    "# time_claude_class_definitions = end - start\n",
    "# time_claude_class_definitions_df = pd.DataFrame({\"time\": [time_claude_class_definitions]})\n",
    "# time_claude_class_definitions_df.to_csv(\"../exp/times_LLMs/Claude/time_claude_class_definitions_prompt.csv\", sep = \",\", index = False)\n",
    "#\n",
    "# # value counts for array\n",
    "# counts_class_def_claude = pd.Series(class_def_array_claude).value_counts()\n",
    "# print(counts_class_def_claude)\n",
    "#\n",
    "# # convert YES to 1 and NO to 0\n",
    "# class_def_array_claude = [1 if response == \"YES\" else 0 for response in class_def_array_claude]\n",
    "#\n",
    "# # save the array to a csv file\n",
    "# class_def_df_claude = pd.DataFrame(class_def_array_claude, columns = [\"y_pred\"])\n",
    "# class_def_df_claude.to_csv(\"../exp/preds_LLMs/Claude/y_pred_claude_class_definitions_prompt.csv\", sep = \",\", index = False)\n",
    "#\n",
    "# class_def_prompt_df_thinking_claude = pd.DataFrame(class_def_thinking_claude, columns = [\"thinking\"])\n",
    "# class_def_prompt_df_thinking_claude.to_csv(\"../exp/preds_LLMs/Claude/Thinking/thinking_claude_class_def_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "71a5f0ea55213998",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:43:23.728057Z",
     "start_time": "2025-05-18T13:43:23.720930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# profiled_simple_array_claude = []\n",
    "# profiled_simple_thinking_claude = []\n",
    "#\n",
    "# client = anthropic.Anthropic(api_key = os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "#\n",
    "# # measure time in seconds\n",
    "# start = time.time()\n",
    "#\n",
    "# # iterate over the test set and save the response for each prompt in an array\n",
    "# for prompt in X_test_profiled_simple_prompt:\n",
    "#     message = client.messages.create(\n",
    "#         model = \"claude-3-7-sonnet-20250219\",\n",
    "#         max_tokens = 20000,\n",
    "#         temperature = 1,\n",
    "#         thinking = {\n",
    "#             \"type\": \"enabled\",\n",
    "#             \"budget_tokens\": 16000\n",
    "#         },\n",
    "#         system = profiled_simple_instruction,\n",
    "#         messages = [\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": [\n",
    "#                     {\n",
    "#                         \"type\": \"text\",\n",
    "#                         \"text\": prompt\n",
    "#                     }\n",
    "#                 ]\n",
    "#             }\n",
    "#         ]\n",
    "#     )\n",
    "#     profiled_simple_array_claude.append(message.content[1].text)\n",
    "#     profiled_simple_thinking_claude.append(message.content[0].thinking)\n",
    "#     print(message.content[1].text)\n",
    "#\n",
    "# end = time.time()\n",
    "# print(f\"Time taken: {end - start} seconds\")\n",
    "# time_claude_profiled_simple = end - start\n",
    "# time_claude_profiled_simple_df = pd.DataFrame({\"time\": [time_claude_profiled_simple]})\n",
    "# time_claude_profiled_simple_df.to_csv(\"../exp/times_LLMs/Claude/time_claude_profiled_simple_prompt.csv\", sep = \",\", index = False)\n",
    "#\n",
    "# # value counts for array\n",
    "# counts_profiled_simple_claude = pd.Series(profiled_simple_array_claude).value_counts()\n",
    "# print(counts_profiled_simple_claude)\n",
    "#\n",
    "# # convert YES to 1 and NO to 0\n",
    "# profiled_simple_array_claude_val = [1 if response == \"YES\" else 0 for response in profiled_simple_array_claude]\n",
    "#\n",
    "# # save the array to a csv file\n",
    "# profiled_simple_df_claude = pd.DataFrame(profiled_simple_array_claude_val, columns = [\"y_pred\"])\n",
    "# profiled_simple_df_claude.to_csv(\"../exp/preds_LLMs/Claude/y_pred_claude_profiled_simple_prompt.csv\", sep = \",\", index = False)\n",
    "#\n",
    "# profiled_simple_prompt_df_thinking_claude = pd.DataFrame(profiled_simple_thinking_claude, columns = [\"thinking\"])\n",
    "# profiled_simple_prompt_df_thinking_claude.to_csv(\"../exp/preds_LLMs/Claude/Thinking/thinking_claude_profiled_simple_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "a1e5ce97442023ea",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:43:58.082398Z",
     "start_time": "2025-05-18T13:43:58.075752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# few_shot_array_claude = []\n",
    "# few_shot_thinking_claude = []\n",
    "#\n",
    "# client = anthropic.Anthropic(api_key = os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "#\n",
    "# # measure time in seconds\n",
    "# start = time.time()\n",
    "#\n",
    "# # iterate over the test set and save the response for each prompt in an array\n",
    "# for prompt in X_test_few_shot_prompt:\n",
    "#     message = client.messages.create(\n",
    "#         model = \"claude-3-7-sonnet-20250219\",\n",
    "#         max_tokens = 20000,\n",
    "#         temperature = 1,\n",
    "#         thinking = {\n",
    "#             \"type\": \"enabled\",\n",
    "#             \"budget_tokens\": 16000\n",
    "#         },\n",
    "#         system = few_shot_instruction,\n",
    "#         messages = [\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": [\n",
    "#                     {\n",
    "#                         \"type\": \"text\",\n",
    "#                         \"text\": prompt\n",
    "#                     }\n",
    "#                 ]\n",
    "#             }\n",
    "#         ]\n",
    "#     )\n",
    "#     few_shot_array_claude.append(message.content[1].text)\n",
    "#     few_shot_thinking_claude.append(message.content[0].thinking)\n",
    "#     print(message.content[1].text)\n",
    "#\n",
    "# end = time.time()\n",
    "# print(f\"Time taken: {end - start} seconds\")\n",
    "# time_claude_few_shot = end - start\n",
    "# time_claude_few_shot_df = pd.DataFrame({\"time\": [time_claude_few_shot]})\n",
    "# time_claude_few_shot_df.to_csv(\"../exp/times_LLMs/Claude/time_claude_few_shot_prompt.csv\", sep = \",\", index = False)\n",
    "#\n",
    "# # value counts for array\n",
    "# counts_few_shot_claude = pd.Series(few_shot_array_claude).value_counts()\n",
    "# print(counts_few_shot_claude)\n",
    "#\n",
    "# # convert YES to 1 and NO to 0\n",
    "# few_shot_array_claude_val = [1 if response == \"YES\" else 0 for response in few_shot_array_claude]\n",
    "#\n",
    "# # save the array to a csv file\n",
    "# few_shot_df_claude = pd.DataFrame(few_shot_array_claude_val, columns = [\"y_pred\"])\n",
    "# few_shot_df_claude.to_csv(\"../exp/preds_LLMs/Claude/y_pred_claude_few_shot_prompt.csv\", sep = \",\", index = False)\n",
    "#\n",
    "# few_shot_prompt_df_thinking_claude = pd.DataFrame(few_shot_thinking_claude, columns = [\"thinking\"])\n",
    "# few_shot_prompt_df_thinking_claude.to_csv(\"../exp/preds_LLMs/Claude/Thinking/thinking_claude_few_shot_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "c7bd23ac0b2904bb",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:44:35.851914Z",
     "start_time": "2025-05-18T13:44:35.845140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# vignette_array_claude = []\n",
    "# vignette_thinking_claude = []\n",
    "#\n",
    "# client = anthropic.Anthropic(api_key = os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "#\n",
    "# # measure time in seconds\n",
    "# start = time.time()\n",
    "#\n",
    "# # iterate over the test set and save the response for each prompt in an array\n",
    "# for prompt in X_test_vignette_prompt:\n",
    "#     message = client.messages.create(\n",
    "#         model = \"claude-3-7-sonnet-20250219\",\n",
    "#         max_tokens = 20000,\n",
    "#         temperature = 1,\n",
    "#         thinking = {\n",
    "#             \"type\": \"enabled\",\n",
    "#             \"budget_tokens\": 16000\n",
    "#         },\n",
    "#         system = vignette_instruction,\n",
    "#         messages = [\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": [\n",
    "#                     {\n",
    "#                         \"type\": \"text\",\n",
    "#                         \"text\": prompt\n",
    "#                     }\n",
    "#                 ]\n",
    "#             }\n",
    "#         ]\n",
    "#     )\n",
    "#     vignette_array_claude.append(message.content[1].text)\n",
    "#     vignette_thinking_claude.append(message.content[0].thinking)\n",
    "#     print(message.content[1].text)\n",
    "#\n",
    "# end = time.time()\n",
    "# print(f\"Time taken: {end - start} seconds\")\n",
    "# time_claude_vignette = end - start\n",
    "# time_claude_vignette_df = pd.DataFrame({\"time\": [time_claude_vignette]})\n",
    "# time_claude_vignette_df.to_csv(\"../exp/times_LLMs/Claude/time_claude_vignette_prompt.csv\", sep = \",\", index = False)\n",
    "#\n",
    "# # value counts for array\n",
    "# counts_vignette_claude = pd.Series(vignette_array_claude).value_counts()\n",
    "# print(counts_vignette_claude)\n",
    "#\n",
    "# # convert YES to 1 and NO to 0\n",
    "# vignette_array_claude_val = [1 if response == \"YES\" else 0 for response in vignette_array_claude]\n",
    "#\n",
    "# # save the array to a csv file\n",
    "# vignette_df_claude = pd.DataFrame(vignette_array_claude_val, columns = [\"y_pred\"])\n",
    "# vignette_df_claude.to_csv(\"../exp/preds_LLMs/Claude/y_pred_claude_vignette_prompt.csv\", sep = \",\", index = False)\n",
    "#\n",
    "# vignette_prompt_df_thinking_claude = pd.DataFrame(vignette_thinking_claude, columns = [\"thinking\"])\n",
    "# vignette_prompt_df_thinking_claude.to_csv(\"../exp/preds_LLMs/Claude/Thinking/thinking_claude_vignette_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "c052ee81007a6a1f",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:47:49.232761Z",
     "start_time": "2025-05-18T13:46:59.579832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "claude_prompt_array_claude = []\n",
    "claude_prompt_explanation_claude = []\n",
    "claude_prompt_thinking_claude = []\n",
    "\n",
    "client = anthropic.Anthropic(api_key = os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_claude_prompt:\n",
    "    message = client.messages.create(\n",
    "        model = \"claude-3-7-sonnet-20250219\",\n",
    "        max_tokens = 20000,\n",
    "        temperature = 1,\n",
    "        thinking = {\n",
    "            \"type\": \"enabled\",\n",
    "            \"budget_tokens\": 16000\n",
    "        },\n",
    "        system = claude_instruction,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    try:\n",
    "        prediction = re.findall(r'Prediction: (.*)', message.content[1].text)[0]\n",
    "        explanation = re.findall(r'Explanation: (.*)', message.content[1].text)[0]\n",
    "        claude_prompt_array_claude.append(prediction)\n",
    "        claude_prompt_explanation_claude.append(explanation)\n",
    "        claude_prompt_thinking_claude.append(message.content[0].thinking)\n",
    "        print(prediction)\n",
    "    except IndexError:\n",
    "        print(\"IndexError\")\n",
    "        claude_prompt_array_claude.append(\"IndexError\")\n",
    "        claude_prompt_explanation_claude.append(\"IndexError\")\n",
    "        claude_prompt_thinking_claude.append(\"IndexError\")\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_claude_claude_prompt = end - start\n",
    "time_claude_claude_prompt_df = pd.DataFrame({\"time\": [time_claude_claude_prompt]})\n",
    "time_claude_claude_prompt_df.to_csv(\"../exp/times_LLMs/Claude/time_claude_claude_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_claude_prompt_claude = pd.Series(claude_prompt_array_claude).value_counts()\n",
    "print(counts_claude_prompt_claude)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "claude_prompt_array_claude_val = [1 if response == \"YES\" else 0 for response in claude_prompt_array_claude]\n",
    "\n",
    "# save the array to a csv file\n",
    "claude_prompt_df_claude = pd.DataFrame(claude_prompt_array_claude_val, columns = [\"y_pred\"])\n",
    "claude_prompt_df_claude.to_csv(\"../exp/preds_LLMs/Claude/y_pred_claude_claude_prompt_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "claude_prompt_prompt_df_thinking_claude = pd.DataFrame(claude_prompt_thinking_claude, columns = [\"thinking\"])\n",
    "claude_prompt_prompt_df_thinking_claude.to_csv(\"../exp/preds_LLMs/Claude/Thinking/thinking_claude_claude_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "claude_prompt_prompt_df_explanation_claude = pd.DataFrame(claude_prompt_explanation_claude, columns = [\"thinking\"])\n",
    "claude_prompt_prompt_df_explanation_claude.to_csv(\"../exp/preds_LLMs/Claude/Thinking/explanation_claude_claude_prompt.csv\", sep = \",\", index = False)\n"
   ],
   "id": "50df74bd8286714",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n",
      "YES\n",
      "Time taken: 49.63070583343506 seconds\n",
      "NO     1\n",
      "YES    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.6 DeepSeek",
   "id": "67c94f3e4d544889"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T10:07:39.080415Z",
     "start_time": "2025-05-16T10:07:39.077254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# client = OpenAI(api_key = os.environ.get(\"DeepSeek_API_Key\"), base_url = \"https://api.deepseek.com\")\n",
    "#\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"deepseek-chat\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "#         {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "#     ],\n",
    "#     stream=False\n",
    "# )\n",
    "#\n",
    "# print(response.choices[0].message.content)"
   ],
   "id": "72f8f909a923d996",
   "outputs": [],
   "execution_count": 86
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
