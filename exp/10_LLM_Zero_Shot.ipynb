{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LLM: Zero-shot classification through LLMs and prompts",
   "id": "dd0560725aae3b41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this section, I will use the prompts created in the previous section to **classify the test set using different LLMs**. The LLMs will be used to classify whether a person develops a psychological disorder between time point T1 and T2.",
   "id": "93304bd561ba3999"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Models**:\n",
    "\n",
    "- ChatGPT (OpenAI)\n",
    "- Gemini (Google)\n",
    "- Gemma (Google)\n",
    "- Llama (Meta)\n",
    "- Claude (Anthropic)\n",
    "- DeepSeek\n",
    "- Grok (xAI)\n",
    "- Mistral ?"
   ],
   "id": "d7a620a322cdfbae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0 Imports",
   "id": "cc42939b0f2e6112"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:13:59.530975Z",
     "start_time": "2025-05-22T10:13:59.527425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import anthropic\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from openai import OpenAI\n",
    "# from mistralai import Mistral\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "2aaae4e734b1e3c4",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:14:00.088429Z",
     "start_time": "2025-05-22T10:14:00.076588Z"
    }
   },
   "cell_type": "code",
   "source": "data_change = pd.read_csv(\"../dat/dips/DIPS_Data_cleaned_change.csv\", sep = \",\", low_memory = False)",
   "id": "2b5d6a870c836ffc",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:14:00.700268Z",
     "start_time": "2025-05-22T10:14:00.619454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import prompts for all test data\n",
    "X_test_simple_prompt_df = pd.read_csv(\"../dat/prompts/X_test_simple_prompt.csv\", sep = \",\", index_col = 0)\n",
    "X_test_class_definitions_prompt_df = pd.read_csv(\"../dat/prompts/X_test_class_definitions_prompt.csv\", sep = \",\", index_col = 0)\n",
    "X_test_profiled_simple_prompt_df = pd.read_csv(\"../dat/prompts/X_test_profiled_simple_prompt.csv\", sep = \",\", index_col = 0)\n",
    "X_test_few_shot_prompt_df = pd.read_csv(\"../dat/prompts/X_test_few_shot_prompt.csv\", sep = \",\", index_col = 0)\n",
    "X_test_vignette_prompt_df = pd.read_csv(\"../dat/prompts/X_test_vignette_prompt.csv\", sep = \",\", index_col = 0)\n",
    "X_test_thinking_prompt_df = pd.read_csv(\"../dat/prompts/X_test_thinking_prompt.csv\", sep = \",\", index_col = 0)"
   ],
   "id": "6f52bfed5f097045",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:14:01.309706Z",
     "start_time": "2025-05-22T10:14:01.305179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert to arrays\n",
    "X_test_simple_prompt = X_test_simple_prompt_df.values.flatten()\n",
    "X_test_class_definitions_prompt = X_test_class_definitions_prompt_df.values.flatten()\n",
    "X_test_profiled_simple_prompt = X_test_profiled_simple_prompt_df.values.flatten()\n",
    "X_test_few_shot_prompt = X_test_few_shot_prompt_df.values.flatten()\n",
    "X_test_vignette_prompt = X_test_vignette_prompt_df.values.flatten()\n",
    "X_test_thinking_prompt = X_test_thinking_prompt_df.values.flatten()"
   ],
   "id": "817be744e50093b5",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:14:02.010376Z",
     "start_time": "2025-05-22T10:14:01.990942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import instructions\n",
    "simple_instruction_df = pd.read_csv(\"../dat/instructions/simple_instruction.csv\", sep = \",\", index_col = 0)\n",
    "class_definitions_instruction_df = pd.read_csv(\"../dat/instructions/class_definitions_instruction.csv\", sep = \",\", index_col = 0)\n",
    "profiled_simple_instruction_df = pd.read_csv(\"../dat/instructions/profiled_simple_instruction.csv\", sep = \",\", index_col = 0)\n",
    "few_shot_instruction_df = pd.read_csv(\"../dat/instructions/few_shot_instruction.csv\", sep = \",\", index_col = 0)\n",
    "vignette_instruction_df = pd.read_csv(\"../dat/instructions/vignette_instruction.csv\", sep = \",\", index_col = 0)\n",
    "thinking_instruction_df = pd.read_csv(\"../dat/instructions/thinking_instruction.csv\", sep = \",\", index_col = 0)"
   ],
   "id": "42733c2134765065",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:14:02.609355Z",
     "start_time": "2025-05-22T10:14:02.605328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert to string\n",
    "simple_instruction = simple_instruction_df[\"0\"].iloc[0]\n",
    "class_definitions_instruction = class_definitions_instruction_df[\"0\"].iloc[0]\n",
    "profiled_simple_instruction = profiled_simple_instruction_df[\"0\"].iloc[0]\n",
    "few_shot_instruction = few_shot_instruction_df[\"0\"].iloc[0]\n",
    "vignette_instruction = vignette_instruction_df[\"0\"].iloc[0]\n",
    "thinking_instruction = thinking_instruction_df[\"0\"].iloc[0]"
   ],
   "id": "4e019f39a07fab7f",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:15:01.290158Z",
     "start_time": "2025-05-22T10:15:01.282477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import retry instructions when output format was wrong\n",
    "retry_instruction_df = pd.read_csv(\"../dat/instructions/retry_instruction.csv\", sep = \",\", index_col = 0)\n",
    "retry_thinking_instruction_df = pd.read_csv(\"../dat/instructions/retry_thinking_instruction.csv\", sep = \",\", index_col = 0)\n",
    "\n",
    "# import instruction for reason of misclassification\n",
    "instruction_reason_df = pd.read_csv(\"../dat/instructions/instruction_reason.csv\", sep=\",\", index_col = 0)"
   ],
   "id": "5ca9c69ef612359e",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:15:01.784693Z",
     "start_time": "2025-05-22T10:15:01.780670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert to string\n",
    "retry_instruction = retry_instruction_df[\"0\"].iloc[0]\n",
    "retry_thinking_instruction = retry_thinking_instruction_df[\"0\"].iloc[0]\n",
    "\n",
    "instruction_reason = instruction_reason_df[\"0\"].iloc[0]"
   ],
   "id": "c61fdd29c05a02be",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:15:02.284092Z",
     "start_time": "2025-05-22T10:15:02.275955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# predictors\n",
    "X = data_change\n",
    "X = X.drop([\"hpi\"], axis = 1)\n",
    "\n",
    "# target\n",
    "y = data_change[\"hpi\"]\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "\n",
    "print(\"LLMs \\n\",\n",
    "      \"X_train shape: \", X_train.shape, round(X_train.shape[0]/len(X), 2), \"\\n\",\n",
    "      \"X_test shape: \", X_test.shape, round(X_test.shape[0]/len(X), 2),  \"\\n\",\n",
    "      \"y_train shape: \", y_train.shape, round(y_train.shape[0]/len(y), 2), \"\\n\",\n",
    "      \"y_test shape: \", y_test.shape, round(y_test.shape[0]/len(y), 2), \"\\n\")"
   ],
   "id": "fdfa4f872ac4f0e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs \n",
      " X_train shape:  (983, 22) 0.8 \n",
      " X_test shape:  (246, 22) 0.2 \n",
      " y_train shape:  (983,) 0.8 \n",
      " y_test shape:  (246,) 0.2 \n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b5e409ae0b81dc9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1 ChatGPT (OpenAI)",
   "id": "7c184484efa02be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1 Testing prompting",
   "id": "b3181ef2a9c5ad83"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-22T10:09:54.982403Z",
     "start_time": "2025-05-22T10:09:54.979358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# client = OpenAI(\n",
    "#     api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    "# )\n",
    "#\n",
    "# # testing\n",
    "# response = client.responses.create(\n",
    "#     model = \"gpt-4o-mini\",\n",
    "#     instructions = \"You are a coding assistant that talks like a pirate.\",\n",
    "#     input = \"How do I check if a Python object is an instance of a class?\",\n",
    "# )\n",
    "#\n",
    "# print(response.output_text)"
   ],
   "id": "738fb998b35415c6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f96758305e52ce67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2 Prompting with ChatGPT-3o",
   "id": "34bf1926b6e19c7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Simple prompt",
   "id": "b8096aaae45a8b7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T11:12:10.057657Z",
     "start_time": "2025-05-22T11:12:05.847469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "simple_y_pred_GPT = []\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_simple_prompt:\n",
    "    response = client.responses.create(\n",
    "        # model = \"o3-2025-04-16\",\n",
    "        model = \"gpt-4o\",\n",
    "        instructions = simple_instruction,\n",
    "        input = prompt\n",
    "    )\n",
    "\n",
    "    if response.output_text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.responses.create(\n",
    "            model = \"gpt-4o\",\n",
    "            instructions = retry_instruction,\n",
    "            input = prompt\n",
    "        )\n",
    "\n",
    "    simple_y_pred_GPT.append(response.output_text)\n",
    "    print(response.output_text)\n",
    "\n",
    "    # save responses to csv after every 50th prompt\n",
    "    if len(simple_y_pred_GPT) % 50 == 0:\n",
    "        print(\"\\n \\n prompt\", len(simple_y_pred_GPT))\n",
    "        # value counts for array\n",
    "        counts_simple_GPT = pd.Series(simple_y_pred_GPT).value_counts()\n",
    "        print(counts_simple_GPT)\n",
    "\n",
    "        # convert YES to 1 and NO to 0\n",
    "        simple_y_pred_GPT = [1 if response == \"YES\" else 0 if response == \"NO\" else np.nan for response in simple_y_pred_GPT]\n",
    "\n",
    "        # save the array to a csv file\n",
    "        simple_prompt_df_GPT = pd.DataFrame(simple_y_pred_GPT, columns = [\"y_pred\"])\n",
    "        simple_prompt_df_GPT.to_csv(\"../exp/y_pred_LLMs/GPT/y_pred_GPT_simple_prompt.csv\", sep = \",\", index = False)\n",
    "        print(\"\\n \\n csv saved \\n \\n\")\n",
    "\n",
    "    # # ask for reason if misclassified\n",
    "    # y_pred = 1 if response.output_text == \"YES\" else 0 if response.output_text == \"NO\" else np.nan\n",
    "    # y_true = y_test.iloc[i]\n",
    "    #\n",
    "    # if y_pred != y_true:\n",
    "    #     prompt = f\"Predicted label: {y_pred}\\nTrue label: {y_true}, Prompt: {prompt}\"\n",
    "    #\n",
    "    #     reason_miscl = client.responses.create(\n",
    "    #         model = \"gpt-4o\",\n",
    "    #         instructions = instruction_reason,\n",
    "    #         input = prompt\n",
    "    #     )\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_GPT_simple_prompt = end - start\n",
    "time_GPT_simple_prompt_df = pd.DataFrame({\"time\": [time_GPT_simple_prompt]})\n",
    "time_GPT_simple_prompt_df.to_csv(\"../exp/times_LLMs/GPT/time_GPT_simple_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_simple_GPT = pd.Series(simple_y_pred_GPT).value_counts()\n",
    "print(counts_simple_GPT)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "simple_y_pred_GPT = [1 if response == \"YES\" else 0 if response == \"NO\" else np.nan for response in simple_y_pred_GPT]\n",
    "\n",
    "# save the array to a csv file\n",
    "simple_prompt_df_GPT = pd.DataFrame(simple_y_pred_GPT, columns = [\"y_pred\"])\n",
    "simple_prompt_df_GPT.to_csv(\"../exp/y_pred_LLMs/GPT/y_pred_GPT_simple_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "bff465ba605628f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n",
      "YES\n",
      "Time taken: 4.180484771728516 seconds\n",
      "YES    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Class definition prompt",
   "id": "54e68b140aa08d95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:48.059799Z",
     "start_time": "2025-05-18T13:53:48.055372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_def_y_pred_GPT = []\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_class_definitions_prompt:\n",
    "    response = client.responses.create(\n",
    "        model = \"o3-2025-04-16\",\n",
    "        instructions = class_definitions_instruction,\n",
    "        input = prompt\n",
    "    )\n",
    "\n",
    "    if response.output_text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.responses.create(\n",
    "            model = \"o3-2025-04-16\",\n",
    "            instructions = retry_instruction,\n",
    "            input = prompt\n",
    "        )\n",
    "\n",
    "    class_def_y_pred_GPT.append(response.output_text)\n",
    "    print(response.output_text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_GPT_class_definitions = end - start\n",
    "time_GPT_class_definitions_df = pd.DataFrame({\"time\": [time_GPT_class_definitions]})\n",
    "time_GPT_class_definitions_df.to_csv(\"../exp/times_LLMs/GPT/time_GPT_class_definitions_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_class_def_GPT = pd.Series(class_def_y_pred_GPT).value_counts()\n",
    "print(counts_class_def_GPT)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "class_def_y_pred_GPT = [1 if response == \"YES\" else 0 for response in class_def_y_pred_GPT]\n",
    "\n",
    "# save the array to a csv file\n",
    "class_def_df_GPT = pd.DataFrame(class_def_y_pred_GPT, columns = [\"y_pred\"])\n",
    "class_def_df_GPT.to_csv(\"../exp/y_pred_LLMs/GPT/y_pred_GPT_class_definitions_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "63f70e77e37a9919",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Profiled simple prompt",
   "id": "e957dc1967268203"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:48.434204Z",
     "start_time": "2025-05-18T13:53:48.430468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "profiled_simple_y_pred_GPT = []\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_profiled_simple_prompt:\n",
    "    response = client.responses.create(\n",
    "        model = \"o3-2025-04-16\",\n",
    "        instructions = profiled_simple_instruction,\n",
    "        input = prompt\n",
    "    )\n",
    "\n",
    "    if response.output_text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.responses.create(\n",
    "            model = \"o3-2025-04-16\",\n",
    "            instructions = retry_instruction,\n",
    "            input = prompt\n",
    "        )\n",
    "\n",
    "    profiled_simple_y_pred_GPT.append(response.output_text)\n",
    "    print(response.output_text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_GPT_profiled_simple = end - start\n",
    "time_GPT_profiled_simple_df = pd.DataFrame({\"time\": [time_GPT_profiled_simple]})\n",
    "time_GPT_profiled_simple_df.to_csv(\"../exp/times_LLMs/GPT/time_GPT_profiled_simple_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_profiled_simple_GPT = pd.Series(profiled_simple_y_pred_GPT).value_counts()\n",
    "print(counts_profiled_simple_GPT)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "profiled_simple_y_pred_GPT_val = [1 if response == \"YES\" else 0 for response in profiled_simple_y_pred_GPT]\n",
    "\n",
    "# save the array to a csv file\n",
    "profiled_simple_df_GPT = pd.DataFrame(profiled_simple_y_pred_GPT_val, columns = [\"y_pred\"])\n",
    "profiled_simple_df_GPT.to_csv(\"../exp/y_pred_LLMs/GPT/y_pred_GPT_profiled_simple_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "8fff5eef0784f0bf",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Few shot prompt",
   "id": "443ef531a747e22e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:48.801391Z",
     "start_time": "2025-05-18T13:53:48.796663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "few_shot_y_pred_GPT = []\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_few_shot_prompt:\n",
    "    response = client.responses.create(\n",
    "        model = \"o3-2025-04-16\",\n",
    "        instructions = few_shot_instruction,\n",
    "        input = prompt\n",
    "    )\n",
    "\n",
    "    if response.output_text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.responses.create(\n",
    "            model = \"o3-2025-04-16\",\n",
    "            instructions = retry_instruction,\n",
    "            input = prompt\n",
    "        )\n",
    "\n",
    "    few_shot_y_pred_GPT.append(response.output_text)\n",
    "    print(response.output_text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_GPT_few_shot = end - start\n",
    "time_GPT_few_shot_df = pd.DataFrame({\"time\": [time_GPT_few_shot]})\n",
    "time_GPT_few_shot_df.to_csv(\"../exp/times_LLMs/GPT/time_GPT_few_shot_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_few_shot_GPT = pd.Series(few_shot_y_pred_GPT).value_counts()\n",
    "print(counts_few_shot_GPT)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "few_shot_y_pred_GPT_val = [1 if response == \"YES\" else 0 for response in few_shot_y_pred_GPT]\n",
    "\n",
    "# save the array to a csv file\n",
    "few_shot_df_GPT = pd.DataFrame(few_shot_y_pred_GPT_val, columns = [\"y_pred\"])\n",
    "few_shot_df_GPT.to_csv(\"../exp/y_pred_LLMs/GPT/y_pred_GPT_few_shot_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "f7389ea105f9761b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Vignette prompt",
   "id": "5e32f09b1cf6622"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:49.144140Z",
     "start_time": "2025-05-18T13:53:49.140437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vignette_y_pred_GPT = []\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_vignette_prompt:\n",
    "    response = client.responses.create(\n",
    "        model = \"o3-2025-04-16\",\n",
    "        instructions = vignette_instruction,\n",
    "        input = prompt\n",
    "    )\n",
    "\n",
    "    if response.output_text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.responses.create(\n",
    "            model = \"o3-2025-04-16\",\n",
    "            instructions = retry_instruction,\n",
    "            input = prompt\n",
    "        )\n",
    "\n",
    "    vignette_y_pred_GPT.append(response.output_text)\n",
    "    print(response.output_text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_GPT_vignette = end - start\n",
    "time_GPT_vignette_df = pd.DataFrame({\"time\": [time_GPT_vignette]})\n",
    "time_GPT_vignette_df.to_csv(\"../exp/times_LLMs/GPT/time_GPT_vignette_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_vignette_GPT = pd.Series(vignette_y_pred_GPT).value_counts()\n",
    "print(counts_vignette_GPT)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "vignette_y_pred_GPT_val = [1 if response == \"YES\" else 0 for response in vignette_y_pred_GPT]\n",
    "\n",
    "# save the array to a csv file\n",
    "vignette_df_GPT = pd.DataFrame(vignette_y_pred_GPT_val, columns = [\"y_pred\"])\n",
    "vignette_df_GPT.to_csv(\"../exp/y_pred_LLMs/GPT/y_pred_GPT_vignette_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "11d6cfb1d391ec89",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Thinking prompt",
   "id": "bf82f46bc3b55c0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:26:44.929493Z",
     "start_time": "2025-05-22T10:26:42.891208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "thinking_y_pred_GPT = []\n",
    "thinking_explanation_GPT = []\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_thinking_prompt:\n",
    "    response = client.responses.create(\n",
    "        model = \"o3-2025-04-16\",\n",
    "        instructions = thinking_instruction,\n",
    "        input = prompt\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        prediction = re.findall(r'Prediction: (.*)', response.output_text)[0].strip()\n",
    "        explanation = re.findall(r'Explanation: (.*)', response.output_text)[0].strip()\n",
    "        thinking_y_pred_GPT.append(prediction)\n",
    "        thinking_explanation_GPT.append(explanation)\n",
    "        print(prediction)\n",
    "    except IndexError:\n",
    "        print(\"IndexError\")\n",
    "        thinking_y_pred_GPT.append(\"IndexError\")\n",
    "        thinking_explanation_GPT.append(\"IndexError\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_GPT_thinking = end - start\n",
    "time_GPT_thinking_df = pd.DataFrame({\"time\": [time_GPT_thinking]})\n",
    "time_GPT_thinking_df.to_csv(\"../exp/times_LLMs/GPT/time_GPT_thinking_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_thinking_GPT = pd.Series(thinking_y_pred_GPT).value_counts()\n",
    "print(counts_thinking_GPT)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "thinking_y_pred_GPT_val = [1 if response == \"YES\" else 0 for response in thinking_y_pred_GPT]\n",
    "\n",
    "# save the array to a csv file\n",
    "thinking_df_GPT = pd.DataFrame(thinking_y_pred_GPT_val, columns = [\"y_pred\"])\n",
    "thinking_df_GPT.to_csv(\"../exp/y_pred_LLMs/GPT/y_pred_GPT_thinking_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "thinking_df_explanation_GPT = pd.DataFrame(thinking_explanation_GPT, columns = [\"thinking\"])\n",
    "thinking_df_explanation_GPT.to_csv(\"../exp/y_pred_LLMs/GPT/explanation_GPT_thinking_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "f4434928c4a30385",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n",
      "Time taken: 2.0060501098632812 seconds\n",
      "NO    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.3 Misclassified cases reasons",
   "id": "1af0af9d4db4cb1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:49.508806Z",
     "start_time": "2025-05-18T13:53:49.505727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# y_pred_GPT_simple_prompt = pd.read_csv(\"../exp/y_pred_LLMs/y_pred_GPT_simple_prompt.csv\", sep = \",\")\n",
    "# y_pred_GPT_class_definition_prompt = pd.read_csv(\"../exp/y_pred_LLMs/y_pred_GPT_class_definitions_prompt.csv\", sep = \",\")\n",
    "# y_pred_GPT_profiled_simple_prompt = pd.read_csv(\"../exp/y_pred_LLMs/y_pred_GPT_profiled_simple_prompt.csv\", sep = \",\")\n",
    "# y_pred_GPT_few_shot_prompt = pd.read_csv(\"../exp/y_pred_LLMs/y_pred_GPT_few_shot_prompt.csv\", sep = \",\")\n",
    "# y_pred_GPT_vignette_prompt = pd.read_csv(\"../exp/y_pred_LLMs/y_pred_GPT_vignette_prompt.csv\", sep = \",\")\n",
    "#\n",
    "# # convert to array\n",
    "# y_pred_GPT_simple_prompt = y_pred_GPT_simple_prompt[\"y_pred\"].to_numpy()\n",
    "# y_pred_GPT_class_definition_prompt = y_pred_GPT_class_definition_prompt[\"y_pred\"].to_numpy()\n",
    "# y_pred_GPT_profiled_simple_prompt = y_pred_GPT_profiled_simple_prompt[\"y_pred\"].to_numpy()\n",
    "# y_pred_GPT_few_shot_prompt = y_pred_GPT_few_shot_prompt[\"y_pred\"].to_numpy()\n",
    "# y_pred_GPT_vignette_prompt = y_pred_GPT_vignette_prompt[\"y_pred\"].to_numpy()"
   ],
   "id": "187e592c37102291",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:49.869783Z",
     "start_time": "2025-05-18T13:53:49.866035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # indentify misclassified cases by comparing y_pred_GPT_XXX and y_test, save index\n",
    "# misclassified_cases_simple = []\n",
    "# misclassified_cases_class_def = []\n",
    "# misclassified_cases_profiled_simple = []\n",
    "# misclassified_cases_few_shot = []\n",
    "# misclassified_cases_vignette = []\n",
    "#\n",
    "# for i in range(len(y_pred_GPT_simple_prompt)):\n",
    "#     if y_pred_GPT_simple_prompt[i] != y_test.iloc[i]:\n",
    "#         misclassified_cases_simple.append(i)\n",
    "# total_cases_simple = len(y_pred_GPT_simple_prompt)\n",
    "# misscl_cases_simple = len(misclassified_cases_simple)\n",
    "# correct_clases_simple = total_cases_simple - misscl_cases_simple\n",
    "#\n",
    "# for i in range(len(y_pred_GPT_class_definition_prompt)):\n",
    "#     if y_pred_GPT_class_definition_prompt[i] != y_test.iloc[i]:\n",
    "#         misclassified_cases_class_def.append(i)\n",
    "# total_cases_class_def = len(y_pred_GPT_class_definition_prompt)\n",
    "# misscl_cases_class_def = len(misclassified_cases_class_def)\n",
    "# correct_clases_class_def = total_cases_class_def - misscl_cases_class_def\n",
    "#\n",
    "# for i in range(len(y_pred_GPT_profiled_simple_prompt)):\n",
    "#     if y_pred_GPT_profiled_simple_prompt[i] != y_test.iloc[i]:\n",
    "#         misclassified_cases_profiled_simple.append(i)\n",
    "# total_cases_profiled = len(y_pred_GPT_profiled_simple_prompt)\n",
    "# misscl_cases_profiled = len(misclassified_cases_profiled_simple)\n",
    "# correct_clases_profiled = total_cases_profiled - misscl_cases_profiled\n",
    "#\n",
    "# for i in range(len(y_pred_GPT_few_shot_prompt)):\n",
    "#     if y_pred_GPT_few_shot_prompt[i] != y_test.iloc[i]:\n",
    "#         misclassified_cases_few_shot.append(i)\n",
    "# total_cases_few_shot = len(y_pred_GPT_few_shot_prompt)\n",
    "# misscl_cases_few_shot = len(misclassified_cases_few_shot)\n",
    "# correct_clases_few_shot = total_cases_few_shot - misscl_cases_few_shot\n",
    "#\n",
    "# for i in range(len(y_pred_GPT_vignette_prompt)):\n",
    "#     if y_pred_GPT_vignette_prompt[i] != y_test.iloc[i]:\n",
    "#         misclassified_cases_vignette.append(i)\n",
    "# total_cases_vignette = len(y_pred_GPT_vignette_prompt)\n",
    "# misscl_cases_vignette = len(misclassified_cases_vignette)\n",
    "# correct_clases_vignette = total_cases_vignette - misscl_cases_vignette"
   ],
   "id": "3aa9a0da4c196d87",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:50.227113Z",
     "start_time": "2025-05-18T13:53:50.221584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # save as df with total, correct and missclassified cases\n",
    "# simple_cases_df = pd.DataFrame({\"total\": [total_cases_simple], \"correct\": [correct_clases_simple], \"missclassified\": [misscl_cases_simple]})\n",
    "# simple_cases_df.to_csv(\"../exp/reasons_miscl_LLMs/GPT/simple_cases_GPT_df.csv\", sep = \",\", index = True)\n",
    "#\n",
    "# class_def_cases_df = pd.DataFrame({\"total\": [total_cases_class_def], \"correct\": [correct_clases_class_def], \"missclassified\": [misscl_cases_class_def]})\n",
    "# class_def_cases_df.to_csv(\"../exp/reasons_miscl_LLMs/GPT/class_def_cases_GPT_df.csv\", sep = \",\", index = True)\n",
    "#\n",
    "# profiled_cases_df = pd.DataFrame({\"total\": [total_cases_profiled], \"correct\": [correct_clases_profiled], \"missclassified\": [misscl_cases_profiled]})\n",
    "# profiled_cases_df.to_csv(\"../exp/reasons_miscl_LLMs/GPT/profiled_cases_GPT_df.csv\", sep = \",\", index = True)\n",
    "#\n",
    "# few_shot_cases_df = pd.DataFrame({\"total\": [total_cases_few_shot], \"correct\": [correct_clases_few_shot], \"missclassified\": [misscl_cases_few_shot]})\n",
    "# few_shot_cases_df.to_csv(\"../exp/reasons_miscl_LLMs/GPT/few_shot_cases_GPT_df.csv\", sep = \",\", index = True)\n",
    "#\n",
    "# vignette_cases_df = pd.DataFrame({\"total\": [total_cases_vignette], \"correct\": [correct_clases_vignette], \"missclassified\": [misscl_cases_vignette]})\n",
    "# vignette_cases_df.to_csv(\"../exp/reasons_miscl_LLMs/GPT/vignette_cases_GPT_df.csv\", sep = \",\", index = True)\n"
   ],
   "id": "7b6031706cb820c3",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:50.630900Z",
     "start_time": "2025-05-18T13:53:50.625301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# simple_prompt_reasons = []\n",
    "# class_def_prompt_reasons = []\n",
    "# profiled_simple_prompt_reasons = []\n",
    "# few_shot_prompt_reasons = []\n",
    "# vignette_prompt_reasons = []\n",
    "#\n",
    "# client = OpenAI(\n",
    "#     api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
    "# )\n",
    "#\n",
    "# instruction_reason = \"Please categorize why you misclassified the data. Respond only with the following categories as reasons for the misclassification in order to improve prompting. Possible categories are: \\nLack of context (emphasize or indicate the context of the query), \\nLack of examples (few-shot prompting with several examples of appropriate responses are shown before posing the actual question missing), \\nLack of feedback (interactive refining the prompt), \\nLack of counterfactual demonstrations (instances containing false facts to improve faithfulness in knowledge conflict situations), \\nLack of opinion-based information (reframe the context as a narrator’s statement and inquire about the narrator’s opinions), \\nKnowledge conflicts (memorized facts became outdated and counterfactual facts), \\nPrediction with Abstention (model is uncertain about their y_pred_ML) \\n \\n Do not mention specific change (e.g., increase or decrease) in predictors, do not go into detail of this specific case and do not repeat the question. Only respond with one or multiple of the categories as reasons for the misclassification, separated by ','. Mention the most important category first.\"\n",
    "#\n",
    "# # iterate over the misclassified cases and save the response for each prompt in an array\n",
    "# print(\"Simple prompt: \\n \\n\")\n",
    "# for i in misclassified_cases_simple:\n",
    "#     response = client.responses.create(\n",
    "#         model = \"gpt-4o\",\n",
    "#         instructions = instruction_reason,\n",
    "#         input = f\"Misclassified case {i}: Prompt: {X_test_simple_prompt[i]} Response: {y_pred_GPT_simple_prompt[i]} True label: {y_test.iloc[i]}\"\n",
    "#     )\n",
    "#     simple_prompt_reasons.append(response.output_text)\n",
    "#     print(response.output_text)\n",
    "#\n",
    "# print(\"\\n \\n Class definition prompt: \\n \\n\")\n",
    "# for i in misclassified_cases_class_def:\n",
    "#     response = client.responses.create(\n",
    "#         model = \"gpt-4o\",\n",
    "#         instructions = instruction_reason,\n",
    "#         input = f\"Misclassified case {i}: Prompt: {X_test_class_definitions_prompt[i]} Response: {y_pred_GPT_class_definition_prompt[i]} True label: {y_test.iloc[i]}\"\n",
    "#     )\n",
    "#     class_def_prompt_reasons.append(response.output_text)\n",
    "#     print(response.output_text)\n",
    "#\n",
    "# print(\"\\n \\n Profiled simple prompt: \\n \\n\")\n",
    "# for i in misclassified_cases_profiled_simple:\n",
    "#     response = client.responses.create(\n",
    "#         model = \"gpt-4o\",\n",
    "#         instructions = instruction_reason,\n",
    "#         input = f\"Misclassified case {i}: Prompt: {X_test_profiled_simple_prompt[i]} Response: {y_pred_GPT_profiled_simple_prompt[i]} True label: {y_test.iloc[i]}\"\n",
    "#     )\n",
    "#     profiled_simple_prompt_reasons.append(response.output_text)\n",
    "#     print(response.output_text)\n",
    "#\n",
    "# print(\"\\n \\n Few shot prompt: \\n \\n\")\n",
    "# for i in misclassified_cases_few_shot:\n",
    "#     response = client.responses.create(\n",
    "#         model = \"gpt-4o\",\n",
    "#         instructions = instruction_reason,\n",
    "#         input = f\"Misclassified case {i}: Prompt: {X_test_few_shot_prompt[i]} Response: {y_pred_GPT_few_shot_prompt[i]} True label: {y_test.iloc[i]}\"\n",
    "#     )\n",
    "#     few_shot_prompt_reasons.append(response.output_text)\n",
    "#     print(response.output_text)\n",
    "#\n",
    "# print(\"\\n \\n Vignette prompt: \\n \\n\")\n",
    "# for i in misclassified_cases_vignette:\n",
    "#     response = client.responses.create(\n",
    "#         model = \"gpt-4o\",\n",
    "#         instructions = instruction_reason,\n",
    "#         input = f\"Misclassified case {i}: Prompt: {X_test_vignette_prompt[i]} Response: {y_pred_GPT_vignette_prompt[i]} True label: {y_test.iloc[i]}\"\n",
    "#     )\n",
    "#     vignette_prompt_reasons.append(response.output_text)\n",
    "#     print(response.output_text)"
   ],
   "id": "286fbda7d1c1de5c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:50.985607Z",
     "start_time": "2025-05-18T13:53:50.980125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# all_reasons_simple = []\n",
    "# all_reasons_class_def = []\n",
    "# all_reasons_profiled_simple = []\n",
    "# all_reasons_few_shot = []\n",
    "# all_reasons_vignette = []\n",
    "#\n",
    "# for reason in simple_prompt_reasons:\n",
    "#     reason = reason.split(\", \")\n",
    "#     reason = [re.sub(r'[^A-Za-z\\s]', '', r).strip() for r in reason]\n",
    "#     all_reasons_simple.append(reason)\n",
    "#\n",
    "# for reason in class_def_prompt_reasons:\n",
    "#     reason = reason.split(\", \")\n",
    "#     reason = [re.sub(r'[^A-Za-z\\s]', '', r).strip() for r in reason]\n",
    "#     all_reasons_class_def.append(reason)\n",
    "#\n",
    "# for reason in profiled_simple_prompt_reasons:\n",
    "#     reason = reason.split(\", \")\n",
    "#     reason = [re.sub(r'[^A-Za-z\\s]', '', r).strip() for r in reason]\n",
    "#     all_reasons_profiled_simple.append(reason)\n",
    "#\n",
    "# for reason in few_shot_prompt_reasons:\n",
    "#     reason = reason.split(\", \")\n",
    "#     reason = [re.sub(r'[^A-Za-z\\s]', '', r).strip() for r in reason]\n",
    "#     all_reasons_few_shot.append(reason)\n",
    "#\n",
    "# for reason in vignette_prompt_reasons:\n",
    "#     reason = reason.split(\", \")\n",
    "#     reason = [re.sub(r'[^A-Za-z\\s]', '', r).strip() for r in reason]\n",
    "#     all_reasons_vignette.append(reason)"
   ],
   "id": "eda04f93a440ac40",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:51.324006Z",
     "start_time": "2025-05-18T13:53:51.319540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# simple_prompt_reasons_dict = {}\n",
    "# class_def_prompt_reasons_dict = {}\n",
    "# profiled_simple_prompt_reasons_dict = {}\n",
    "# few_shot_prompt_reasons_dict = {}\n",
    "# vignette_prompt_reasons_dict = {}\n",
    "#\n",
    "# for i in all_reasons_simple:\n",
    "#     for j in i:\n",
    "#         # count the occurrences of each reason\n",
    "#         if j in simple_prompt_reasons_dict:\n",
    "#             simple_prompt_reasons_dict[j] += 1\n",
    "#         else:\n",
    "#             simple_prompt_reasons_dict[j] = 1\n",
    "# simple_prompt_reasons_df = pd.DataFrame.from_dict(simple_prompt_reasons_dict, orient='index', columns=['count'])\n",
    "# # simple_prompt_reasons_df.to_csv(\"../exp/reasons_miscl_LLMs/GPT/simple_prompt_reasons.csv\", sep = \",\", index = True)\n",
    "#\n",
    "#\n",
    "# for i in all_reasons_class_def:\n",
    "#     for j in i:\n",
    "#         # count the occurrences of each reason\n",
    "#         if j in class_def_prompt_reasons_dict:\n",
    "#             class_def_prompt_reasons_dict[j] += 1\n",
    "#         else:\n",
    "#             class_def_prompt_reasons_dict[j] = 1\n",
    "# class_def_prompt_reasons_df = pd.DataFrame.from_dict(class_def_prompt_reasons_dict, orient='index', columns=['count'])\n",
    "# # class_def_prompt_reasons_df.to_csv(\"../exp/reasons_miscl_LLMs/GPT/class_def_prompt_reasons.csv\", sep = \",\", index = True)\n",
    "#\n",
    "# for i in all_reasons_profiled_simple:\n",
    "#     for j in i:\n",
    "#         # count the occurrences of each reason\n",
    "#         if j in profiled_simple_prompt_reasons_dict:\n",
    "#             profiled_simple_prompt_reasons_dict[j] += 1\n",
    "#         else:\n",
    "#             profiled_simple_prompt_reasons_dict[j] = 1\n",
    "# profiled_simple_prompt_reasons_df = pd.DataFrame.from_dict(profiled_simple_prompt_reasons_dict, orient='index', columns=['count'])\n",
    "# # profiled_simple_prompt_reasons_df.to_csv(\"../exp/reasons_miscl_LLMs/GPT/profiled_simple_prompt_reasons.csv\", sep = \",\", index = True)\n",
    "#\n",
    "#\n",
    "# for i in all_reasons_few_shot:\n",
    "#     for j in i:\n",
    "#         # count the occurrences of each reason\n",
    "#         if j in few_shot_prompt_reasons_dict:\n",
    "#             few_shot_prompt_reasons_dict[j] += 1\n",
    "#         else:\n",
    "#             few_shot_prompt_reasons_dict[j] = 1\n",
    "# few_shot_prompt_reasons_df = pd.DataFrame.from_dict(few_shot_prompt_reasons_dict, orient='index', columns=['count'])\n",
    "# # few_shot_prompt_reasons_df.to_csv(\"../exp/reasons_miscl_LLMs/GPT/few_shot_prompt_reasons.csv\", sep = \",\", index = True)\n",
    "#\n",
    "# for i in all_reasons_vignette:\n",
    "#     for j in i:\n",
    "#         # count the occurrences of each reason\n",
    "#         if j in vignette_prompt_reasons_dict:\n",
    "#             vignette_prompt_reasons_dict[j] += 1\n",
    "#         else:\n",
    "#             vignette_prompt_reasons_dict[j] = 1\n",
    "# vignette_prompt_reasons_df = pd.DataFrame.from_dict(vignette_prompt_reasons_dict, orient='index', columns=['count'])\n",
    "# # vignette_prompt_reasons_df.to_csv(\"../exp/reasons_miscl_LLMs/GPT/vignette_prompt_reasons.csv\", sep = \",\", index = True)"
   ],
   "id": "953f9cd49f772076",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:51.660345Z",
     "start_time": "2025-05-18T13:53:51.657495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(simple_prompt_reasons_dict, \"\\n \\n\")\n",
    "# print(class_def_prompt_reasons_dict, \"\\n \\n\")\n",
    "# print(profiled_simple_prompt_reasons_dict, \"\\n \\n\")\n",
    "# print(few_shot_prompt_reasons_dict, \"\\n \\n\")\n",
    "# print(vignette_prompt_reasons_dict)"
   ],
   "id": "a17c7952d84333ba",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "36a05f87c5fa026e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2 Gemini (Google)",
   "id": "603d4a5c922bb11d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1 Testing prompting",
   "id": "929a34c8bb3628be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:29:17.634781Z",
     "start_time": "2025-05-22T10:28:51.589223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# client = genai.Client(api_key = os.environ.get(\"GEMINI_API_KEY\"))\n",
    "#\n",
    "# response = client.models.generate_content(\n",
    "#     model = \"gemini-2.5-pro-preview-05-06\",\n",
    "#     contents = \"Explain how AI works in a few words\",\n",
    "# )\n",
    "#\n",
    "# print(response.text)"
   ],
   "id": "db7c84d6d0cf7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here are a few options, pick the one you like best:\n",
      "\n",
      "1.  **AI learns from data to make smart decisions.**\n",
      "2.  **Software mimicking human intelligence using patterns in data.**\n",
      "3.  **Computers recognizing patterns to perform tasks.**\n",
      "4.  **AI learns from examples to act or predict.**\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:29:49.192975Z",
     "start_time": "2025-05-22T10:29:41.033984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# client = genai.Client(\n",
    "#     api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "# )\n",
    "#\n",
    "# response = client.models.generate_content(\n",
    "#     model = \"gemini-2.5-pro-preview-05-06\",\n",
    "#     config = types.GenerateContentConfig(system_instruction = simple_instruction),\n",
    "#     contents = X_test_simple_prompt[0]\n",
    "# )\n",
    "#\n",
    "# print(response.text)"
   ],
   "id": "b921335db7bb8d34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 Prompting with Gemini 2.5 Pro",
   "id": "2bbf0494ad4f1765"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Simple prompt",
   "id": "39f3117e08634133"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:36:58.779998Z",
     "start_time": "2025-05-22T10:36:51.180555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_simple_gemini = []\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_simple_prompt:\n",
    "    response = client.models.generate_content(\n",
    "        model = \"gemini-2.5-pro-preview-05-06\",\n",
    "        config = types.GenerateContentConfig(system_instruction = simple_instruction),\n",
    "        contents = prompt,\n",
    "    )\n",
    "\n",
    "    if response.text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.models.generate_content(\n",
    "            model = \"gemini-2.5-pro-preview-05-06\",\n",
    "            config = types.GenerateContentConfig(system_instruction = retry_instruction),\n",
    "            contents = prompt,\n",
    "        )\n",
    "\n",
    "    y_pred_simple_gemini.append(response.text)\n",
    "    print(response.text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_gemini_simple_prompt = end - start\n",
    "time_gemini_simple_prompt_df = pd.DataFrame({\"time\": [time_gemini_simple_prompt]})\n",
    "time_gemini_simple_prompt_df.to_csv(\"../exp/times_LLMs/Gemini/time_gemini_simple_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_simple_gemini = pd.Series(y_pred_simple_gemini).value_counts()\n",
    "print(counts_simple_gemini)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "y_pred_simple_gemini_val = [1 if response == \"YES\" else 0 if response == \"NO\" else np.nan for response in y_pred_simple_gemini]\n",
    "\n",
    "# save the array to a csv file\n",
    "simple_prompt_df_gemini = pd.DataFrame(y_pred_simple_gemini_val, columns = [\"y_pred\"])\n",
    "simple_prompt_df_gemini.to_csv(\"../exp/y_pred_LLMs/Gemini/y_pred_gemini_simple_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "c3e792db40c69e7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n",
      "Time taken: 7.57573127746582 seconds\n",
      "NO    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Class definition prompt",
   "id": "fb8ae78d326953fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:43:17.139643Z",
     "start_time": "2025-05-22T10:40:29.502773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_class_def_gemini = []\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_class_definitions_prompt:\n",
    "    response = client.models.generate_content(\n",
    "        model = \"gemini-2.5-pro-preview-05-06\",\n",
    "        config = types.GenerateContentConfig(system_instruction = class_definitions_instruction),\n",
    "        contents = prompt,\n",
    "    )\n",
    "\n",
    "    if response.text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.models.generate_content(\n",
    "            model = \"gemini-2.5-pro-preview-05-06\",\n",
    "            config = types.GenerateContentConfig(system_instruction = retry_instruction),\n",
    "            contents = prompt,\n",
    "        )\n",
    "\n",
    "    y_pred_class_def_gemini.append(response.text)\n",
    "    print(response.text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_gemini_class_def = end - start\n",
    "time_gemini_class_def_df = pd.DataFrame({\"time\": [time_gemini_class_def]})\n",
    "time_gemini_class_def_df.to_csv(\"../exp/times_LLMs/Gemini/time_gemini_class_definitions_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_class_def_gemini = pd.Series(y_pred_class_def_gemini).value_counts()\n",
    "print(counts_class_def_gemini)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "y_pred_class_def_gemini_val = [1 if response == \"YES\" else 0 for response in y_pred_class_def_gemini]\n",
    "\n",
    "# save the array to a csv file\n",
    "class_def_df_gemini = pd.DataFrame(y_pred_class_def_gemini_val, columns = [\"y_pred\"])\n",
    "class_def_df_gemini.to_csv(\"../exp/y_pred_LLMs/Gemini/y_pred_gemini_class_definitions_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "25e52ad63131ba2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n",
      "NO\n",
      "Time taken: 167.61307907104492 seconds\n",
      "NO    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Profiled simple prompt",
   "id": "ed0088c4c970b8a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:44:59.602168Z",
     "start_time": "2025-05-22T10:43:56.605680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_profiled_simple_gemini = []\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_profiled_simple_prompt:\n",
    "    response = client.models.generate_content(\n",
    "        model = \"gemini-2.5-pro-preview-05-06\",\n",
    "        config = types.GenerateContentConfig(system_instruction = simple_instruction),\n",
    "        contents = prompt,\n",
    "    )\n",
    "\n",
    "    if response.text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.models.generate_content(\n",
    "            model = \"gemini-2.5-pro-preview-05-06\",\n",
    "            config = types.GenerateContentConfig(system_instruction = retry_instruction),\n",
    "            contents = prompt,\n",
    "        )\n",
    "\n",
    "    y_pred_profiled_simple_gemini.append(response.text)\n",
    "    print(response.text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_gemini_profiled_simple = end - start\n",
    "time_gemini_profiled_simple_df = pd.DataFrame({\"time\": [time_gemini_profiled_simple]})\n",
    "time_gemini_profiled_simple_df.to_csv(\"../exp/times_LLMs/Gemini/time_gemini_profiled_simple_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_profiled_simple_gemini = pd.Series(y_pred_profiled_simple_gemini).value_counts()\n",
    "print(counts_profiled_simple_gemini)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "y_pred_profiled_simple_gemini_val = [1 if response == \"YES\" else 0 for response in y_pred_profiled_simple_gemini]\n",
    "\n",
    "# save the array to a csv file\n",
    "profiled_simple_df_gemini = pd.DataFrame(y_pred_profiled_simple_gemini_val, columns = [\"y_pred\"])\n",
    "profiled_simple_df_gemini.to_csv(\"../exp/y_pred_LLMs/Gemini/y_pred_gemini_profiled_simple_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "c367fa11cdeeae7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n",
      "YES\n",
      "Time taken: 62.97297215461731 seconds\n",
      "NO     1\n",
      "YES    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Few shot prompt",
   "id": "9143b9b98ffe2182"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:48:30.801061Z",
     "start_time": "2025-05-22T10:46:03.126779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_few_shot_gemini = []\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_few_shot_prompt:\n",
    "    response = client.models.generate_content(\n",
    "        model = \"gemini-2.5-pro-preview-05-06\",\n",
    "        config = types.GenerateContentConfig(system_instruction = simple_instruction),\n",
    "        contents = prompt,\n",
    "    )\n",
    "\n",
    "    if response.text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.models.generate_content(\n",
    "            model = \"gemini-2.5-pro-preview-05-06\",\n",
    "            config = types.GenerateContentConfig(system_instruction = retry_instruction),\n",
    "            contents = prompt,\n",
    "        )\n",
    "\n",
    "    y_pred_few_shot_gemini.append(response.text)\n",
    "    print(response.text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_gemini_few_shot = end - start\n",
    "time_gemini_few_shot_df = pd.DataFrame({\"time\": [time_gemini_few_shot]})\n",
    "time_gemini_few_shot_df.to_csv(\"../exp/times_LLMs/Gemini/time_gemini_few_shot_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_few_shot_gemini = pd.Series(y_pred_few_shot_gemini).value_counts()\n",
    "print(counts_few_shot_gemini)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "y_pred_few_shot_gemini_val = [1 if response == \"YES\" else 0 for response in y_pred_few_shot_gemini]\n",
    "\n",
    "# save the array to a csv file\n",
    "few_shot_df_gemini = pd.DataFrame(y_pred_few_shot_gemini_val, columns = [\"y_pred\"])\n",
    "few_shot_df_gemini.to_csv(\"../exp/y_pred_LLMs/Gemini/y_pred_gemini_few_shot_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "d9d03f91edd0119b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n",
      "YES\n",
      "Time taken: 147.65277194976807 seconds\n",
      "YES    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Vignette prompt",
   "id": "cf27680a836a03e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:49:41.438119Z",
     "start_time": "2025-05-22T10:48:31.306816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_vignette_gemini = []\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_vignette_prompt:\n",
    "    response = client.models.generate_content(\n",
    "        model = \"gemini-2.5-pro-preview-05-06\",\n",
    "        config = types.GenerateContentConfig(system_instruction = simple_instruction),\n",
    "        contents = prompt,\n",
    "    )\n",
    "\n",
    "    if response.text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.models.generate_content(\n",
    "            model = \"gemini-2.5-pro-preview-05-06\",\n",
    "            config = types.GenerateContentConfig(system_instruction = retry_instruction),\n",
    "            contents = prompt,\n",
    "        )\n",
    "\n",
    "    y_pred_vignette_gemini.append(response.text)\n",
    "    print(response.text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_gemini_vignette = end - start\n",
    "time_gemini_vignette_df = pd.DataFrame({\"time\": [time_gemini_vignette]})\n",
    "time_gemini_vignette_df.to_csv(\"../exp/times_LLMs/Gemini/time_gemini_vignette_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_vignette_gemini = pd.Series(y_pred_vignette_gemini).value_counts()\n",
    "print(counts_vignette_gemini)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "y_pred_vignette_gemini_val = [1 if response == \"YES\" else 0 for response in y_pred_vignette_gemini]\n",
    "\n",
    "# save the array to a csv file\n",
    "vignette_df_gemini = pd.DataFrame(y_pred_vignette_gemini_val, columns = [\"y_pred\"])\n",
    "vignette_df_gemini.to_csv(\"../exp/y_pred_LLMs/Gemini/y_pred_gemini_vignette_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "ed6cc313e8bfe255",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n",
      "YES\n",
      "Time taken: 70.11247110366821 seconds\n",
      "NO     1\n",
      "YES    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Thinking prompt",
   "id": "7219332dfb796b36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:51:15.829376Z",
     "start_time": "2025-05-22T10:49:42.033530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_thinking_gemini = []\n",
    "explanation_thinking_gemini = []\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_thinking_prompt:\n",
    "    response = client.models.generate_content(\n",
    "        model = \"gemini-2.5-pro-preview-05-06\",\n",
    "        config = types.GenerateContentConfig(system_instruction = simple_instruction),\n",
    "        contents = prompt,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        prediction = re.findall(r'Prediction: (.*)', response.text)[0].strip()\n",
    "        explanation = re.findall(r'Explanation: (.*)', response.text)[0].strip()\n",
    "        y_pred_thinking_gemini.append(prediction)\n",
    "        explanation_thinking_gemini.append(explanation)\n",
    "        print(prediction)\n",
    "    except IndexError:\n",
    "        print(\"IndexError\")\n",
    "        y_pred_thinking_gemini.append(\"IndexError\")\n",
    "        explanation_thinking_gemini.append(\"IndexError\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_gemini_thinking = end - start\n",
    "time_gemini_thinking_df = pd.DataFrame({\"time\": [time_gemini_thinking]})\n",
    "time_gemini_thinking_df.to_csv(\"../exp/times_LLMs/Gemini/time_gemini_thinking_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_thinking_gemini = pd.Series(y_pred_thinking_gemini).value_counts()\n",
    "print(counts_thinking_gemini)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "y_pred_thinking_gemini_val = [1 if response == \"YES\" else 0 for response in y_pred_thinking_gemini]\n",
    "\n",
    "# save the array to a csv file\n",
    "thinking_df_gemini = pd.DataFrame(y_pred_thinking_gemini_val, columns = [\"y_pred\"])\n",
    "thinking_df_gemini.to_csv(\"../exp/y_pred_LLMs/Gemini/y_pred_gemini_thinking_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "thinking_df_explanation_gemini = pd.DataFrame(explanation_thinking_gemini, columns = [\"thinking\"])\n",
    "thinking_df_explanation_gemini.to_csv(\"../exp/y_pred_LLMs/Gemini/explanation_gemini_thinking_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "eaca8af44744f053",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n",
      "YES\n",
      "Time taken: 93.7668228149414 seconds\n",
      "NO     1\n",
      "YES    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3 Gemma (Google)",
   "id": "2121e6074cbc6d60"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Testing prompting",
   "id": "3702c698b908bc62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:56:05.880742Z",
     "start_time": "2025-05-22T10:56:04.291792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# client = genai.Client(\n",
    "#     api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "# )\n",
    "#\n",
    "# response = client.models.generate_content(\n",
    "#     model = \"gemma-3-27b-it\",\n",
    "#     contents = \"Roses are red...\",\n",
    "# )\n",
    "#\n",
    "# print(response.text)"
   ],
   "id": "97a3d2d36e949284",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roses are red... violets are blue, \n",
      "\n",
      "Sugar is sweet, and so are you! 😊 \n",
      "\n",
      "(A classic response! Is there anything specific you'd like me to add to it, or a different direction you'd like to take the rhyme?)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T10:57:51.483030Z",
     "start_time": "2025-05-22T10:57:50.664517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# client = genai.Client(\n",
    "#     api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "# )\n",
    "#\n",
    "# response = client.models.generate_content(\n",
    "#     model = \"gemma-3-27b-it\",\n",
    "#     contents = [simple_instruction, X_test_simple_prompt[0]]\n",
    "# )\n",
    "#\n",
    "# print(response.text)"
   ],
   "id": "ec22e26e6d7db2ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 Prompting with Gemma 3",
   "id": "3d89f2ca2f769df5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Simple prompt",
   "id": "e6be8665bb862ee9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T11:02:38.210199Z",
     "start_time": "2025-05-22T11:02:36.868028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_simple_gemma = []\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_simple_prompt:\n",
    "    response = client.models.generate_content(\n",
    "        model = \"gemma-3-27b-it\",\n",
    "        contents = [simple_instruction, prompt]\n",
    "    )\n",
    "\n",
    "    if response.text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.models.generate_content(\n",
    "            model = \"gemma-3-27b-it\",\n",
    "            contents = [retry_instruction, prompt]\n",
    "        )\n",
    "\n",
    "    y_pred_simple_gemma.append(response.text)\n",
    "    print(response.text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_gemma_simple_prompt = end - start\n",
    "time_gemma_simple_prompt_df = pd.DataFrame({\"time\": [time_gemma_simple_prompt]})\n",
    "time_gemma_simple_prompt_df.to_csv(\"../exp/times_LLMs/Gemma/time_gemma_simple_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_simple_gemma = pd.Series(y_pred_simple_gemma).value_counts()\n",
    "print(counts_simple_gemma)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "y_pred_simple_gemma_val = [1 if response == \"YES\" else 0 if response == \"NO\" else np.nan for response in y_pred_simple_gemma]\n",
    "\n",
    "# save the array to a csv file\n",
    "simple_prompt_df_gemma = pd.DataFrame(y_pred_simple_gemma_val, columns = [\"y_pred\"])\n",
    "simple_prompt_df_gemma.to_csv(\"../exp/y_pred_LLMs/Gemma/y_pred_gemma_simple_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "fbbbed48cd583",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n",
      "YES\n",
      "Time taken: 1.2872350215911865 seconds\n",
      "YES    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Class definition prompt",
   "id": "7434e11a61a322a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T11:02:52.822803Z",
     "start_time": "2025-05-22T11:02:51.358007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_class_def_gemma = []\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_class_definitions_prompt:\n",
    "    response = client.models.generate_content(\n",
    "        model = \"gemma-3-27b-it\",\n",
    "        contents = [simple_instruction, prompt]\n",
    "    )\n",
    "\n",
    "    if response.text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.models.generate_content(\n",
    "            model = \"gemma-3-27b-it\",\n",
    "            contents = [retry_instruction, prompt]\n",
    "        )\n",
    "\n",
    "    y_pred_class_def_gemma.append(response.text)\n",
    "    print(response.text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_gemma_class_def = end - start\n",
    "time_gemma_class_def_df = pd.DataFrame({\"time\": [time_gemma_class_def]})\n",
    "time_gemma_class_def_df.to_csv(\"../exp/times_LLMs/Gemma/time_gemma_class_definitions_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_class_def_gemma = pd.Series(y_pred_class_def_gemma).value_counts()\n",
    "print(counts_class_def_gemma)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "y_pred_class_def_gemma_val = [1 if response == \"YES\" else 0 for response in y_pred_class_def_gemma]\n",
    "\n",
    "# save the array to a csv file\n",
    "class_def_df_gemma = pd.DataFrame(y_pred_class_def_gemma_val, columns = [\"y_pred\"])\n",
    "class_def_df_gemma.to_csv(\"../exp/y_pred_LLMs/Gemma/y_pred_gemma_class_definitions_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "93ee1eaeb7e3bbcb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n",
      "NO\n",
      "Time taken: 1.4389429092407227 seconds\n",
      "YES    1\n",
      "NO     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Profiled simple prompt",
   "id": "609cbad84bf02a1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T11:03:47.972481Z",
     "start_time": "2025-05-22T11:03:46.778042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_profiled_simple_gemma = []\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_profiled_simple_prompt:\n",
    "    response = client.models.generate_content(\n",
    "        model = \"gemma-3-27b-it\",\n",
    "        contents = [simple_instruction, prompt]\n",
    "    )\n",
    "\n",
    "    if response.text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.models.generate_content(\n",
    "            model = \"gemma-3-27b-it\",\n",
    "            contents = [retry_instruction, prompt]\n",
    "        )\n",
    "\n",
    "    y_pred_profiled_simple_gemma.append(response.text)\n",
    "    print(response.text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_gemma_profiled_simple = end - start\n",
    "time_gemma_profiled_simple_df = pd.DataFrame({\"time\": [time_gemma_profiled_simple]})\n",
    "time_gemma_profiled_simple_df.to_csv(\"../exp/times_LLMs/Gemma/time_gemma_profiled_simple_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_profiled_simple_gemma = pd.Series(y_pred_profiled_simple_gemma).value_counts()\n",
    "print(counts_profiled_simple_gemma)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "y_pred_profiled_simple_gemma_val = [1 if response == \"YES\" else 0 for response in y_pred_profiled_simple_gemma]\n",
    "\n",
    "# save the array to a csv file\n",
    "profiled_simple_df_gemma = pd.DataFrame(y_pred_profiled_simple_gemma_val, columns = [\"y_pred\"])\n",
    "profiled_simple_df_gemma.to_csv(\"../exp/y_pred_LLMs/Gemma/y_pred_gemma_profiled_simple_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "825f46d8d1380b01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n",
      "YES\n",
      "Time taken: 1.1719717979431152 seconds\n",
      "YES    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Few shot prompt",
   "id": "c42f820885018472"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T11:03:59.461364Z",
     "start_time": "2025-05-22T11:03:56.630029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_few_shot_gemma = []\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_few_shot_prompt:\n",
    "    response = client.models.generate_content(\n",
    "        model = \"gemma-3-27b-it\",\n",
    "        contents = [simple_instruction, prompt]\n",
    "    )\n",
    "\n",
    "    if response.text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.models.generate_content(\n",
    "            model = \"gemma-3-27b-it\",\n",
    "            contents = [retry_instruction, prompt]\n",
    "        )\n",
    "\n",
    "    y_pred_few_shot_gemma.append(response.text)\n",
    "    print(response.text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_gemma_few_shot = end - start\n",
    "time_gemma_few_shot_df = pd.DataFrame({\"time\": [time_gemma_few_shot]})\n",
    "time_gemma_few_shot_df.to_csv(\"../exp/times_LLMs/Gemma/time_gemma_few_shot_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_few_shot_gemma = pd.Series(y_pred_few_shot_gemma).value_counts()\n",
    "print(counts_few_shot_gemma)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "y_pred_few_shot_gemma_val = [1 if response == \"YES\" else 0 for response in y_pred_few_shot_gemma]\n",
    "\n",
    "# save the array to a csv file\n",
    "few_shot_df_gemma = pd.DataFrame(y_pred_few_shot_gemma_val, columns = [\"y_pred\"])\n",
    "few_shot_df_gemma.to_csv(\"../exp/y_pred_LLMs/Gemma/y_pred_gemma_few_shot_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "ddf1121a7d7dd790",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n",
      "YES\n",
      "Time taken: 2.81103515625 seconds\n",
      "YES    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Vignette prompt",
   "id": "38b259358be63616"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T11:04:11.571666Z",
     "start_time": "2025-05-22T11:04:09.848934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_vignette_gemma = []\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_vignette_prompt:\n",
    "    response = client.models.generate_content(\n",
    "        model = \"gemma-3-27b-it\",\n",
    "        contents = [simple_instruction, prompt]\n",
    "    )\n",
    "\n",
    "    if response.text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.models.generate_content(\n",
    "            model = \"gemma-3-27b-it\",\n",
    "            contents = [retry_instruction, prompt]\n",
    "        )\n",
    "\n",
    "    y_pred_vignette_gemma.append(response.text)\n",
    "    print(response.text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_gemma_vignette = end - start\n",
    "time_gemma_vignette_df = pd.DataFrame({\"time\": [time_gemma_vignette]})\n",
    "time_gemma_vignette_df.to_csv(\"../exp/times_LLMs/Gemma/time_gemma_vignette_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_vignette_gemma = pd.Series(y_pred_vignette_gemma).value_counts()\n",
    "print(counts_vignette_gemma)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "y_pred_vignette_gemma_val = [1 if response == \"YES\" else 0 for response in y_pred_vignette_gemma]\n",
    "\n",
    "# save the array to a csv file\n",
    "vignette_df_gemma = pd.DataFrame(y_pred_vignette_gemma_val, columns = [\"y_pred\"])\n",
    "vignette_df_gemma.to_csv(\"../exp/y_pred_LLMs/Gemma/y_pred_gemma_vignette_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "3c97d7ff2a45412c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n",
      "NO\n",
      "Time taken: 1.702314853668213 seconds\n",
      "NO    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Thinking prompt",
   "id": "9e47700299ea4b61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T11:04:47.464250Z",
     "start_time": "2025-05-22T11:04:43.443832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_thinking_gemma = []\n",
    "explanation_thinking_gemma = []\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_thinking_prompt:\n",
    "    response = client.models.generate_content(\n",
    "        model = \"gemma-3-27b-it\",\n",
    "        contents = [simple_instruction, prompt]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        prediction = re.findall(r'Prediction: (.*)', response.text)[0].strip()\n",
    "        explanation = re.findall(r'Explanation: (.*)', response.text)[0].strip()\n",
    "        y_pred_thinking_gemma.append(prediction)\n",
    "        explanation_thinking_gemma.append(explanation)\n",
    "        print(prediction)\n",
    "    except IndexError:\n",
    "        print(\"IndexError\")\n",
    "        y_pred_thinking_gemma.append(\"IndexError\")\n",
    "        explanation_thinking_gemma.append(\"IndexError\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_gemma_thinking = end - start\n",
    "time_gemma_thinking_df = pd.DataFrame({\"time\": [time_gemma_thinking]})\n",
    "time_gemma_thinking_df.to_csv(\"../exp/times_LLMs/Gemma/time_gemma_thinking_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_thinking_gemma = pd.Series(y_pred_thinking_gemma).value_counts()\n",
    "print(counts_thinking_gemma)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "y_pred_thinking_gemma_val = [1 if response == \"YES\" else 0 for response in y_pred_thinking_gemma]\n",
    "\n",
    "# save the array to a csv file\n",
    "thinking_df_gemma = pd.DataFrame(y_pred_thinking_gemma_val, columns = [\"y_pred\"])\n",
    "thinking_df_gemma.to_csv(\"../exp/y_pred_LLMs/Gemma/y_pred_gemma_thinking_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "thinking_df_explanation_gemma = pd.DataFrame(explanation_thinking_gemma, columns = [\"thinking\"])\n",
    "thinking_df_explanation_gemma.to_csv(\"../exp/y_pred_LLMs/Gemma/explanation_gemma_thinking_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "7d4b9a209a578894",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n",
      "YES\n",
      "Time taken: 3.9979469776153564 seconds\n",
      "YES    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4 Llama (Meta)",
   "id": "f599161aabc9b77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:53.083700Z",
     "start_time": "2025-05-18T13:53:53.080879Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "325176ecf397915c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5 Claude (Anthropic)",
   "id": "c4e1b70cdbf6eee3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.1 Testing prompting",
   "id": "536d090fb805aeed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:53.458474Z",
     "start_time": "2025-05-18T13:53:53.454782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# client = anthropic.Anthropic(api_key = os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "#\n",
    "# message = client.messages.create(\n",
    "#     model = \"claude-3-7-sonnet-20250219\",\n",
    "#     max_tokens = 20000,\n",
    "#     temperature = 1,\n",
    "#     thinking = {\n",
    "#         \"type\": \"enabled\",\n",
    "#         \"budget_tokens\": 16000\n",
    "#     },\n",
    "#     system = claude_instruction,\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [\n",
    "#                 {\n",
    "#                     \"type\": \"text\",\n",
    "#                     \"text\": X_test_claude_prompt[0]\n",
    "#                 }\n",
    "#             ]\n",
    "#         }\n",
    "#     ]\n",
    "# )\n",
    "# print(message.content)"
   ],
   "id": "42f91f18bea3b4ff",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:53.831410Z",
     "start_time": "2025-05-18T13:53:53.827558Z"
    }
   },
   "cell_type": "code",
   "source": "# print(message.content[0].thinking)",
   "id": "79e3b3290d646d8f",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:54.188237Z",
     "start_time": "2025-05-18T13:53:54.182450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prediction = re.findall(r'Prediction: (.*)', message.content[1].text)\n",
    "# prediction[0]"
   ],
   "id": "2e198379eb34a06",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:54.582103Z",
     "start_time": "2025-05-18T13:53:54.578739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # extract what comes after Explanation:\n",
    "# explanation = re.findall(r'Explanation: (.*)', message.content[1].text)\n",
    "# explanation[0]"
   ],
   "id": "fb9e1f526e0ea4fe",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.2 Prompting with Claude 3.7 Sonnet",
   "id": "121677042242e32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Simple prompt",
   "id": "b9086a3d4ed74a48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:54.945425Z",
     "start_time": "2025-05-18T13:53:54.940351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "simple_prompt_y_pred_claude = []\n",
    "simple_prompt_thinking_claude = []\n",
    "\n",
    "client = anthropic.Anthropic(api_key = os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_simple_prompt:\n",
    "    message = client.messages.create(\n",
    "        model = \"claude-3-7-sonnet-20250219\",\n",
    "        max_tokens = 20000,\n",
    "        temperature = 1,\n",
    "        thinking = {\n",
    "            \"type\": \"enabled\",\n",
    "            \"budget_tokens\": 16000\n",
    "        },\n",
    "        system = simple_instruction,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if message.content[1].text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        message = client.messages.create(\n",
    "            model = \"claude-3-7-sonnet-20250219\",\n",
    "            max_tokens = 20000,\n",
    "            temperature = 1,\n",
    "            thinking = {\n",
    "                \"type\": \"enabled\",\n",
    "                \"budget_tokens\": 16000\n",
    "            },\n",
    "            system = retry_instruction,\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    simple_prompt_y_pred_claude.append(message.content[1].text)\n",
    "    simple_prompt_thinking_claude.append(message.content[0].thinking)\n",
    "    print(message.content[1].text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_claude_simple_prompt = end - start\n",
    "time_claude_simple_prompt_df = pd.DataFrame({\"time\": [time_claude_simple_prompt]})\n",
    "time_claude_simple_prompt_df.to_csv(\"../exp/times_LLMs/Claude/time_claude_simple_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_simple_claude = pd.Series(simple_prompt_y_pred_claude).value_counts()\n",
    "print(counts_simple_claude)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "simple_prompt_y_pred_claude = [1 if response == \"YES\" else 0 if response == \"NO\" else np.nan for response in simple_prompt_y_pred_claude]\n",
    "\n",
    "# save the array to a csv file\n",
    "simple_prompt_df_claude = pd.DataFrame(simple_prompt_y_pred_claude, columns = [\"y_pred\"])\n",
    "simple_prompt_df_claude.to_csv(\"../exp/y_pred_LLMs/Claude/y_pred_claude_simple_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "simple_prompt_df_thinking_claude = pd.DataFrame(simple_prompt_thinking_claude, columns = [\"thinking\"])\n",
    "simple_prompt_df_thinking_claude.to_csv(\"../exp/y_pred_LLMs/Claude/Thinking/thinking_claude_simple_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "82b0ac9aa76bd4a3",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Class definition prompt",
   "id": "45ad8d736772b2d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:55.280539Z",
     "start_time": "2025-05-18T13:53:55.276871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_def_y_pred_claude = []\n",
    "class_def_thinking_claude = []\n",
    "\n",
    "client = anthropic.Anthropic(api_key = os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_class_definitions_prompt:\n",
    "    message = client.messages.create(\n",
    "        model = \"claude-3-7-sonnet-20250219\",\n",
    "        max_tokens = 20000,\n",
    "        temperature = 1,\n",
    "        thinking = {\n",
    "            \"type\": \"enabled\",\n",
    "            \"budget_tokens\": 16000\n",
    "        },\n",
    "        system = class_definitions_instruction,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if message.content[1].text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        message = client.messages.create(\n",
    "            model = \"claude-3-7-sonnet-20250219\",\n",
    "            max_tokens = 20000,\n",
    "            temperature = 1,\n",
    "            thinking = {\n",
    "                \"type\": \"enabled\",\n",
    "                \"budget_tokens\": 16000\n",
    "            },\n",
    "            system = retry_instruction,\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    class_def_y_pred_claude.append(message.content[1].text)\n",
    "    class_def_thinking_claude.append(message.content[0].thinking)\n",
    "    print(message.content[1].text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_claude_class_definitions = end - start\n",
    "time_claude_class_definitions_df = pd.DataFrame({\"time\": [time_claude_class_definitions]})\n",
    "time_claude_class_definitions_df.to_csv(\"../exp/times_LLMs/Claude/time_claude_class_definitions_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_class_def_claude = pd.Series(class_def_y_pred_claude).value_counts()\n",
    "print(counts_class_def_claude)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "class_def_y_pred_claude = [1 if response == \"YES\" else 0 for response in class_def_y_pred_claude]\n",
    "\n",
    "# save the array to a csv file\n",
    "class_def_df_claude = pd.DataFrame(class_def_y_pred_claude, columns = [\"y_pred\"])\n",
    "class_def_df_claude.to_csv(\"../exp/y_pred_LLMs/Claude/y_pred_claude_class_definitions_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "class_def_prompt_df_thinking_claude = pd.DataFrame(class_def_thinking_claude, columns = [\"thinking\"])\n",
    "class_def_prompt_df_thinking_claude.to_csv(\"../exp/y_pred_LLMs/Claude/Thinking/thinking_claude_class_def_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "71a5f0ea55213998",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Profiled simple prompt",
   "id": "42d06e79240cb72f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:55.614807Z",
     "start_time": "2025-05-18T13:53:55.610572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "profiled_simple_y_pred_claude = []\n",
    "profiled_simple_thinking_claude = []\n",
    "\n",
    "client = anthropic.Anthropic(api_key = os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_profiled_simple_prompt:\n",
    "    message = client.messages.create(\n",
    "        model = \"claude-3-7-sonnet-20250219\",\n",
    "        max_tokens = 20000,\n",
    "        temperature = 1,\n",
    "        thinking = {\n",
    "            \"type\": \"enabled\",\n",
    "            \"budget_tokens\": 16000\n",
    "        },\n",
    "        system = profiled_simple_instruction,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if message.content[1].text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        message = client.messages.create(\n",
    "            model = \"claude-3-7-sonnet-20250219\",\n",
    "            max_tokens = 20000,\n",
    "            temperature = 1,\n",
    "            thinking = {\n",
    "                \"type\": \"enabled\",\n",
    "                \"budget_tokens\": 16000\n",
    "            },\n",
    "            system = retry_instruction,\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    profiled_simple_y_pred_claude.append(message.content[1].text)\n",
    "    profiled_simple_thinking_claude.append(message.content[0].thinking)\n",
    "    print(message.content[1].text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_claude_profiled_simple = end - start\n",
    "time_claude_profiled_simple_df = pd.DataFrame({\"time\": [time_claude_profiled_simple]})\n",
    "time_claude_profiled_simple_df.to_csv(\"../exp/times_LLMs/Claude/time_claude_profiled_simple_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_profiled_simple_claude = pd.Series(profiled_simple_y_pred_claude).value_counts()\n",
    "print(counts_profiled_simple_claude)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "profiled_simple_y_pred_claude_val = [1 if response == \"YES\" else 0 for response in profiled_simple_y_pred_claude]\n",
    "\n",
    "# save the array to a csv file\n",
    "profiled_simple_df_claude = pd.DataFrame(profiled_simple_y_pred_claude_val, columns = [\"y_pred\"])\n",
    "profiled_simple_df_claude.to_csv(\"../exp/y_pred_LLMs/Claude/y_pred_claude_profiled_simple_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "profiled_simple_prompt_df_thinking_claude = pd.DataFrame(profiled_simple_thinking_claude, columns = [\"thinking\"])\n",
    "profiled_simple_prompt_df_thinking_claude.to_csv(\"../exp/y_pred_LLMs/Claude/Thinking/thinking_claude_profiled_simple_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "a1e5ce97442023ea",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Few shot prompt",
   "id": "7edaaf1b4cecd916"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:55.938967Z",
     "start_time": "2025-05-18T13:53:55.934984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "few_shot_y_pred_claude = []\n",
    "few_shot_thinking_claude = []\n",
    "\n",
    "client = anthropic.Anthropic(api_key = os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_few_shot_prompt:\n",
    "    message = client.messages.create(\n",
    "        model = \"claude-3-7-sonnet-20250219\",\n",
    "        max_tokens = 20000,\n",
    "        temperature = 1,\n",
    "        thinking = {\n",
    "            \"type\": \"enabled\",\n",
    "            \"budget_tokens\": 16000\n",
    "        },\n",
    "        system = few_shot_instruction,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if message.content[1].text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        message = client.messages.create(\n",
    "            model = \"claude-3-7-sonnet-20250219\",\n",
    "            max_tokens = 20000,\n",
    "            temperature = 1,\n",
    "            thinking = {\n",
    "                \"type\": \"enabled\",\n",
    "                \"budget_tokens\": 16000\n",
    "            },\n",
    "            system = retry_instruction,\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    few_shot_y_pred_claude.append(message.content[1].text)\n",
    "    few_shot_thinking_claude.append(message.content[0].thinking)\n",
    "    print(message.content[1].text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_claude_few_shot = end - start\n",
    "time_claude_few_shot_df = pd.DataFrame({\"time\": [time_claude_few_shot]})\n",
    "time_claude_few_shot_df.to_csv(\"../exp/times_LLMs/Claude/time_claude_few_shot_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_few_shot_claude = pd.Series(few_shot_y_pred_claude).value_counts()\n",
    "print(counts_few_shot_claude)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "few_shot_y_pred_claude_val = [1 if response == \"YES\" else 0 for response in few_shot_y_pred_claude]\n",
    "\n",
    "# save the array to a csv file\n",
    "few_shot_df_claude = pd.DataFrame(few_shot_y_pred_claude_val, columns = [\"y_pred\"])\n",
    "few_shot_df_claude.to_csv(\"../exp/y_pred_LLMs/Claude/y_pred_claude_few_shot_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "few_shot_prompt_df_thinking_claude = pd.DataFrame(few_shot_thinking_claude, columns = [\"thinking\"])\n",
    "few_shot_prompt_df_thinking_claude.to_csv(\"../exp/y_pred_LLMs/Claude/Thinking/thinking_claude_few_shot_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "c7bd23ac0b2904bb",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Vignette prompt",
   "id": "ff7c5780e8d79746"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T13:53:56.284349Z",
     "start_time": "2025-05-18T13:53:56.280388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vignette_y_pred_claude = []\n",
    "vignette_thinking_claude = []\n",
    "\n",
    "client = anthropic.Anthropic(api_key = os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_vignette_prompt:\n",
    "    message = client.messages.create(\n",
    "        model = \"claude-3-7-sonnet-20250219\",\n",
    "        max_tokens = 20000,\n",
    "        temperature = 1,\n",
    "        thinking = {\n",
    "            \"type\": \"enabled\",\n",
    "            \"budget_tokens\": 16000\n",
    "        },\n",
    "        system = vignette_instruction,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if message.content[1].text.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        message = client.messages.create(\n",
    "            model = \"claude-3-7-sonnet-20250219\",\n",
    "            max_tokens = 20000,\n",
    "            temperature = 1,\n",
    "            thinking = {\n",
    "                \"type\": \"enabled\",\n",
    "                \"budget_tokens\": 16000\n",
    "            },\n",
    "            system = retry_instruction,\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    vignette_y_pred_claude.append(message.content[1].text)\n",
    "    vignette_thinking_claude.append(message.content[0].thinking)\n",
    "    print(message.content[1].text)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_claude_vignette = end - start\n",
    "time_claude_vignette_df = pd.DataFrame({\"time\": [time_claude_vignette]})\n",
    "time_claude_vignette_df.to_csv(\"../exp/times_LLMs/Claude/time_claude_vignette_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_vignette_claude = pd.Series(vignette_y_pred_claude).value_counts()\n",
    "print(counts_vignette_claude)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "vignette_y_pred_claude_val = [1 if response == \"YES\" else 0 for response in vignette_y_pred_claude]\n",
    "\n",
    "# save the array to a csv file\n",
    "vignette_df_claude = pd.DataFrame(vignette_y_pred_claude_val, columns = [\"y_pred\"])\n",
    "vignette_df_claude.to_csv(\"../exp/y_pred_LLMs/Claude/y_pred_claude_vignette_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "vignette_prompt_df_thinking_claude = pd.DataFrame(vignette_thinking_claude, columns = [\"thinking\"])\n",
    "vignette_prompt_df_thinking_claude.to_csv(\"../exp/y_pred_LLMs/Claude/Thinking/thinking_claude_vignette_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "c052ee81007a6a1f",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Thinking prompt",
   "id": "2fb62df8fc30455"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T12:44:24.874762Z",
     "start_time": "2025-05-19T12:43:30.459011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "thinking_prompt_y_pred_claude = []\n",
    "thinking_prompt_explanation_claude = []\n",
    "thinking_prompt_thinking_claude = []\n",
    "\n",
    "client = anthropic.Anthropic(api_key = os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_thinking_prompt:\n",
    "    message = client.messages.create(\n",
    "        model = \"claude-3-7-sonnet-20250219\",\n",
    "        max_tokens = 20000,\n",
    "        temperature = 1,\n",
    "        thinking = {\n",
    "            \"type\": \"enabled\",\n",
    "            \"budget_tokens\": 16000\n",
    "        },\n",
    "        system = thinking_instruction,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    try:\n",
    "        prediction = re.findall(r'Prediction: (.*)', message.content[1].text)[0].strip()\n",
    "        explanation = re.findall(r'Explanation: (.*)', message.content[1].text)[0].strip()\n",
    "        thinking_prompt_y_pred_claude.append(prediction)\n",
    "        thinking_prompt_explanation_claude.append(explanation)\n",
    "        thinking_prompt_thinking_claude.append(message.content[0].thinking)\n",
    "        print(prediction)\n",
    "    except IndexError:\n",
    "        print(\"IndexError\")\n",
    "        thinking_prompt_y_pred_claude.append(\"IndexError\")\n",
    "        thinking_prompt_explanation_claude.append(\"IndexError\")\n",
    "        thinking_prompt_thinking_claude.append(\"IndexError\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_claude_thinking_prompt = end - start\n",
    "time_claude_thinking_prompt_df = pd.DataFrame({\"time\": [time_claude_thinking_prompt]})\n",
    "time_claude_thinking_prompt_df.to_csv(\"../exp/times_LLMs/Claude/time_claude_thinking_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_thinking_prompt_claude = pd.Series(thinking_prompt_y_pred_claude).value_counts()\n",
    "print(counts_thinking_prompt_claude)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "thinking_prompt_y_pred_claude_val = [1 if response == \"YES\" else 0 for response in thinking_prompt_y_pred_claude]\n",
    "\n",
    "# save the array to a csv file\n",
    "thinking_prompt_df_claude = pd.DataFrame(thinking_prompt_y_pred_claude_val, columns = [\"y_pred\"])\n",
    "thinking_prompt_df_claude.to_csv(\"../exp/y_pred_LLMs/Claude/y_pred_claude_thinking_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "thinking_prompt_df_thinking_claude = pd.DataFrame(thinking_prompt_thinking_claude, columns = [\"thinking\"])\n",
    "thinking_prompt_df_thinking_claude.to_csv(\"../exp/y_pred_LLMs/Claude/Thinking/thinking_claude_thinking_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "thinking_prompt_df_explanation_claude = pd.DataFrame(thinking_prompt_explanation_claude, columns = [\"thinking\"])\n",
    "thinking_prompt_df_explanation_claude.to_csv(\"../exp/y_pred_LLMs/Claude/explanation_claude_thinking_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "50df74bd8286714",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n",
      "NO\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# iterate over the test set and save the response for each prompt in an array\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m X_test_thinking_prompt:\n\u001B[0;32m---> 12\u001B[0m     message \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mclaude-3-7-sonnet-20250219\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mthinking\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtype\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43menabled\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbudget_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m16000\u001B[39;49m\n\u001B[1;32m     19\u001B[0m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m        \u001B[49m\u001B[43msystem\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mthinking_instruction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m                    \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m                        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtype\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m                        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m                    \u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m                \u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m        \u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     34\u001B[0m         prediction \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39mfindall(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPrediction: (.*)\u001B[39m\u001B[38;5;124m'\u001B[39m, message\u001B[38;5;241m.\u001B[39mcontent[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mtext)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/anthropic/_utils/_utils.py:283\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    281\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    282\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[0;32m--> 283\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/anthropic/resources/messages/messages.py:954\u001B[0m, in \u001B[0;36mMessages.create\u001B[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    947\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model \u001B[38;5;129;01min\u001B[39;00m DEPRECATED_MODELS:\n\u001B[1;32m    948\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    949\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe model \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is deprecated and will reach end-of-life on \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mDEPRECATED_MODELS[model]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    950\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[1;32m    951\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[1;32m    952\u001B[0m     )\n\u001B[0;32m--> 954\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    955\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/v1/messages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    956\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    957\u001B[0m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m    958\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    959\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    960\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    961\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    962\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop_sequences\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop_sequences\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    963\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    964\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msystem\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43msystem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    965\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    966\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mthinking\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mthinking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    967\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    968\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    969\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_k\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    970\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    971\u001B[0m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    972\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessage_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMessageCreateParamsStreaming\u001B[49m\n\u001B[1;32m    973\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\n\u001B[1;32m    974\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmessage_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMessageCreateParamsNonStreaming\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    975\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    976\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    977\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[1;32m    978\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    979\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMessage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    980\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    981\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mRawMessageStreamEvent\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    982\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/anthropic/_base_client.py:1290\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1276\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1277\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1278\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1285\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1286\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1287\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1288\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1289\u001B[0m     )\n\u001B[0;32m-> 1290\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/anthropic/_base_client.py:1020\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1018\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1019\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1020\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1021\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1022\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_should_stream_response_body\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1023\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1024\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1025\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mTimeoutException \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m   1026\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncountered httpx.TimeoutException\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/httpx/_client.py:914\u001B[0m, in \u001B[0;36mClient.send\u001B[0;34m(self, request, stream, auth, follow_redirects)\u001B[0m\n\u001B[1;32m    910\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_timeout(request)\n\u001B[1;32m    912\u001B[0m auth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_request_auth(request, auth)\n\u001B[0;32m--> 914\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_auth\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    915\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    916\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    917\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    918\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    919\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    920\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    921\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n",
      "File \u001B[0;32m~/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/httpx/_client.py:942\u001B[0m, in \u001B[0;36mClient._send_handling_auth\u001B[0;34m(self, request, auth, follow_redirects, history)\u001B[0m\n\u001B[1;32m    939\u001B[0m request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(auth_flow)\n\u001B[1;32m    941\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 942\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_redirects\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    943\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    944\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    945\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhistory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    946\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    947\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    948\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/httpx/_client.py:979\u001B[0m, in \u001B[0;36mClient._send_handling_redirects\u001B[0;34m(self, request, follow_redirects, history)\u001B[0m\n\u001B[1;32m    976\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event_hooks[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequest\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m    977\u001B[0m     hook(request)\n\u001B[0;32m--> 979\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_single_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    980\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    981\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event_hooks[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[0;32m~/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/httpx/_client.py:1014\u001B[0m, in \u001B[0;36mClient._send_single_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m   1009\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   1010\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAttempted to send an async request with a sync Client instance.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1011\u001B[0m     )\n\u001B[1;32m   1013\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39mrequest):\n\u001B[0;32m-> 1014\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mtransport\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, SyncByteStream)\n\u001B[1;32m   1018\u001B[0m response\u001B[38;5;241m.\u001B[39mrequest \u001B[38;5;241m=\u001B[39m request\n",
      "File \u001B[0;32m~/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/httpx/_transports/default.py:250\u001B[0m, in \u001B[0;36mHTTPTransport.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    237\u001B[0m req \u001B[38;5;241m=\u001B[39m httpcore\u001B[38;5;241m.\u001B[39mRequest(\n\u001B[1;32m    238\u001B[0m     method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[1;32m    239\u001B[0m     url\u001B[38;5;241m=\u001B[39mhttpcore\u001B[38;5;241m.\u001B[39mURL(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    247\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[1;32m    248\u001B[0m )\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[0;32m--> 250\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_pool\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp\u001B[38;5;241m.\u001B[39mstream, typing\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Response(\n\u001B[1;32m    255\u001B[0m     status_code\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mstatus,\n\u001B[1;32m    256\u001B[0m     headers\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[1;32m    257\u001B[0m     stream\u001B[38;5;241m=\u001B[39mResponseStream(resp\u001B[38;5;241m.\u001B[39mstream),\n\u001B[1;32m    258\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mresp\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[1;32m    259\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    253\u001B[0m         closing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assign_requests_to_connections()\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_connections(closing)\n\u001B[0;32m--> 256\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    258\u001B[0m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[1;32m    259\u001B[0m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n\u001B[1;32m    260\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, typing\u001B[38;5;241m.\u001B[39mIterable)\n",
      "File \u001B[0;32m~/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    232\u001B[0m connection \u001B[38;5;241m=\u001B[39m pool_request\u001B[38;5;241m.\u001B[39mwait_for_connection(timeout\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[0;32m--> 236\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    237\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpool_request\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[1;32m    240\u001B[0m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[1;32m    242\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n\u001B[1;32m    244\u001B[0m     pool_request\u001B[38;5;241m.\u001B[39mclear_connection()\n",
      "File \u001B[0;32m~/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001B[0m, in \u001B[0;36mHTTPConnection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_connect_failed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[0;32m--> 103\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_connection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:136\u001B[0m, in \u001B[0;36mHTTP11Connection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_closed\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[1;32m    135\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_closed()\n\u001B[0;32m--> 136\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "File \u001B[0;32m~/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:106\u001B[0m, in \u001B[0;36mHTTP11Connection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreceive_response_headers\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request, kwargs\n\u001B[1;32m     99\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[1;32m    100\u001B[0m     (\n\u001B[1;32m    101\u001B[0m         http_version,\n\u001B[1;32m    102\u001B[0m         status,\n\u001B[1;32m    103\u001B[0m         reason_phrase,\n\u001B[1;32m    104\u001B[0m         headers,\n\u001B[1;32m    105\u001B[0m         trailing_data,\n\u001B[0;32m--> 106\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_receive_response_headers\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m     trace\u001B[38;5;241m.\u001B[39mreturn_value \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    108\u001B[0m         http_version,\n\u001B[1;32m    109\u001B[0m         status,\n\u001B[1;32m    110\u001B[0m         reason_phrase,\n\u001B[1;32m    111\u001B[0m         headers,\n\u001B[1;32m    112\u001B[0m     )\n\u001B[1;32m    114\u001B[0m network_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_network_stream\n",
      "File \u001B[0;32m~/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:177\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_response_headers\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    174\u001B[0m timeout \u001B[38;5;241m=\u001B[39m timeouts\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 177\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_receive_event\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11\u001B[38;5;241m.\u001B[39mResponse):\n\u001B[1;32m    179\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:217\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_event\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    214\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mnext_event()\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11\u001B[38;5;241m.\u001B[39mNEED_DATA:\n\u001B[0;32m--> 217\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_network_stream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    218\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mREAD_NUM_BYTES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    221\u001B[0m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[1;32m    222\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;241m==\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mtheir_state \u001B[38;5;241m==\u001B[39m h11\u001B[38;5;241m.\u001B[39mSEND_RESPONSE:\n",
      "File \u001B[0;32m~/PycharmProjects/master_thesis/.venv/lib/python3.10/site-packages/httpcore/_backends/sync.py:128\u001B[0m, in \u001B[0;36mSyncStream.read\u001B[0;34m(self, max_bytes, timeout)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[1;32m    127\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39msettimeout(timeout)\n\u001B[0;32m--> 128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1259\u001B[0m, in \u001B[0;36mSSLSocket.recv\u001B[0;34m(self, buflen, flags)\u001B[0m\n\u001B[1;32m   1255\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1256\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1257\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1258\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1259\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuflen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1260\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1261\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv(buflen, flags)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1132\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1130\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m, buffer)\n\u001B[1;32m   1131\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1132\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1133\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m SSLError \u001B[38;5;28;01mas\u001B[39;00m x:\n\u001B[1;32m   1134\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m SSL_ERROR_EOF \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msuppress_ragged_eofs:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6 DeepSeek",
   "id": "67c94f3e4d544889"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Off-Peak Discounts**：DeepSeek-R1 with 75% off at off-peak hours (16:30-00:30 UTC daily)",
   "id": "645ad98c6d42d431"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6.1 Testing prompting",
   "id": "9aa1e23112271d8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T14:52:45.716650Z",
     "start_time": "2025-05-18T14:51:56.622630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client = OpenAI(api_key = os.environ.get(\"DeepSeek_API_Key\"), base_url = \"https://api.deepseek.com\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = \"deepseek-reasoner\",\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": simple_instruction},\n",
    "        {\"role\": \"user\", \"content\": X_test_simple_prompt[0]},\n",
    "    ],\n",
    "    stream = False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "72f8f909a923d996",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T14:53:32.558249Z",
     "start_time": "2025-05-18T14:53:32.553707Z"
    }
   },
   "cell_type": "code",
   "source": "response.choices[0].message.reasoning_content",
   "id": "ba3227e6778f7899",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, let's see. I need to determine if the person develops a psychological disorder between T1 and T2 based on the given data. The answer should be just YES or NO. \\n\\nFirst, looking at the T1 values. The General psychopathology (GSI) at T1 is 0.0172, which is very low. That suggests that at T1, their psychopathology isn't severe. Other factors like stress are 0.44, which is moderate. Positive mental health is slightly negative but close to zero. Social support, self-efficacy, life satisfaction are all positive, but not extremely high. Problem-focused coping is quite high at 1.73, which might be a protective factor. Emotion-focused coping is lower. Anxiety sensitivity and fear of bodily sensations are low to moderate. Dysfunctional attitudes are around 0.27.\\n\\nNow, looking at the changes from T1 to T2. The change in GSI (General psychopathology) is -0.8256, which means it decreased. That's a significant drop. But wait, if GSI decreases, that would imply their psychopathology is getting better, right? So from 0.0172 to 0.0172 -0.8256? Wait, that would make GSI at T2 negative? That doesn't make sense because GSI is a measure that's usually a positive value. Maybe the change is subtracted? Or maybe the scores are standardized? Hmm. Maybe the initial GSI was already very low, and a decrease would mean even lower, but maybe below a certain threshold? Not sure. But the key point is that the change is negative, so improvement.\\n\\nOther changes: Positive mental health decreased by -0.75, which is a big drop. Social support increased by 0.705. General self-efficacy decreased slightly. Life satisfaction increased a bit. Anxiety sensitivity and fear of bodily sensations both decreased significantly (around -0.86 and -0.84). Dysfunctional attitudes increased by 0.48, which is a notable increase. But GSI decreased a lot.\\n\\nSo, the main factors that might contribute to developing a disorder: increased dysfunctional attitudes, decreased positive mental health. However, GSI is going down, which suggests less psychopathology. Stress isn't mentioned in the change data, so we don't know how that changed. The initial stress was 0.44, but without knowing the change, maybe it stayed the same or increased?\\n\\nBut the GSI is the main indicator of general psychopathology. If that's decreasing, even with some negative changes in other areas, the overall trend is improvement. However, the question is whether they developed a disorder between T1 and T2. At T1, their GSI was very low, almost non-existent. Even with a decrease (which might not be possible if it's already low), maybe the T2 GSI is even lower. But developing a disorder would require an increase in symptoms. Wait, the change in GSI is negative, so T2 GSI is T1 GSI plus the change. So 0.0172 -0.8256 = negative? That's confusing. Maybe the data is standardized, so negative values indicate below the mean. But if GSI is a measure where higher is worse, then a decrease would be improvement. If at T1, their GSI was already low, and it's decreasing further, that's better. But how does that relate to developing a disorder? They would need an increase. \\n\\nWait, maybe the problem is that even if GSI is decreasing, other factors like dysfunctional attitudes increasing could contribute. Dysfunctional attitudes are a risk factor for depression, for example. But their GSI is going down. Positive mental health is decreasing, which is bad, but social support is increasing. Stress level at T1 was moderate. So conflicting factors. However, the key indicator for a psychological disorder is likely the GSI. If that's going down, even with some negative changes elsewhere, the person is less likely to have developed a disorder. Unless the other factors outweigh. But I'm not sure. The answer might be NO because the GSI decreased, indicating improvement, despite some negative changes. However, the increase in dysfunctional attitudes is a concern. But without knowing the thresholds for these variables, it's hard. Since the question is likely based on the GSI change, which is the main psychopathology measure, and it's improving, the answer is NO.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6.2 Prompting with DeepSeek Reasoning R1",
   "id": "805d4589490aa7c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Simple prompt",
   "id": "142dfd5d56cffa1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:43:23.493643Z",
     "start_time": "2025-05-18T15:41:56.551519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "simple_prompt_y_pred_deeps = []\n",
    "simple_prompt_thinking_deeps = []\n",
    "\n",
    "client = OpenAI(api_key = os.environ.get(\"DeepSeek_API_Key\"), base_url = \"https://api.deepseek.com\")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_simple_prompt:\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"deepseek-reasoner\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": simple_instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        stream = False\n",
    "    )\n",
    "\n",
    "    if response.choices[0].message.content.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.chat.completions.create(\n",
    "            model = \"deepseek-reasoner\",\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": retry_instruction},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            stream = False\n",
    "        )\n",
    "\n",
    "    simple_prompt_y_pred_deeps.append(response.choices[0].message.content)\n",
    "    simple_prompt_thinking_deeps.append(response.choices[0].message.reasoning_content)\n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_deeps_simple_prompt = end - start\n",
    "time_deeps_simple_prompt_df = pd.DataFrame({\"time\": [time_deeps_simple_prompt]})\n",
    "time_deeps_simple_prompt_df.to_csv(\"../exp/times_LLMs/DeepSeek/time_deeps_simple_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_simple_deeps = pd.Series(simple_prompt_y_pred_deeps).value_counts()\n",
    "print(counts_simple_deeps)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "simple_prompt_y_pred_deeps = [1 if response == \"YES\" else 0 if response == \"NO\" else np.nan for response in simple_prompt_y_pred_deeps]\n",
    "\n",
    "# save the array to a csv file\n",
    "simple_prompt_df_deeps = pd.DataFrame(simple_prompt_y_pred_deeps, columns = [\"y_pred\"])\n",
    "simple_prompt_df_deeps.to_csv(\"../exp/y_pred_LLMs/DeepSeek/y_pred_deeps_simple_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "simple_prompt_df_thinking_deeps = pd.DataFrame(simple_prompt_thinking_deeps, columns = [\"thinking\"])\n",
    "simple_prompt_df_thinking_deeps.to_csv(\"../exp/y_pred_LLMs/DeepSeek/Thinking/thinking_deeps_simple_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "c4e8afcb4089cc18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the following person develop a psychological disorder between time point T1 and T2? Data: T1 Positive mental health: -0.0279170753483525, T1 Social support: 0.1421238143169474, T1 General self-efficacy: 0.3649793457412237, T1 Life satisfaction: 0.3372886835461141, T1 Stress: 0.4419361727222826, T1 Problem-focused coping: 1.7319368683783989, T1 Emotion-focused coping: 0.2078300133169115, T1 Anxiety sensitivity: 0.1594156886399411, T1 Fear of bodily sensations: 0.2863750811390516, T1 Dysfunctional attitudes: 0.2750686254386546, T1 General psychopathology: Global Severity Index (GSI): 0.0172227087467131, Education: 2.0, T1 BMI: 1.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): 0.7057099569575698, Change in General self-efficacy (T2-T1): -0.1819798191096402, Change in Life satisfaction (T2-T1): 0.1407378746091848, Change in Anxiety sensitivity (T2-T1): -0.8617238998696288, Change in Fear of bodily sensations (T2-T1): -0.846980042266938, Change in Dysfunctional attitudes (T2-T1): 0.4849021865236002, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.8256637821414634\n",
      "NO\n",
      "Does the following person develop a psychological disorder between time point T1 and T2? Data: T1 Positive mental health: -0.0279170753483525, T1 Social support: 1.0790451808002794, T1 General self-efficacy: -2.3549374763869046, T1 Life satisfaction: -0.471818725223128, T1 Stress: 0.0419806820921755, T1 Problem-focused coping: 0.8532782878883876, T1 Emotion-focused coping: -0.8456281474585625, T1 Anxiety sensitivity: 0.5557512543954795, T1 Fear of bodily sensations: 1.241666884265578, T1 Dysfunctional attitudes: -0.2250808121114963, T1 General psychopathology: Global Severity Index (GSI): 1.5062628203345918, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): -0.460474567131092, Change in General self-efficacy (T2-T1): 1.7532147220525784, Change in Life satisfaction (T2-T1): 0.3167090300485398, Change in Anxiety sensitivity (T2-T1): 0.5185167790561809, Change in Fear of bodily sensations (T2-T1): 1.781472184716851, Change in Dysfunctional attitudes (T2-T1): 0.8097267435207866, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -1.09104984935167\n",
      "NO\n",
      "Time taken: 86.92125701904297 seconds\n",
      "NO    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Class definition prompt",
   "id": "d50675df7897e346"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:44:57.706820Z",
     "start_time": "2025-05-18T15:43:57.589571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_def_y_pred_deeps = []\n",
    "class_def_thinking_deeps = []\n",
    "\n",
    "client = OpenAI(api_key = os.environ.get(\"DeepSeek_API_Key\"), base_url = \"https://api.deepseek.com\")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_class_definitions_prompt:\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"deepseek-reasoner\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": class_definitions_instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        stream = False\n",
    "    )\n",
    "\n",
    "    if response.choices[0].message.content.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.chat.completions.create(\n",
    "            model = \"deepseek-reasoner\",\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": retry_instruction},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            stream = False\n",
    "        )\n",
    "\n",
    "    class_def_y_pred_deeps.append(response.choices[0].message.content)\n",
    "    class_def_thinking_deeps.append(response.choices[0].message.reasoning_content)\n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_deeps_class_definitions = end - start\n",
    "time_deeps_class_definitions_df = pd.DataFrame({\"time\": [time_deeps_class_definitions]})\n",
    "time_deeps_class_definitions_df.to_csv(\"../exp/times_LLMs/DeepSeek/time_deeps_class_definitions_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_class_def_deeps = pd.Series(class_def_y_pred_deeps).value_counts()\n",
    "print(counts_class_def_deeps)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "class_def_y_pred_deeps = [1 if response == \"YES\" else 0 for response in class_def_y_pred_deeps]\n",
    "\n",
    "# save the array to a csv file\n",
    "class_def_df_deeps = pd.DataFrame(class_def_y_pred_deeps, columns = [\"y_pred\"])\n",
    "class_def_df_deeps.to_csv(\"../exp/y_pred_LLMs/DeepSeek/y_pred_deeps_class_definitions_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "class_def_prompt_df_thinking_deeps = pd.DataFrame(class_def_thinking_deeps, columns = [\"thinking\"])\n",
    "class_def_prompt_df_thinking_deeps.to_csv(\"../exp/y_pred_LLMs/DeepSeek/Thinking/thinking_deeps_class_def_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "9f90a9516821da3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following data, classify whether this person develops a psychological disorder between T1 and T2 according to the instructions provided and data measured by F-DIPS structural interviews. Respond with YES or NO. Instructions: NO: The person did not develop any new psychological disorder between T1 and T2. This means they were either healthy at both time points, had an ongoing disorder across both time points, or had already recovered from a previous disorder. YES: The person was psychologically healthy at T1 but developed a psychological disorder at T2. Does the following person develop a psychological disorder between time point T1 and T2? Data: T1 Positive mental health: -0.0279170753483525, T1 Social support: 0.1421238143169474, T1 General self-efficacy: 0.3649793457412237, T1 Life satisfaction: 0.3372886835461141, T1 Stress: 0.4419361727222826, T1 Problem-focused coping: 1.7319368683783989, T1 Emotion-focused coping: 0.2078300133169115, T1 Anxiety sensitivity: 0.1594156886399411, T1 Fear of bodily sensations: 0.2863750811390516, T1 Dysfunctional attitudes: 0.2750686254386546, T1 General psychopathology: Global Severity Index (GSI): 0.0172227087467131, Education: 2.0, T1 BMI: 1.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): 0.7057099569575698, Change in General self-efficacy (T2-T1): -0.1819798191096402, Change in Life satisfaction (T2-T1): 0.1407378746091848, Change in Anxiety sensitivity (T2-T1): -0.8617238998696288, Change in Fear of bodily sensations (T2-T1): -0.846980042266938, Change in Dysfunctional attitudes (T2-T1): 0.4849021865236002, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.8256637821414634\n",
      "NO\n",
      "Given the following data, classify whether this person develops a psychological disorder between T1 and T2 according to the instructions provided and data measured by F-DIPS structural interviews. Respond with YES or NO. Instructions: NO: The person did not develop any new psychological disorder between T1 and T2. This means they were either healthy at both time points, had an ongoing disorder across both time points, or had already recovered from a previous disorder. YES: The person was psychologically healthy at T1 but developed a psychological disorder at T2. Does the following person develop a psychological disorder between time point T1 and T2? Data: T1 Positive mental health: -0.0279170753483525, T1 Social support: 1.0790451808002794, T1 General self-efficacy: -2.3549374763869046, T1 Life satisfaction: -0.471818725223128, T1 Stress: 0.0419806820921755, T1 Problem-focused coping: 0.8532782878883876, T1 Emotion-focused coping: -0.8456281474585625, T1 Anxiety sensitivity: 0.5557512543954795, T1 Fear of bodily sensations: 1.241666884265578, T1 Dysfunctional attitudes: -0.2250808121114963, T1 General psychopathology: Global Severity Index (GSI): 1.5062628203345918, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): -0.460474567131092, Change in General self-efficacy (T2-T1): 1.7532147220525784, Change in Life satisfaction (T2-T1): 0.3167090300485398, Change in Anxiety sensitivity (T2-T1): 0.5185167790561809, Change in Fear of bodily sensations (T2-T1): 1.781472184716851, Change in Dysfunctional attitudes (T2-T1): 0.8097267435207866, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -1.09104984935167\n",
      "NO\n",
      "Time taken: 60.093127965927124 seconds\n",
      "NO    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Profiled simple prompt",
   "id": "bfce78f53e271dd3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:46:01.317594Z",
     "start_time": "2025-05-18T15:45:07.530604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "profiled_simple_y_pred_deeps = []\n",
    "profiled_simple_thinking_deeps = []\n",
    "\n",
    "client = OpenAI(api_key = os.environ.get(\"DeepSeek_API_Key\"), base_url = \"https://api.deepseek.com\")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_profiled_simple_prompt:\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"deepseek-reasoner\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": profiled_simple_instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        stream = False\n",
    "    )\n",
    "\n",
    "    if response.choices[0].message.content.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.chat.completions.create(\n",
    "            model = \"deepseek-reasoner\",\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": retry_instruction},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            stream = False\n",
    "        )\n",
    "        \n",
    "    profiled_simple_y_pred_deeps.append(response.choices[0].message.content)\n",
    "    profiled_simple_thinking_deeps.append(response.choices[0].message.reasoning_content)\n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_deeps_profiled_simple = end - start\n",
    "time_deeps_profiled_simple_df = pd.DataFrame({\"time\": [time_deeps_profiled_simple]})\n",
    "time_deeps_profiled_simple_df.to_csv(\"../exp/times_LLMs/DeepSeek/time_deeps_profiled_simple_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_profiled_simple_deeps = pd.Series(profiled_simple_y_pred_deeps).value_counts()\n",
    "print(counts_profiled_simple_deeps)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "profiled_simple_y_pred_deeps_val = [1 if response == \"YES\" else 0 for response in profiled_simple_y_pred_deeps]\n",
    "\n",
    "# save the array to a csv file\n",
    "profiled_simple_df_deeps = pd.DataFrame(profiled_simple_y_pred_deeps_val, columns = [\"y_pred\"])\n",
    "profiled_simple_df_deeps.to_csv(\"../exp/y_pred_LLMs/DeepSeek/y_pred_deeps_profiled_simple_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "profiled_simple_prompt_df_thinking_deeps = pd.DataFrame(profiled_simple_thinking_deeps, columns = [\"thinking\"])\n",
    "profiled_simple_prompt_df_thinking_deeps.to_csv(\"../exp/y_pred_LLMs/DeepSeek/Thinking/thinking_deeps_profiled_simple_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "bdac2e6ac9872c91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in clinical psychology and mental health diagnostics. You are trained to analyze longitudinal data to assess whether a person develops a psychological disorder over time. You know how to analyze data measured with F-DIPS structural interviews at two time points T1 and T2. Does the following person develop a psychological disorder between time point T1 and T2? Data: T1 Positive mental health: -0.0279170753483525, T1 Social support: 0.1421238143169474, T1 General self-efficacy: 0.3649793457412237, T1 Life satisfaction: 0.3372886835461141, T1 Stress: 0.4419361727222826, T1 Problem-focused coping: 1.7319368683783989, T1 Emotion-focused coping: 0.2078300133169115, T1 Anxiety sensitivity: 0.1594156886399411, T1 Fear of bodily sensations: 0.2863750811390516, T1 Dysfunctional attitudes: 0.2750686254386546, T1 General psychopathology: Global Severity Index (GSI): 0.0172227087467131, Education: 2.0, T1 BMI: 1.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): 0.7057099569575698, Change in General self-efficacy (T2-T1): -0.1819798191096402, Change in Life satisfaction (T2-T1): 0.1407378746091848, Change in Anxiety sensitivity (T2-T1): -0.8617238998696288, Change in Fear of bodily sensations (T2-T1): -0.846980042266938, Change in Dysfunctional attitudes (T2-T1): 0.4849021865236002, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.8256637821414634\n",
      "NO\n",
      "You are an expert in clinical psychology and mental health diagnostics. You are trained to analyze longitudinal data to assess whether a person develops a psychological disorder over time. You know how to analyze data measured with F-DIPS structural interviews at two time points T1 and T2. Does the following person develop a psychological disorder between time point T1 and T2? Data: T1 Positive mental health: -0.0279170753483525, T1 Social support: 1.0790451808002794, T1 General self-efficacy: -2.3549374763869046, T1 Life satisfaction: -0.471818725223128, T1 Stress: 0.0419806820921755, T1 Problem-focused coping: 0.8532782878883876, T1 Emotion-focused coping: -0.8456281474585625, T1 Anxiety sensitivity: 0.5557512543954795, T1 Fear of bodily sensations: 1.241666884265578, T1 Dysfunctional attitudes: -0.2250808121114963, T1 General psychopathology: Global Severity Index (GSI): 1.5062628203345918, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): -0.460474567131092, Change in General self-efficacy (T2-T1): 1.7532147220525784, Change in Life satisfaction (T2-T1): 0.3167090300485398, Change in Anxiety sensitivity (T2-T1): 0.5185167790561809, Change in Fear of bodily sensations (T2-T1): 1.781472184716851, Change in Dysfunctional attitudes (T2-T1): 0.8097267435207866, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -1.09104984935167\n",
      "YES\n",
      "Time taken: 53.76289391517639 seconds\n",
      "NO     1\n",
      "YES    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Few shot prompt",
   "id": "f41f6d5b13c773a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:51:00.985261Z",
     "start_time": "2025-05-18T15:46:01.709688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "few_shot_y_pred_deeps = []\n",
    "few_shot_thinking_deeps = []\n",
    "\n",
    "client = OpenAI(api_key = os.environ.get(\"DeepSeek_API_Key\"), base_url = \"https://api.deepseek.com\")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_few_shot_prompt:\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"deepseek-reasoner\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": few_shot_instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        stream = False\n",
    "    )\n",
    "\n",
    "    if response.choices[0].message.content.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.chat.completions.create(\n",
    "            model = \"deepseek-reasoner\",\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": retry_instruction},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            stream = False\n",
    "        )\n",
    "\n",
    "    few_shot_y_pred_deeps.append(response.choices[0].message.content)\n",
    "    few_shot_thinking_deeps.append(response.choices[0].message.reasoning_content)\n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_deeps_few_shot = end - start\n",
    "time_deeps_few_shot_df = pd.DataFrame({\"time\": [time_deeps_few_shot]})\n",
    "time_deeps_few_shot_df.to_csv(\"../exp/times_LLMs/DeepSeek/time_deeps_few_shot_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_few_shot_deeps = pd.Series(few_shot_y_pred_deeps).value_counts()\n",
    "print(counts_few_shot_deeps)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "few_shot_y_pred_deeps_val = [1 if response == \"YES\" else 0 for response in few_shot_y_pred_deeps]\n",
    "\n",
    "# save the array to a csv file\n",
    "few_shot_df_deeps = pd.DataFrame(few_shot_y_pred_deeps_val, columns = [\"y_pred\"])\n",
    "few_shot_df_deeps.to_csv(\"../exp/y_pred_LLMs/DeepSeek/y_pred_deeps_few_shot_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "few_shot_prompt_df_thinking_deeps = pd.DataFrame(few_shot_thinking_deeps, columns = [\"thinking\"])\n",
    "few_shot_prompt_df_thinking_deeps.to_csv(\"../exp/y_pred_LLMs/DeepSeek/Thinking/thinking_deeps_few_shot_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "dc9a2fbf596bb1b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please refer to the following examples of correctly classified data points with 'Total mental disorders incidence point prevalence' being the true classification: ['Example 1: T1 Positive mental health: -0.0279170753483525, T1 Social support: -0.167356999046327, T1 General self-efficacy: -0.5416595949681524, T1 Life satisfaction: -0.471818725223128, T1 Stress: 0.241958427407229, T1 Problem-focused coping: 0.8532782878883876, T1 Emotion-focused coping: 0.2078300133169115, T1 Anxiety sensitivity: 0.6878631096473256, T1 Fear of bodily sensations: 0.3819042614516962, T1 Dysfunctional attitudes: -0.2667599319073422, T1 General psychopathology: Global Severity Index (GSI): -0.2733217032704306, Education: 2.0, T1 BMI: 1.0, Socioeconomic status: 1.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): 0.8692924015129987, Change in General self-efficacy (T2-T1): -0.1819798191096402, Change in Life satisfaction (T2-T1): 0.6686513409272499, Change in Anxiety sensitivity (T2-T1): -1.137772035654791, Change in Fear of bodily sensations (T2-T1): -0.8469800422669385, Change in Dysfunctional attitudes (T2-T1): 0.97213902201938, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.3833536701244625, Total mental disorders incidence point prevalence: 1.0', 'Example 2: T1 Positive mental health: 0.9385642092467285, T1 Social support: 0.7229302722726627, T1 General self-efficacy: 0.3649793457412237, T1 Life satisfaction: 0.4991101652999784, T1 Stress: 0.241958427407229, T1 Problem-focused coping: 1.5562051522803964, T1 Emotion-focused coping: 1.7474996329118353, T1 Anxiety sensitivity: -0.1048080218637509, T1 Fear of bodily sensations: 1.5282544252035275, T1 Dysfunctional attitudes: 0.6501807036012678, T1 General psychopathology: Global Severity Index (GSI): 0.1624949147552884, Education: 2.0, T1 BMI: 1.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): 0.0935950031373072, Change in General self-efficacy (T2-T1): 0.0944765439135338, Change in Life satisfaction (T2-T1): 0.1407378746091641, Change in Anxiety sensitivity (T2-T1): -0.3096276282993049, Change in Fear of bodily sensations (T2-T1): -1.236380372190466, Change in Dysfunctional attitudes (T2-T1): -0.0023346489721794, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.6045087261329618, Total mental disorders incidence point prevalence: 1.0', 'Example 3: T1 Positive mental health: 0.9385642092467285, T1 Social support: 0.8119589994045722, T1 General self-efficacy: 0.3649793457412237, T1 Life satisfaction: 0.8227531288076683, T1 Stress: -0.557952553852985, T1 Problem-focused coping: 1.204741720084392, T1 Emotion-focused coping: -0.3594166886391129, T1 Anxiety sensitivity: 0.5557512543954795, T1 Fear of bodily sensations: 0.3819042614516962, T1 Dysfunctional attitudes: -0.2667599319073422, T1 General psychopathology: Global Severity Index (GSI): 0.6542168480146436, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -1.2686339879073294, Change in Social support (T2-T1): 0.0935950031373072, Change in General self-efficacy (T2-T1): -0.1819798191096402, Change in Life satisfaction (T2-T1): 0.4926801854879145, Change in Anxiety sensitivity (T2-T1): -0.1716035604067239, Change in Fear of bodily sensations (T2-T1): -0.4575797123434105, Change in Dysfunctional attitudes (T2-T1): -0.0023346489721794, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -1.0706737655396472, Total mental disorders incidence point prevalence: 0.0', 'Example 4: T1 Positive mental health: 0.4553235669491879, T1 Social support: 0.3668153637450668, T1 General self-efficacy: 0.3649793457412237, T1 Life satisfaction: 0.0136457200384251, T1 Stress: -0.7579302991680386, T1 Problem-focused coping: 1.4683392942313953, T1 Emotion-focused coping: 0.3699004995900614, T1 Anxiety sensitivity: 0.2915275438917872, T1 Fear of bodily sensations: 0.5729626220770014, T1 Dysfunctional attitudes: 1.066971901559727, T1 General psychopathology: Global Severity Index (GSI): 0.2869556530491485, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -1.2686339879073294, Change in Social support (T2-T1): -0.5712884811847251, Change in General self-efficacy (T2-T1): -0.1819798191096402, Change in Life satisfaction (T2-T1): 0.1407378746091632, Change in Anxiety sensitivity (T2-T1): -0.4476516961918859, Change in Fear of bodily sensations (T2-T1): -0.554929794824284, Change in Dysfunctional attitudes (T2-T1): 0.1600776295264137, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): 0.7035364590678871, Total mental disorders incidence point prevalence: 0.0', 'Example 5: T1 Positive mental health: 0.4553235669491879, T1 Social support: 0.5448728180088648, T1 General self-efficacy: 1.498278021627944, T1 Life satisfaction: -1.28092613399239, T1 Stress: 0.6419139180373361, T1 Problem-focused coping: 1.4683392942313953, T1 Emotion-focused coping: 0.5319709858632113, T1 Anxiety sensitivity: 0.8199749648991717, T1 Fear of bodily sensations: 0.5729626220770014, T1 Dysfunctional attitudes: 0.7752180629888056, T1 General psychopathology: Global Severity Index (GSI): -0.4185939092790058, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.235399282050399, Change in Social support (T2-T1): -0.0172189109163777, Change in General self-efficacy (T2-T1): -0.4584361821328143, Change in Life satisfaction (T2-T1): -0.3871755917089017, Change in Anxiety sensitivity (T2-T1): -0.9997479677622098, Change in Fear of bodily sensations (T2-T1): -0.7496299597860477, Change in Dysfunctional attitudes (T2-T1): 0.3766273341912047, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): 0.0147254306908422, Total mental disorders incidence point prevalence: 0.0'] Based on the previous example data prompts, classify the following data. Does the following person develop a psychological disorder between time point T1 and T2? Data: T1 Positive mental health: -0.0279170753483525, T1 Social support: 0.1421238143169474, T1 General self-efficacy: 0.3649793457412237, T1 Life satisfaction: 0.3372886835461141, T1 Stress: 0.4419361727222826, T1 Problem-focused coping: 1.7319368683783989, T1 Emotion-focused coping: 0.2078300133169115, T1 Anxiety sensitivity: 0.1594156886399411, T1 Fear of bodily sensations: 0.2863750811390516, T1 Dysfunctional attitudes: 0.2750686254386546, T1 General psychopathology: Global Severity Index (GSI): 0.0172227087467131, Education: 2.0, T1 BMI: 1.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): 0.7057099569575698, Change in General self-efficacy (T2-T1): -0.1819798191096402, Change in Life satisfaction (T2-T1): 0.1407378746091848, Change in Anxiety sensitivity (T2-T1): -0.8617238998696288, Change in Fear of bodily sensations (T2-T1): -0.846980042266938, Change in Dysfunctional attitudes (T2-T1): 0.4849021865236002, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.8256637821414634\n",
      "NO\n",
      "Please refer to the following examples of correctly classified data points with 'Total mental disorders incidence point prevalence' being the true classification: ['Example 1: T1 Positive mental health: 0.4553235669491879, T1 Social support: 0.6339015451407742, T1 General self-efficacy: -0.0883401246134643, T1 Life satisfaction: -0.1481757617154391, T1 Stress: 0.241958427407229, T1 Problem-focused coping: 0.413948997643382, T1 Emotion-focused coping: -0.6025224180488378, T1 Anxiety sensitivity: 0.8199749648991717, T1 Fear of bodily sensations: 1.4327252448908838, T1 Dysfunctional attitudes: -0.5585137704782637, T1 General psychopathology: Global Severity Index (GSI): 0.5256754297767214, Education: 3.0, T1 BMI: 2.0, Socioeconomic status: 3.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): -0.1280328249700628, Change in General self-efficacy (T2-T1): 0.3709329069367079, Change in Life satisfaction (T2-T1): 1.196564807245358, Change in Anxiety sensitivity (T2-T1): 0.6565408469487619, Change in Fear of bodily sensations (T2-T1): 0.6132711949462831, Change in Dysfunctional attitudes (T2-T1): -0.2730217798031681, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.9141258045448658, Total mental disorders incidence point prevalence: 0.0', 'Example 2: T1 Positive mental health: -0.0279170753483525, T1 Social support: -0.0783282719144384, T1 General self-efficacy: -0.3149998597908083, T1 Life satisfaction: 0.0136457200384251, T1 Stress: -0.157997063222878, T1 Problem-focused coping: 0.9411441459373888, T1 Emotion-focused coping: 0.2078300133169115, T1 Anxiety sensitivity: 0.2915275438917872, T1 Fear of bodily sensations: 0.7640209827023066, T1 Dysfunctional attitudes: 0.6501807036012678, T1 General psychopathology: Global Severity Index (GSI): 1.2785608794559835, Education: 3.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): -0.0172189109163517, Change in General self-efficacy (T2-T1): 0.647389269959882, Change in Life satisfaction (T2-T1): -0.7391179025876333, Change in Anxiety sensitivity (T2-T1): 0.2424686432710189, Change in Fear of bodily sensations (T2-T1): -0.4575797123434101, Change in Dysfunctional attitudes (T2-T1): 0.8638641696869843, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -1.2118154192619288, Total mental disorders incidence point prevalence: 0.0', 'Example 3: T1 Positive mental health: -0.9943983599434336, T1 Social support: 0.3668153637450668, T1 General self-efficacy: -0.9949790653228404, T1 Life satisfaction: 0.4991101652999784, T1 Stress: 0.8418916633523896, T1 Problem-focused coping: 1.204741720084392, T1 Emotion-focused coping: -0.9266633905951376, T1 Anxiety sensitivity: -0.3690317323674431, T1 Fear of bodily sensations: 0.3819042614516962, T1 Dysfunctional attitudes: -0.8085884892533391, T1 General psychopathology: Global Severity Index (GSI): 2.123669695871029, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): 0.281218070878066, Change in Social support (T2-T1): -0.0172189109163777, Change in General self-efficacy (T2-T1): 0.647389269959882, Change in Life satisfaction (T2-T1): 0.3167090300485388, Change in Anxiety sensitivity (T2-T1): 0.6565408469487619, Change in Fear of bodily sensations (T2-T1): 0.1265207825418816, Change in Dysfunctional attitudes (T2-T1): 0.7555893173545889, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -3.1256763646298897, Total mental disorders incidence point prevalence: 0.0', 'Example 4: T1 Positive mental health: -0.0279170753483525, T1 Social support: 0.8119589994045722, T1 General self-efficacy: -0.3149998597908083, T1 Life satisfaction: 0.4843991215041691, T1 Stress: -0.3579748085379315, T1 Problem-focused coping: 0.8532782878883876, T1 Emotion-focused coping: -0.5214871749122628, T1 Anxiety sensitivity: 0.5557512543954795, T1 Fear of bodily sensations: 1.241666884265578, T1 Dysfunctional attitudes: 0.9419345421721892, T1 General psychopathology: Global Severity Index (GSI): 0.0172227087467131, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 1.0, Change in Positive mental health (T2-T1): 0.7978354238065312, Change in Social support (T2-T1): -0.0172189109163777, Change in General self-efficacy (T2-T1): 0.647389269959882, Change in Life satisfaction (T2-T1): -0.0192359030629718, Change in Anxiety sensitivity (T2-T1): 0.5185167790561809, Change in Fear of bodily sensations (T2-T1): 0.41857102998452, Change in Dysfunctional attitudes (T2-T1): -0.1647469274707726, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.3391226589227578, Total mental disorders incidence point prevalence: 0.0', 'Example 5: T1 Positive mental health: -0.511157717645893, T1 Social support: 0.9009877265364608, T1 General self-efficacy: -0.3149998597908083, T1 Life satisfaction: -0.9572831704846811, T1 Stress: 0.0419806820921755, T1 Problem-focused coping: 1.204741720084392, T1 Emotion-focused coping: 0.2888652564534865, T1 Anxiety sensitivity: -0.3690317323674431, T1 Fear of bodily sensations: -0.3823291810495087, T1 Dysfunctional attitudes: -0.2250808121114963, T1 General psychopathology: Global Severity Index (GSI): 0.5256754297767214, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 3.0, Change in Positive mental health (T2-T1): -0.235399282050399, Change in Social support (T2-T1): -0.2388467390237219, Change in General self-efficacy (T2-T1): 0.923845632983056, Change in Life satisfaction (T2-T1): 0.1407378746091632, Change in Anxiety sensitivity (T2-T1): -0.4476516961918859, Change in Fear of bodily sensations (T2-T1): 0.0291707000609912, Change in Dysfunctional attitudes (T2-T1): 0.4849021865236002, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.2506606365193577, Total mental disorders incidence point prevalence: 1.0'] Based on the previous example data prompts, classify the following data. Does the following person develop a psychological disorder between time point T1 and T2? Data: T1 Positive mental health: -0.0279170753483525, T1 Social support: 1.0790451808002794, T1 General self-efficacy: -2.3549374763869046, T1 Life satisfaction: -0.471818725223128, T1 Stress: 0.0419806820921755, T1 Problem-focused coping: 0.8532782878883876, T1 Emotion-focused coping: -0.8456281474585625, T1 Anxiety sensitivity: 0.5557512543954795, T1 Fear of bodily sensations: 1.241666884265578, T1 Dysfunctional attitudes: -0.2250808121114963, T1 General psychopathology: Global Severity Index (GSI): 1.5062628203345918, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): -0.460474567131092, Change in General self-efficacy (T2-T1): 1.7532147220525784, Change in Life satisfaction (T2-T1): 0.3167090300485398, Change in Anxiety sensitivity (T2-T1): 0.5185167790561809, Change in Fear of bodily sensations (T2-T1): 1.781472184716851, Change in Dysfunctional attitudes (T2-T1): 0.8097267435207866, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -1.09104984935167\n",
      "YES\n",
      "Time taken: 299.2469460964203 seconds\n",
      "NO     1\n",
      "YES    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Vignette prompt",
   "id": "b905c177e4fbbe97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:52:54.940647Z",
     "start_time": "2025-05-18T15:51:01.466286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vignette_y_pred_deeps = []\n",
    "vignette_thinking_deeps = []\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"DeepSeek_API_Key\"),\n",
    "    base_url = \"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_vignette_prompt:\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"deepseek-reasoner\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": vignette_instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        stream = False\n",
    "    )\n",
    "\n",
    "    if response.choices[0].message.content.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        response = client.chat.completions.create(\n",
    "            model = \"deepseek-reasoner\",\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": retry_instruction},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            stream = False\n",
    "        )\n",
    "\n",
    "    vignette_y_pred_deeps.append(response.choices[0].message.content)\n",
    "    vignette_thinking_deeps.append(response.choices[0].message.reasoning_content)\n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_deeps_vignette = end - start\n",
    "time_deeps_vignette_df = pd.DataFrame({\"time\": [time_deeps_vignette]})\n",
    "time_deeps_vignette_df.to_csv(\"../exp/times_LLMs/DeepSeek/time_deeps_vignette_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_vignette_deeps = pd.Series(vignette_y_pred_deeps).value_counts()\n",
    "print(counts_vignette_deeps)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "vignette_y_pred_deeps_val = [1 if response == \"YES\" else 0 for response in vignette_y_pred_deeps]\n",
    "\n",
    "# save the array to a csv file\n",
    "vignette_df_deeps = pd.DataFrame(vignette_y_pred_deeps_val, columns = [\"y_pred\"])\n",
    "vignette_df_deeps.to_csv(\"../exp/y_pred_LLMs/DeepSeek/y_pred_deeps_vignette_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "vignette_prompt_df_thinking_deeps = pd.DataFrame(vignette_thinking_deeps, columns = [\"thinking\"])\n",
    "vignette_prompt_df_thinking_deeps.to_csv(\"../exp/y_pred_LLMs/DeepSeek/Thinking/thinking_deeps_vignette_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "223551e5e99ddca3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A woman with a BMI of 1.0, an education level of 2.0, and a socioeconomic status of 2.0 has the following psychological profile: At time point 1, she showed average (-0.028) positive mental health, average (0.142) social support, and average (0.365) general self-efficacy. Her life satisfaction was average (0.337), and she relied on above average (1.732) problem-focused and average (0.208) emotion-focused coping strategies. Anxiety sensitivity was average (0.159), and her fear of bodily sensations was average (0.286). She reported average (0.275) levels of dysfunctional attitudes and average (0.017) levels of general psychopathology. Her stress level was average (0.442). By time point 2, approximately 17 months later, she reported similar (-0.752) positive mental health, similar (0.706) social support, and similar (-0.182) self-efficacy. Life satisfaction was similar (0.141). Anxiety sensitivity was reported to be similar (-0.862), and fear of bodily sensations was similar (-0.847). Dysfunctional attitudes were similar (0.485), and general psychopathology was similar (-0.826) compared to time point 1. Does this person develop a psychological disorder between time point T1 and T2?\n",
      "NO\n",
      "A woman with a BMI of 2.0, an education level of 2.0, and a socioeconomic status of 2.0 has the following psychological profile: At time point 1, she showed average (-0.028) positive mental health, above average (1.079) social support, and below average (-2.355) general self-efficacy. Her life satisfaction was average (-0.472), and she relied on average (0.853) problem-focused and average (-0.846) emotion-focused coping strategies. Anxiety sensitivity was average (0.556), and her fear of bodily sensations was above average (1.242). She reported average (-0.225) levels of dysfunctional attitudes and above average (1.506) levels of general psychopathology. Her stress level was average (0.042). By time point 2, approximately 17 months later, she reported similar (-0.752) positive mental health, similar (-0.460) social support, and increased (1.753) self-efficacy. Life satisfaction was similar (0.317). Anxiety sensitivity was reported to be similar (0.519), and fear of bodily sensations was increased (1.781). Dysfunctional attitudes were similar (0.810), and general psychopathology was decreased (-1.091) compared to time point 1. Does this person develop a psychological disorder between time point T1 and T2?\n",
      "NO\n",
      "Time taken: 113.45054316520691 seconds\n",
      "NO    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Thinking prompt",
   "id": "36ae9c63ada712ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T12:30:55.475588Z",
     "start_time": "2025-05-19T12:30:01.773530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "thinking_prompt_y_pred_deeps = []\n",
    "thinking_prompt_explanation_deeps = []\n",
    "thinking_prompt_thinking_deeps = []\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"DeepSeek_API_Key\"),\n",
    "    base_url = \"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_thinking_prompt:\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"deepseek-reasoner\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": thinking_instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        stream = False\n",
    "    )\n",
    "    try:\n",
    "        prediction = re.findall(r'Prediction: (.*)', response.choices[0].message.content)[0].strip()\n",
    "        explanation = re.findall(r'Explanation: (.*)', response.choices[0].message.content)[0].strip()\n",
    "        thinking_prompt_y_pred_deeps.append(prediction)\n",
    "        thinking_prompt_explanation_deeps.append(explanation)\n",
    "        thinking_prompt_thinking_deeps.append(response.choices[0].message.reasoning_content)\n",
    "        print(prediction)\n",
    "    except IndexError:\n",
    "        print(\"IndexError\")\n",
    "        thinking_prompt_y_pred_deeps.append(\"IndexError\")\n",
    "        thinking_prompt_explanation_deeps.append(\"IndexError\")\n",
    "        thinking_prompt_thinking_deeps.append(\"IndexError\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_deeps_thinking_prompt = end - start\n",
    "time_deeps_thinking_prompt_df = pd.DataFrame({\"time\": [time_deeps_thinking_prompt]})\n",
    "time_deeps_thinking_prompt_df.to_csv(\"../exp/times_LLMs/DeepSeek/time_deeps_thinking_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_thinking_prompt_deeps = pd.Series(thinking_prompt_y_pred_deeps).value_counts()\n",
    "print(counts_thinking_prompt_deeps)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "thinking_prompt_y_pred_deeps_val = [1 if response == \"YES\" else 0 for response in thinking_prompt_y_pred_deeps]\n",
    "\n",
    "# save the array to a csv file\n",
    "thinking_prompt_df_deeps = pd.DataFrame(thinking_prompt_y_pred_deeps_val, columns = [\"y_pred\"])\n",
    "thinking_prompt_df_deeps.to_csv(\"../exp/y_pred_LLMs/DeepSeek/y_pred_deeps_thinking_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "thinking_prompt_df_thinking_deeps = pd.DataFrame(thinking_prompt_thinking_deeps, columns = [\"thinking\"])\n",
    "thinking_prompt_df_thinking_deeps.to_csv(\"../exp/y_pred_LLMs/DeepSeek/Thinking/thinking_deeps_thinking_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "thinking_prompt_df_explanation_deeps = pd.DataFrame(thinking_prompt_explanation_deeps, columns = [\"thinking\"])\n",
    "thinking_prompt_df_explanation_deeps.to_csv(\"../exp/y_pred_LLMs/DeepSeek/explanation_deeps_thinking_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "1e4c957b61635f6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO  \n",
      "YES  \n",
      "Time taken: 53.60858988761902 seconds\n",
      "NO       1\n",
      "YES      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5b34a0d70a4c6d00"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7 Grok (xAI)",
   "id": "2de5034f0724ce29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.1 Testing prompting",
   "id": "890c30dbf136a459"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:23:23.609563Z",
     "start_time": "2025-05-18T15:23:07.546284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# client = OpenAI(\n",
    "#     api_key = os.environ.get(\"XAI_API_KEY\"),\n",
    "#     base_url = \"https://api.x.ai/v1\",\n",
    "# )\n",
    "#\n",
    "# completion = client.chat.completions.create(\n",
    "#     model = \"grok-3-beta\",\n",
    "#     # model = \"grok-3-mini-beta\",\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": simple_instruction},\n",
    "#         {\"role\": \"user\", \"content\": X_test_simple_prompt[0]},\n",
    "#     ],\n",
    "#     # reasoning_effort = \"high\"\n",
    "# )\n",
    "# print(completion.choices[0].message)"
   ],
   "id": "85c8051c51bdfa03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='NO', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning_content='First, the user asks me to respond only with YES or NO to whether the person develops a psychological disorder between T1 and T2.\\n\\nI have data points at T1 and changes from T1 to T2 for various psychological measures.\\n\\nKey data includes:\\n\\n- T1 General psychopathology: Global Severity Index (GSI): 0.0172227087467131\\n\\n- Change in General psychopathology: GSI (T2-T1): -0.8256637821414634\\n\\nGSI is a measure of general psychopathology. A higher GSI typically indicates more severe symptoms.\\n\\nAt T1, GSI is very low (0.0172), suggesting low psychopathology.\\n\\nThe change from T1 to T2 is -0.8257, which is negative. This means the GSI decreased from T1 to T2. So, at T2, the GSI is even lower.\\n\\nA decrease in GSI suggests an improvement in mental health, not the development of a disorder.\\n\\nThe question is about developing a psychological disorder. If GSI decreases, it\\'s unlikely that a disorder developed.\\n\\nI should check if there\\'s any other indicator. The data includes changes in other variables like anxiety sensitivity, fear of bodily sensations, etc., which decreased, and some increased like dysfunctional attitudes.\\n\\nBut the primary measure for psychopathology is the GSI. A negative change indicates reduction in symptoms.\\n\\nTo develop a disorder, we might expect an increase in GSI or other pathological measures.\\n\\nHere, change in GSI is negative, meaning less psychopathology.\\n\\nAlso, change in anxiety sensitivity and fear of bodily sensations are both negative and large in magnitude, suggesting improvement.\\n\\nChange in dysfunctional attitudes is positive (increase), which might be a risk factor, but GSI is the direct measure.\\n\\nMoreover, positive mental health changed negatively (-0.752), which could indicate worsening, but GSI is specifically for psychopathology.\\n\\nThe GSI change is strongly negative, indicating no development of disorder.\\n\\nPerhaps I need to know the absolute values at T2.\\n\\nT1 GSI is 0.0172, change is -0.8257, so T2 GSI = T1 GSI + change = 0.0172 - 0.8257 = -0.8085. A negative GSI? That might not make sense if GSI is a score that can\\'t be negative, but in the data, it\\'s given as a change, so perhaps it\\'s scaled or possible.\\n\\nAssuming the scale allows it, a very low or negative value would still indicate low psychopathology.\\n\\nIn standard GSI, scores are usually non-negative, but perhaps this is standardized or z-scored.\\n\\nLooking back at T1 GSI: 0.0172, which is very low, and change is large negative, so T2 is even lower.\\n\\nThis suggests improvement, not development of a disorder.\\n\\nOther changes: positive mental health decreased, but GSI improved.\\n\\nThe question is about developing a psychological disorder, and GSI is a direct measure.\\n\\nI think based on this, the answer should be NO.\\n\\nBut let me make sure there\\'s no trick. The user said \"Respond only with YES or NO\", so I need to be concise.\\n\\nPerhaps I need to consider if any disorder developed despite GSI decrease, but that seems unlikely with the data provided.\\n\\nMaybe the changes in other variables indicate risk, but GSI is the outcome measure.\\n\\nIn the data, there\\'s \"General psychopathology: GSI\" and its change.\\n\\nI think it\\'s safe to say NO.')\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:23:23.951328Z",
     "start_time": "2025-05-18T15:23:23.945858Z"
    }
   },
   "cell_type": "code",
   "source": "# completion.choices[0].message.content",
   "id": "def9ffd295fcac01",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NO'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.2 Prompting with Grok 3 Beta",
   "id": "96ac301e06013696"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Simple prompt",
   "id": "f164c30f99e0b873"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:36:02.089806Z",
     "start_time": "2025-05-18T15:36:00.571542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "simple_prompt_y_pred_grok = []\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"XAI_API_KEY\"),\n",
    "    base_url = \"https://api.x.ai/v1\",\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_simple_prompt:\n",
    "    completion = client.chat.completions.create(\n",
    "        model = \"grok-3-beta\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": simple_instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    if completion.choices[0].message.content.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        completion = client.chat.completions.create(\n",
    "            model = \"grok-3-beta\",\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": retry_instruction},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    simple_prompt_y_pred_grok.append(completion.choices[0].message.content)\n",
    "    print(completion.choices[0].message.content)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_grok_simple_prompt = end - start\n",
    "time_grok_simple_prompt_df = pd.DataFrame({\"time\": [time_grok_simple_prompt]})\n",
    "time_grok_simple_prompt_df.to_csv(\"../exp/times_LLMs/Grok/time_grok_simple_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_simple_grok = pd.Series(simple_prompt_y_pred_grok).value_counts()\n",
    "print(counts_simple_grok)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "simple_prompt_y_pred_grok = [1 if response == \"YES\" else 0 if response == \"NO\" else np.nan for response in simple_prompt_y_pred_grok]\n",
    "\n",
    "# save the array to a csv file\n",
    "simple_prompt_df_grok = pd.DataFrame(simple_prompt_y_pred_grok, columns = [\"y_pred\"])\n",
    "simple_prompt_df_grok.to_csv(\"../exp/y_pred_LLMs/Grok/y_pred_grok_simple_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "7b9d68135474d0dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the following person develop a psychological disorder between time point T1 and T2? Data: T1 Positive mental health: -0.0279170753483525, T1 Social support: 0.1421238143169474, T1 General self-efficacy: 0.3649793457412237, T1 Life satisfaction: 0.3372886835461141, T1 Stress: 0.4419361727222826, T1 Problem-focused coping: 1.7319368683783989, T1 Emotion-focused coping: 0.2078300133169115, T1 Anxiety sensitivity: 0.1594156886399411, T1 Fear of bodily sensations: 0.2863750811390516, T1 Dysfunctional attitudes: 0.2750686254386546, T1 General psychopathology: Global Severity Index (GSI): 0.0172227087467131, Education: 2.0, T1 BMI: 1.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): 0.7057099569575698, Change in General self-efficacy (T2-T1): -0.1819798191096402, Change in Life satisfaction (T2-T1): 0.1407378746091848, Change in Anxiety sensitivity (T2-T1): -0.8617238998696288, Change in Fear of bodily sensations (T2-T1): -0.846980042266938, Change in Dysfunctional attitudes (T2-T1): 0.4849021865236002, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.8256637821414634\n",
      "NO\n",
      "Does the following person develop a psychological disorder between time point T1 and T2? Data: T1 Positive mental health: -0.0279170753483525, T1 Social support: 1.0790451808002794, T1 General self-efficacy: -2.3549374763869046, T1 Life satisfaction: -0.471818725223128, T1 Stress: 0.0419806820921755, T1 Problem-focused coping: 0.8532782878883876, T1 Emotion-focused coping: -0.8456281474585625, T1 Anxiety sensitivity: 0.5557512543954795, T1 Fear of bodily sensations: 1.241666884265578, T1 Dysfunctional attitudes: -0.2250808121114963, T1 General psychopathology: Global Severity Index (GSI): 1.5062628203345918, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): -0.460474567131092, Change in General self-efficacy (T2-T1): 1.7532147220525784, Change in Life satisfaction (T2-T1): 0.3167090300485398, Change in Anxiety sensitivity (T2-T1): 0.5185167790561809, Change in Fear of bodily sensations (T2-T1): 1.781472184716851, Change in Dysfunctional attitudes (T2-T1): 0.8097267435207866, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -1.09104984935167\n",
      "NO\n",
      "Time taken: 1.4973251819610596 seconds\n",
      "NO    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Class definition prompt",
   "id": "4cf0edda07794184"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:35:46.004670Z",
     "start_time": "2025-05-18T15:35:34.557395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_def_y_pred_grok = []\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"XAI_API_KEY\"),\n",
    "    base_url = \"https://api.x.ai/v1\",\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_class_definitions_prompt:\n",
    "    completion = client.chat.completions.create(\n",
    "        model = \"grok-3-beta\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": class_definitions_instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    if completion.choices[0].message.content.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        completion = client.chat.completions.create(\n",
    "            model = \"grok-3-beta\",\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": retry_instruction},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    class_def_y_pred_grok.append(completion.choices[0].message.content)\n",
    "    print(completion.choices[0].message.content)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_grok_class_definitions = end - start\n",
    "time_grok_class_definitions_df = pd.DataFrame({\"time\": [time_grok_class_definitions]})\n",
    "time_grok_class_definitions_df.to_csv(\"../exp/times_LLMs/Grok/time_grok_class_definitions_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_class_def_grok = pd.Series(class_def_y_pred_grok).value_counts()\n",
    "print(counts_class_def_grok)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "class_def_y_pred_grok = [1 if response == \"YES\" else 0 for response in class_def_y_pred_grok]\n",
    "\n",
    "# save the array to a csv file\n",
    "class_def_df_grok = pd.DataFrame(class_def_y_pred_grok, columns = [\"y_pred\"])\n",
    "class_def_df_grok.to_csv(\"../exp/y_pred_LLMs/Grok/y_pred_grok_class_definitions_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "6fda4258177ebdd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following data, classify whether this person develops a psychological disorder between T1 and T2 according to the instructions provided and data measured by F-DIPS structural interviews. Respond with YES or NO. Instructions: NO: The person did not develop any new psychological disorder between T1 and T2. This means they were either healthy at both time points, had an ongoing disorder across both time points, or had already recovered from a previous disorder. YES: The person was psychologically healthy at T1 but developed a psychological disorder at T2. Does the following person develop a psychological disorder between time point T1 and T2? Data: T1 Positive mental health: -0.0279170753483525, T1 Social support: 0.1421238143169474, T1 General self-efficacy: 0.3649793457412237, T1 Life satisfaction: 0.3372886835461141, T1 Stress: 0.4419361727222826, T1 Problem-focused coping: 1.7319368683783989, T1 Emotion-focused coping: 0.2078300133169115, T1 Anxiety sensitivity: 0.1594156886399411, T1 Fear of bodily sensations: 0.2863750811390516, T1 Dysfunctional attitudes: 0.2750686254386546, T1 General psychopathology: Global Severity Index (GSI): 0.0172227087467131, Education: 2.0, T1 BMI: 1.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): 0.7057099569575698, Change in General self-efficacy (T2-T1): -0.1819798191096402, Change in Life satisfaction (T2-T1): 0.1407378746091848, Change in Anxiety sensitivity (T2-T1): -0.8617238998696288, Change in Fear of bodily sensations (T2-T1): -0.846980042266938, Change in Dysfunctional attitudes (T2-T1): 0.4849021865236002, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.8256637821414634\n",
      "NO\n",
      "Given the following data, classify whether this person develops a psychological disorder between T1 and T2 according to the instructions provided and data measured by F-DIPS structural interviews. Respond with YES or NO. Instructions: NO: The person did not develop any new psychological disorder between T1 and T2. This means they were either healthy at both time points, had an ongoing disorder across both time points, or had already recovered from a previous disorder. YES: The person was psychologically healthy at T1 but developed a psychological disorder at T2. Does the following person develop a psychological disorder between time point T1 and T2? Data: T1 Positive mental health: -0.0279170753483525, T1 Social support: 1.0790451808002794, T1 General self-efficacy: -2.3549374763869046, T1 Life satisfaction: -0.471818725223128, T1 Stress: 0.0419806820921755, T1 Problem-focused coping: 0.8532782878883876, T1 Emotion-focused coping: -0.8456281474585625, T1 Anxiety sensitivity: 0.5557512543954795, T1 Fear of bodily sensations: 1.241666884265578, T1 Dysfunctional attitudes: -0.2250808121114963, T1 General psychopathology: Global Severity Index (GSI): 1.5062628203345918, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): -0.460474567131092, Change in General self-efficacy (T2-T1): 1.7532147220525784, Change in Life satisfaction (T2-T1): 0.3167090300485398, Change in Anxiety sensitivity (T2-T1): 0.5185167790561809, Change in Fear of bodily sensations (T2-T1): 1.781472184716851, Change in Dysfunctional attitudes (T2-T1): 0.8097267435207866, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -1.09104984935167\n",
      "NO\n",
      "Time taken: 11.4238440990448 seconds\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Profiled simple prompt",
   "id": "1d9a4198fc41052d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:37:08.687474Z",
     "start_time": "2025-05-18T15:36:57.124219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "profiled_simple_y_pred_grok = []\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"XAI_API_KEY\"),\n",
    "    base_url = \"https://api.x.ai/v1\",\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_profiled_simple_prompt:\n",
    "    completion = client.chat.completions.create(\n",
    "        model = \"grok-3-beta\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": profiled_simple_instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    if completion.choices[0].message.content.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        completion = client.chat.completions.create(\n",
    "            model = \"grok-3-beta\",\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": retry_instruction},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    profiled_simple_y_pred_grok.append(completion.choices[0].message.content)\n",
    "    print(completion.choices[0].message.content)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_grok_profiled_simple = end - start\n",
    "time_grok_profiled_simple_df = pd.DataFrame({\"time\": [time_grok_profiled_simple]})\n",
    "time_grok_profiled_simple_df.to_csv(\"../exp/times_LLMs/Grok/time_grok_profiled_simple_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_profiled_simple_grok = pd.Series(profiled_simple_y_pred_grok).value_counts()\n",
    "print(counts_profiled_simple_grok)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "profiled_simple_y_pred_grok_val = [1 if response == \"YES\" else 0 for response in profiled_simple_y_pred_grok]\n",
    "\n",
    "# save the array to a csv file\n",
    "profiled_simple_df_grok = pd.DataFrame(profiled_simple_y_pred_grok_val, columns = [\"y_pred\"])\n",
    "profiled_simple_df_grok.to_csv(\"../exp/y_pred_LLMs/Grok/y_pred_grok_profiled_simple_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "27d9d12f57eaa20a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in clinical psychology and mental health diagnostics. You are trained to analyze longitudinal data to assess whether a person develops a psychological disorder over time. You know how to analyze data measured with F-DIPS structural interviews at two time points T1 and T2. Does the following person develop a psychological disorder between time point T1 and T2? Data: T1 Positive mental health: -0.0279170753483525, T1 Social support: 0.1421238143169474, T1 General self-efficacy: 0.3649793457412237, T1 Life satisfaction: 0.3372886835461141, T1 Stress: 0.4419361727222826, T1 Problem-focused coping: 1.7319368683783989, T1 Emotion-focused coping: 0.2078300133169115, T1 Anxiety sensitivity: 0.1594156886399411, T1 Fear of bodily sensations: 0.2863750811390516, T1 Dysfunctional attitudes: 0.2750686254386546, T1 General psychopathology: Global Severity Index (GSI): 0.0172227087467131, Education: 2.0, T1 BMI: 1.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): 0.7057099569575698, Change in General self-efficacy (T2-T1): -0.1819798191096402, Change in Life satisfaction (T2-T1): 0.1407378746091848, Change in Anxiety sensitivity (T2-T1): -0.8617238998696288, Change in Fear of bodily sensations (T2-T1): -0.846980042266938, Change in Dysfunctional attitudes (T2-T1): 0.4849021865236002, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.8256637821414634\n",
      "NO\n",
      "You are an expert in clinical psychology and mental health diagnostics. You are trained to analyze longitudinal data to assess whether a person develops a psychological disorder over time. You know how to analyze data measured with F-DIPS structural interviews at two time points T1 and T2. Does the following person develop a psychological disorder between time point T1 and T2? Data: T1 Positive mental health: -0.0279170753483525, T1 Social support: 1.0790451808002794, T1 General self-efficacy: -2.3549374763869046, T1 Life satisfaction: -0.471818725223128, T1 Stress: 0.0419806820921755, T1 Problem-focused coping: 0.8532782878883876, T1 Emotion-focused coping: -0.8456281474585625, T1 Anxiety sensitivity: 0.5557512543954795, T1 Fear of bodily sensations: 1.241666884265578, T1 Dysfunctional attitudes: -0.2250808121114963, T1 General psychopathology: Global Severity Index (GSI): 1.5062628203345918, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): -0.460474567131092, Change in General self-efficacy (T2-T1): 1.7532147220525784, Change in Life satisfaction (T2-T1): 0.3167090300485398, Change in Anxiety sensitivity (T2-T1): 0.5185167790561809, Change in Fear of bodily sensations (T2-T1): 1.781472184716851, Change in Dysfunctional attitudes (T2-T1): 0.8097267435207866, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -1.09104984935167\n",
      "NO\n",
      "Time taken: 11.54093623161316 seconds\n",
      "NO    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Few shot prompt",
   "id": "4964eef2d6a0de79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:37:41.630485Z",
     "start_time": "2025-05-18T15:37:40.542722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "few_shot_y_pred_grok = []\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"XAI_API_KEY\"),\n",
    "    base_url = \"https://api.x.ai/v1\",\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_few_shot_prompt:\n",
    "    completion = client.chat.completions.create(\n",
    "        model = \"grok-3-beta\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": few_shot_instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    if completion.choices[0].message.content.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        completion = client.chat.completions.create(\n",
    "            model = \"grok-3-beta\",\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": retry_instruction},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    few_shot_y_pred_grok.append(completion.choices[0].message.content)\n",
    "    print(completion.choices[0].message.content)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_grok_few_shot = end - start\n",
    "time_grok_few_shot_df = pd.DataFrame({\"time\": [time_grok_few_shot]})\n",
    "time_grok_few_shot_df.to_csv(\"../exp/times_LLMs/Grok/time_grok_few_shot_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_few_shot_grok = pd.Series(few_shot_y_pred_grok).value_counts()\n",
    "print(counts_few_shot_grok)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "few_shot_y_pred_grok_val = [1 if response == \"YES\" else 0 for response in few_shot_y_pred_grok]\n",
    "\n",
    "# save the array to a csv file\n",
    "few_shot_df_grok = pd.DataFrame(few_shot_y_pred_grok_val, columns = [\"y_pred\"])\n",
    "few_shot_df_grok.to_csv(\"../exp/y_pred_LLMs/Grok/y_pred_grok_few_shot_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "9f8614647bda3e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please refer to the following examples of correctly classified data points with 'Total mental disorders incidence point prevalence' being the true classification: ['Example 1: T1 Positive mental health: -0.0279170753483525, T1 Social support: -0.167356999046327, T1 General self-efficacy: -0.5416595949681524, T1 Life satisfaction: -0.471818725223128, T1 Stress: 0.241958427407229, T1 Problem-focused coping: 0.8532782878883876, T1 Emotion-focused coping: 0.2078300133169115, T1 Anxiety sensitivity: 0.6878631096473256, T1 Fear of bodily sensations: 0.3819042614516962, T1 Dysfunctional attitudes: -0.2667599319073422, T1 General psychopathology: Global Severity Index (GSI): -0.2733217032704306, Education: 2.0, T1 BMI: 1.0, Socioeconomic status: 1.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): 0.8692924015129987, Change in General self-efficacy (T2-T1): -0.1819798191096402, Change in Life satisfaction (T2-T1): 0.6686513409272499, Change in Anxiety sensitivity (T2-T1): -1.137772035654791, Change in Fear of bodily sensations (T2-T1): -0.8469800422669385, Change in Dysfunctional attitudes (T2-T1): 0.97213902201938, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.3833536701244625, Total mental disorders incidence point prevalence: 1.0', 'Example 2: T1 Positive mental health: 0.9385642092467285, T1 Social support: 0.7229302722726627, T1 General self-efficacy: 0.3649793457412237, T1 Life satisfaction: 0.4991101652999784, T1 Stress: 0.241958427407229, T1 Problem-focused coping: 1.5562051522803964, T1 Emotion-focused coping: 1.7474996329118353, T1 Anxiety sensitivity: -0.1048080218637509, T1 Fear of bodily sensations: 1.5282544252035275, T1 Dysfunctional attitudes: 0.6501807036012678, T1 General psychopathology: Global Severity Index (GSI): 0.1624949147552884, Education: 2.0, T1 BMI: 1.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): 0.0935950031373072, Change in General self-efficacy (T2-T1): 0.0944765439135338, Change in Life satisfaction (T2-T1): 0.1407378746091641, Change in Anxiety sensitivity (T2-T1): -0.3096276282993049, Change in Fear of bodily sensations (T2-T1): -1.236380372190466, Change in Dysfunctional attitudes (T2-T1): -0.0023346489721794, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.6045087261329618, Total mental disorders incidence point prevalence: 1.0', 'Example 3: T1 Positive mental health: 0.9385642092467285, T1 Social support: 0.8119589994045722, T1 General self-efficacy: 0.3649793457412237, T1 Life satisfaction: 0.8227531288076683, T1 Stress: -0.557952553852985, T1 Problem-focused coping: 1.204741720084392, T1 Emotion-focused coping: -0.3594166886391129, T1 Anxiety sensitivity: 0.5557512543954795, T1 Fear of bodily sensations: 0.3819042614516962, T1 Dysfunctional attitudes: -0.2667599319073422, T1 General psychopathology: Global Severity Index (GSI): 0.6542168480146436, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -1.2686339879073294, Change in Social support (T2-T1): 0.0935950031373072, Change in General self-efficacy (T2-T1): -0.1819798191096402, Change in Life satisfaction (T2-T1): 0.4926801854879145, Change in Anxiety sensitivity (T2-T1): -0.1716035604067239, Change in Fear of bodily sensations (T2-T1): -0.4575797123434105, Change in Dysfunctional attitudes (T2-T1): -0.0023346489721794, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -1.0706737655396472, Total mental disorders incidence point prevalence: 0.0', 'Example 4: T1 Positive mental health: 0.4553235669491879, T1 Social support: 0.3668153637450668, T1 General self-efficacy: 0.3649793457412237, T1 Life satisfaction: 0.0136457200384251, T1 Stress: -0.7579302991680386, T1 Problem-focused coping: 1.4683392942313953, T1 Emotion-focused coping: 0.3699004995900614, T1 Anxiety sensitivity: 0.2915275438917872, T1 Fear of bodily sensations: 0.5729626220770014, T1 Dysfunctional attitudes: 1.066971901559727, T1 General psychopathology: Global Severity Index (GSI): 0.2869556530491485, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -1.2686339879073294, Change in Social support (T2-T1): -0.5712884811847251, Change in General self-efficacy (T2-T1): -0.1819798191096402, Change in Life satisfaction (T2-T1): 0.1407378746091632, Change in Anxiety sensitivity (T2-T1): -0.4476516961918859, Change in Fear of bodily sensations (T2-T1): -0.554929794824284, Change in Dysfunctional attitudes (T2-T1): 0.1600776295264137, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): 0.7035364590678871, Total mental disorders incidence point prevalence: 0.0', 'Example 5: T1 Positive mental health: 0.4553235669491879, T1 Social support: 0.5448728180088648, T1 General self-efficacy: 1.498278021627944, T1 Life satisfaction: -1.28092613399239, T1 Stress: 0.6419139180373361, T1 Problem-focused coping: 1.4683392942313953, T1 Emotion-focused coping: 0.5319709858632113, T1 Anxiety sensitivity: 0.8199749648991717, T1 Fear of bodily sensations: 0.5729626220770014, T1 Dysfunctional attitudes: 0.7752180629888056, T1 General psychopathology: Global Severity Index (GSI): -0.4185939092790058, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.235399282050399, Change in Social support (T2-T1): -0.0172189109163777, Change in General self-efficacy (T2-T1): -0.4584361821328143, Change in Life satisfaction (T2-T1): -0.3871755917089017, Change in Anxiety sensitivity (T2-T1): -0.9997479677622098, Change in Fear of bodily sensations (T2-T1): -0.7496299597860477, Change in Dysfunctional attitudes (T2-T1): 0.3766273341912047, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): 0.0147254306908422, Total mental disorders incidence point prevalence: 0.0'] Based on the previous example data prompts, classify the following data. Does the following person develop a psychological disorder between time point T1 and T2? Data: T1 Positive mental health: -0.0279170753483525, T1 Social support: 0.1421238143169474, T1 General self-efficacy: 0.3649793457412237, T1 Life satisfaction: 0.3372886835461141, T1 Stress: 0.4419361727222826, T1 Problem-focused coping: 1.7319368683783989, T1 Emotion-focused coping: 0.2078300133169115, T1 Anxiety sensitivity: 0.1594156886399411, T1 Fear of bodily sensations: 0.2863750811390516, T1 Dysfunctional attitudes: 0.2750686254386546, T1 General psychopathology: Global Severity Index (GSI): 0.0172227087467131, Education: 2.0, T1 BMI: 1.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): 0.7057099569575698, Change in General self-efficacy (T2-T1): -0.1819798191096402, Change in Life satisfaction (T2-T1): 0.1407378746091848, Change in Anxiety sensitivity (T2-T1): -0.8617238998696288, Change in Fear of bodily sensations (T2-T1): -0.846980042266938, Change in Dysfunctional attitudes (T2-T1): 0.4849021865236002, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.8256637821414634\n",
      "NO\n",
      "Please refer to the following examples of correctly classified data points with 'Total mental disorders incidence point prevalence' being the true classification: ['Example 1: T1 Positive mental health: 0.4553235669491879, T1 Social support: 0.6339015451407742, T1 General self-efficacy: -0.0883401246134643, T1 Life satisfaction: -0.1481757617154391, T1 Stress: 0.241958427407229, T1 Problem-focused coping: 0.413948997643382, T1 Emotion-focused coping: -0.6025224180488378, T1 Anxiety sensitivity: 0.8199749648991717, T1 Fear of bodily sensations: 1.4327252448908838, T1 Dysfunctional attitudes: -0.5585137704782637, T1 General psychopathology: Global Severity Index (GSI): 0.5256754297767214, Education: 3.0, T1 BMI: 2.0, Socioeconomic status: 3.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): -0.1280328249700628, Change in General self-efficacy (T2-T1): 0.3709329069367079, Change in Life satisfaction (T2-T1): 1.196564807245358, Change in Anxiety sensitivity (T2-T1): 0.6565408469487619, Change in Fear of bodily sensations (T2-T1): 0.6132711949462831, Change in Dysfunctional attitudes (T2-T1): -0.2730217798031681, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.9141258045448658, Total mental disorders incidence point prevalence: 0.0', 'Example 2: T1 Positive mental health: -0.0279170753483525, T1 Social support: -0.0783282719144384, T1 General self-efficacy: -0.3149998597908083, T1 Life satisfaction: 0.0136457200384251, T1 Stress: -0.157997063222878, T1 Problem-focused coping: 0.9411441459373888, T1 Emotion-focused coping: 0.2078300133169115, T1 Anxiety sensitivity: 0.2915275438917872, T1 Fear of bodily sensations: 0.7640209827023066, T1 Dysfunctional attitudes: 0.6501807036012678, T1 General psychopathology: Global Severity Index (GSI): 1.2785608794559835, Education: 3.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): -0.0172189109163517, Change in General self-efficacy (T2-T1): 0.647389269959882, Change in Life satisfaction (T2-T1): -0.7391179025876333, Change in Anxiety sensitivity (T2-T1): 0.2424686432710189, Change in Fear of bodily sensations (T2-T1): -0.4575797123434101, Change in Dysfunctional attitudes (T2-T1): 0.8638641696869843, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -1.2118154192619288, Total mental disorders incidence point prevalence: 0.0', 'Example 3: T1 Positive mental health: -0.9943983599434336, T1 Social support: 0.3668153637450668, T1 General self-efficacy: -0.9949790653228404, T1 Life satisfaction: 0.4991101652999784, T1 Stress: 0.8418916633523896, T1 Problem-focused coping: 1.204741720084392, T1 Emotion-focused coping: -0.9266633905951376, T1 Anxiety sensitivity: -0.3690317323674431, T1 Fear of bodily sensations: 0.3819042614516962, T1 Dysfunctional attitudes: -0.8085884892533391, T1 General psychopathology: Global Severity Index (GSI): 2.123669695871029, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): 0.281218070878066, Change in Social support (T2-T1): -0.0172189109163777, Change in General self-efficacy (T2-T1): 0.647389269959882, Change in Life satisfaction (T2-T1): 0.3167090300485388, Change in Anxiety sensitivity (T2-T1): 0.6565408469487619, Change in Fear of bodily sensations (T2-T1): 0.1265207825418816, Change in Dysfunctional attitudes (T2-T1): 0.7555893173545889, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -3.1256763646298897, Total mental disorders incidence point prevalence: 0.0', 'Example 4: T1 Positive mental health: -0.0279170753483525, T1 Social support: 0.8119589994045722, T1 General self-efficacy: -0.3149998597908083, T1 Life satisfaction: 0.4843991215041691, T1 Stress: -0.3579748085379315, T1 Problem-focused coping: 0.8532782878883876, T1 Emotion-focused coping: -0.5214871749122628, T1 Anxiety sensitivity: 0.5557512543954795, T1 Fear of bodily sensations: 1.241666884265578, T1 Dysfunctional attitudes: 0.9419345421721892, T1 General psychopathology: Global Severity Index (GSI): 0.0172227087467131, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 1.0, Change in Positive mental health (T2-T1): 0.7978354238065312, Change in Social support (T2-T1): -0.0172189109163777, Change in General self-efficacy (T2-T1): 0.647389269959882, Change in Life satisfaction (T2-T1): -0.0192359030629718, Change in Anxiety sensitivity (T2-T1): 0.5185167790561809, Change in Fear of bodily sensations (T2-T1): 0.41857102998452, Change in Dysfunctional attitudes (T2-T1): -0.1647469274707726, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.3391226589227578, Total mental disorders incidence point prevalence: 0.0', 'Example 5: T1 Positive mental health: -0.511157717645893, T1 Social support: 0.9009877265364608, T1 General self-efficacy: -0.3149998597908083, T1 Life satisfaction: -0.9572831704846811, T1 Stress: 0.0419806820921755, T1 Problem-focused coping: 1.204741720084392, T1 Emotion-focused coping: 0.2888652564534865, T1 Anxiety sensitivity: -0.3690317323674431, T1 Fear of bodily sensations: -0.3823291810495087, T1 Dysfunctional attitudes: -0.2250808121114963, T1 General psychopathology: Global Severity Index (GSI): 0.5256754297767214, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 3.0, Change in Positive mental health (T2-T1): -0.235399282050399, Change in Social support (T2-T1): -0.2388467390237219, Change in General self-efficacy (T2-T1): 0.923845632983056, Change in Life satisfaction (T2-T1): 0.1407378746091632, Change in Anxiety sensitivity (T2-T1): -0.4476516961918859, Change in Fear of bodily sensations (T2-T1): 0.0291707000609912, Change in Dysfunctional attitudes (T2-T1): 0.4849021865236002, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -0.2506606365193577, Total mental disorders incidence point prevalence: 1.0'] Based on the previous example data prompts, classify the following data. Does the following person develop a psychological disorder between time point T1 and T2? Data: T1 Positive mental health: -0.0279170753483525, T1 Social support: 1.0790451808002794, T1 General self-efficacy: -2.3549374763869046, T1 Life satisfaction: -0.471818725223128, T1 Stress: 0.0419806820921755, T1 Problem-focused coping: 0.8532782878883876, T1 Emotion-focused coping: -0.8456281474585625, T1 Anxiety sensitivity: 0.5557512543954795, T1 Fear of bodily sensations: 1.241666884265578, T1 Dysfunctional attitudes: -0.2250808121114963, T1 General psychopathology: Global Severity Index (GSI): 1.5062628203345918, Education: 2.0, T1 BMI: 2.0, Socioeconomic status: 2.0, Change in Positive mental health (T2-T1): -0.7520166349788642, Change in Social support (T2-T1): -0.460474567131092, Change in General self-efficacy (T2-T1): 1.7532147220525784, Change in Life satisfaction (T2-T1): 0.3167090300485398, Change in Anxiety sensitivity (T2-T1): 0.5185167790561809, Change in Fear of bodily sensations (T2-T1): 1.781472184716851, Change in Dysfunctional attitudes (T2-T1): 0.8097267435207866, Change in General psychopathology: Global Severity Index (GSI) (T2-T1): -1.09104984935167\n",
      "NO\n",
      "Time taken: 1.0658280849456787 seconds\n",
      "NO    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Vignette prompt",
   "id": "de0f35854b57f842"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T15:38:27.698617Z",
     "start_time": "2025-05-18T15:38:16.344262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vignette_y_pred_grok = []\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"XAI_API_KEY\"),\n",
    "    base_url = \"https://api.x.ai/v1\",\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_vignette_prompt:\n",
    "    completion = client.chat.completions.create(\n",
    "        model = \"grok-3-beta\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": vignette_instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    if completion.choices[0].message.content.strip() not in (\"YES\", \"NO\"):\n",
    "        print(\"\\n Invalid output. Retry prompting. \\n\")\n",
    "        completion = client.chat.completions.create(\n",
    "            model = \"grok-3-beta\",\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": retry_instruction},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    vignette_y_pred_grok.append(completion.choices[0].message.content)\n",
    "    print(completion.choices[0].message.content)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_grok_vignette = end - start\n",
    "time_grok_vignette_df = pd.DataFrame({\"time\": [time_grok_vignette]})\n",
    "time_grok_vignette_df.to_csv(\"../exp/times_LLMs/Grok/time_grok_vignette_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_vignette_grok = pd.Series(vignette_y_pred_grok).value_counts()\n",
    "print(counts_vignette_grok)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "vignette_y_pred_grok_val = [1 if response == \"YES\" else 0 for response in vignette_y_pred_grok]\n",
    "\n",
    "# save the array to a csv file\n",
    "vignette_df_grok = pd.DataFrame(vignette_y_pred_grok_val, columns = [\"y_pred\"])\n",
    "vignette_df_grok.to_csv(\"../exp/y_pred_LLMs/Grok/y_pred_grok_vignette_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "fc98053086ee26ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A woman with a BMI of 1.0, an education level of 2.0, and a socioeconomic status of 2.0 has the following psychological profile: At time point 1, she showed average (-0.028) positive mental health, average (0.142) social support, and average (0.365) general self-efficacy. Her life satisfaction was average (0.337), and she relied on above average (1.732) problem-focused and average (0.208) emotion-focused coping strategies. Anxiety sensitivity was average (0.159), and her fear of bodily sensations was average (0.286). She reported average (0.275) levels of dysfunctional attitudes and average (0.017) levels of general psychopathology. Her stress level was average (0.442). By time point 2, approximately 17 months later, she reported similar (-0.752) positive mental health, similar (0.706) social support, and similar (-0.182) self-efficacy. Life satisfaction was similar (0.141). Anxiety sensitivity was reported to be similar (-0.862), and fear of bodily sensations was similar (-0.847). Dysfunctional attitudes were similar (0.485), and general psychopathology was similar (-0.826) compared to time point 1. Does this person develop a psychological disorder between time point T1 and T2?\n",
      "NO\n",
      "A woman with a BMI of 2.0, an education level of 2.0, and a socioeconomic status of 2.0 has the following psychological profile: At time point 1, she showed average (-0.028) positive mental health, above average (1.079) social support, and below average (-2.355) general self-efficacy. Her life satisfaction was average (-0.472), and she relied on average (0.853) problem-focused and average (-0.846) emotion-focused coping strategies. Anxiety sensitivity was average (0.556), and her fear of bodily sensations was above average (1.242). She reported average (-0.225) levels of dysfunctional attitudes and above average (1.506) levels of general psychopathology. Her stress level was average (0.042). By time point 2, approximately 17 months later, she reported similar (-0.752) positive mental health, similar (-0.460) social support, and increased (1.753) self-efficacy. Life satisfaction was similar (0.317). Anxiety sensitivity was reported to be similar (0.519), and fear of bodily sensations was increased (1.781). Dysfunctional attitudes were similar (0.810), and general psychopathology was decreased (-1.091) compared to time point 1. Does this person develop a psychological disorder between time point T1 and T2?\n",
      "NO\n",
      "Time taken: 11.324376106262207 seconds\n",
      "NO    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Thinking prompt",
   "id": "f4745638f7750586"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T12:41:34.047851Z",
     "start_time": "2025-05-19T12:41:15.522292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "thinking_prompt_y_pred_grok = []\n",
    "thinking_prompt_explanation_grok = []\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"XAI_API_KEY\"),\n",
    "    base_url = \"https://api.x.ai/v1\",\n",
    ")\n",
    "\n",
    "# measure time in seconds\n",
    "start = time.time()\n",
    "\n",
    "# iterate over the test set and save the response for each prompt in an array\n",
    "for prompt in X_test_thinking_prompt:\n",
    "    completion = client.chat.completions.create(\n",
    "        model = \"grok-3-beta\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": thinking_instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    try:\n",
    "        prediction = re.findall(r'Prediction: (.*)', completion.choices[0].message.content)[0].strip()\n",
    "        explanation = re.findall(r'Explanation: (.*)', completion.choices[0].message.content)[0].strip()\n",
    "        thinking_prompt_y_pred_grok.append(prediction)\n",
    "        thinking_prompt_explanation_grok.append(explanation)\n",
    "        print(prediction)\n",
    "    except IndexError:\n",
    "        print(\"IndexError\")\n",
    "        thinking_prompt_y_pred_grok.append(\"IndexError\")\n",
    "        thinking_prompt_explanation_grok.append(\"IndexError\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")\n",
    "time_grok_thinking_prompt = end - start\n",
    "time_grok_thinking_prompt_df = pd.DataFrame({\"time\": [time_grok_thinking_prompt]})\n",
    "time_grok_thinking_prompt_df.to_csv(\"../exp/times_LLMs/Grok/time_grok_thinking_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "# value counts for array\n",
    "counts_thinking_prompt_grok = pd.Series(thinking_prompt_y_pred_grok).value_counts()\n",
    "print(counts_thinking_prompt_grok)\n",
    "\n",
    "# convert YES to 1 and NO to 0\n",
    "thinking_prompt_y_pred_grok_val = [1 if response == \"YES\" else 0 for response in thinking_prompt_y_pred_grok]\n",
    "\n",
    "# save the array to a csv file\n",
    "thinking_prompt_df_grok = pd.DataFrame(thinking_prompt_y_pred_grok_val, columns = [\"y_pred\"])\n",
    "thinking_prompt_df_grok.to_csv(\"../exp/y_pred_LLMs/Grok/y_pred_grok_thinking_prompt.csv\", sep = \",\", index = False)\n",
    "\n",
    "thinking_prompt_df_explanation_grok = pd.DataFrame(thinking_prompt_explanation_grok, columns = [\"thinking\"])\n",
    "thinking_prompt_df_explanation_grok.to_csv(\"../exp/y_pred_LLMs/Grok/explanation_grok_thinking_prompt.csv\", sep = \",\", index = False)"
   ],
   "id": "d391c3382881230f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n",
      "NO\n",
      "Time taken: 18.500372171401978 seconds\n",
      "NO    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b65a368b0299d5ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8 Mistral",
   "id": "56ed79ae4b6367e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# client = Mistral(api_key = os.environ[\"MISTRAL_API_KEY\"])\n",
    "#\n",
    "# chat_response = client.chat.complete(\n",
    "#     model = \"mistral-large-latest\",\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"What is the best French cheese?\",\n",
    "#         },\n",
    "#     ]\n",
    "# )\n",
    "# print(chat_response.choices[0].message.content)"
   ],
   "id": "7443857e89b1d96f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
